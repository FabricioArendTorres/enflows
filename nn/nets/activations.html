<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.0">
<title>flowcon.nn.nets.activations API documentation</title>
<meta name="description" content="Base, including ClipSwish https://github.com/yperugachidiaz/invertible_densenets/blob/master/lib/layers/base/activations.py …">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>flowcon.nn.nets.activations</code></h1>
</header>
<section id="section-intro">
<p>Base, including ClipSwish <a href="https://github.com/yperugachidiaz/invertible_densenets/blob/master/lib/layers/base/activations.py">https://github.com/yperugachidiaz/invertible_densenets/blob/master/lib/layers/base/activations.py</a></p>
<p>MIT License</p>
<p>Copyright (c) 2019 Ricky Tian Qi Chen
Copyright (c) 2020 Cheng Lu
Copyright (c) 2021 Yura Perugachi-Diaz
Copyright (c) 2022 Byeongkeun Ahn
Copyright (c) 2023 Fabricio Arend Torres</p>
<p>Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:</p>
<p>The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.</p>
<p>THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.</p>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="flowcon.nn.nets.activations.CLipSwish"><code class="flex name class">
<span>class <span class="ident">CLipSwish</span></span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CLipSwish(nn.Module):

    def __init__(self):
        super(CLipSwish, self).__init__()
        self.swish = Swish()
        self._does_concat = True

    def forward(self, x):
        x = torch.cat((x, -x), 1)
        return self.swish(x).div_(1.004)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.nn.nets.activations.CLipSwish.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.nn.nets.activations.CLipSwish.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.nn.nets.activations.CLipSwish.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.nn.nets.activations.CLipSwish.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="flowcon.nn.nets.activations.CSin"><code class="flex name class">
<span>class <span class="ident">CSin</span></span>
<span>(</span><span>w0=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CSin(nn.Module):
    def __init__(self, w0=1):
        super(CSin, self).__init__()
        self.w0 = w0
        self._does_concat = True

    def forward(self, x):
        x = torch.cat((x, -x), 1)
        return torch.sin(x * self.w0) / (self.w0 * math.sqrt(2))

    def build_clone(self):
        return copy.deepcopy(self)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.nn.nets.activations.CSin.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.nn.nets.activations.CSin.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.nn.nets.activations.CSin.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.nn.nets.activations.CSin.build_clone"><code class="name flex">
<span>def <span class="ident">build_clone</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.nn.nets.activations.CSin.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="flowcon.nn.nets.activations.FullSort"><code class="flex name class">
<span>class <span class="ident">FullSort</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FullSort(nn.Module):

    def forward(self, x):
        return torch.sort(x, 1)[0]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.nn.nets.activations.FullSort.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.nn.nets.activations.FullSort.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.nn.nets.activations.FullSort.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.nn.nets.activations.FullSort.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="flowcon.nn.nets.activations.LeakyLSwish"><code class="flex name class">
<span>class <span class="ident">LeakyLSwish</span></span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LeakyLSwish(nn.Module):

    def __init__(self):
        super(LeakyLSwish, self).__init__()
        self.alpha = nn.Parameter(torch.tensor([-3.]))
        self.beta = nn.Parameter(torch.tensor([0.5]))

    def forward(self, x):
        alpha = torch.sigmoid(self.alpha)
        return alpha * x + (1 - alpha) * (x * torch.sigmoid_(x * F.softplus(self.beta))).div_(1.1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.nn.nets.activations.LeakyLSwish.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.nn.nets.activations.LeakyLSwish.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.nn.nets.activations.LeakyLSwish.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.nn.nets.activations.LeakyLSwish.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="flowcon.nn.nets.activations.LipSwish"><code class="flex name class">
<span>class <span class="ident">LipSwish</span></span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LipSwish(nn.Module):

    def __init__(self):
        super(LipSwish, self).__init__()
        self.swish = Swish()

    def forward(self, x):
        return self.swish(x).div_(1.004)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.nn.nets.activations.LipSwish.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.nn.nets.activations.LipSwish.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.nn.nets.activations.LipSwish.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.nn.nets.activations.LipSwish.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="flowcon.nn.nets.activations.LipschitzCube"><code class="flex name class">
<span>class <span class="ident">LipschitzCube</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LipschitzCube(nn.Module):

    def forward(self, x):
        return (x &gt;= 1).to(x) * (x - 2 / 3) + (x &lt;= -1).to(x) * (x + 2 / 3) + ((x &gt; -1) * (x &lt; 1)).to(x) * x ** 3 / 3</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.nn.nets.activations.LipschitzCube.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.nn.nets.activations.LipschitzCube.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.nn.nets.activations.LipschitzCube.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.nn.nets.activations.LipschitzCube.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="flowcon.nn.nets.activations.MaxMin"><code class="flex name class">
<span>class <span class="ident">MaxMin</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Module that computes max and min values of input tensor.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MaxMin(nn.Module):
    &#34;&#34;&#34;
    Module that computes max and min values of input tensor.
    &#34;&#34;&#34;
    def forward(self, x):
        b, d = x.shape
        max_vals = torch.max(x.view(b, d // 2, 2), 2)[0]
        min_vals = torch.min(x.view(b, d // 2, 2), 2)[0]
        return torch.cat([max_vals, min_vals], 1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.nn.nets.activations.MaxMin.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.nn.nets.activations.MaxMin.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.nn.nets.activations.MaxMin.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.nn.nets.activations.MaxMin.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="flowcon.nn.nets.activations.Sin"><code class="flex name class">
<span>class <span class="ident">Sin</span></span>
<span>(</span><span>w0=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Sin(nn.Module):
    def __init__(self, w0=1):
        super(Sin, self).__init__()
        self.w0 = w0

    def forward(self, x):
        return torch.sin(x * self.w0) / self.w0

    def build_clone(self):
        return copy.deepcopy(self)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.nn.nets.activations.Sin.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.nn.nets.activations.Sin.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.nn.nets.activations.Sin.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.nn.nets.activations.Sin.build_clone"><code class="name flex">
<span>def <span class="ident">build_clone</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.nn.nets.activations.Sin.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="flowcon.nn.nets.activations.Swish"><code class="flex name class">
<span>class <span class="ident">Swish</span></span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Swish(nn.Module):

    def __init__(self):
        super(Swish, self).__init__()
        self.beta = nn.Parameter(torch.tensor([0.5]))

    def forward(self, x):
        return (x * torch.sigmoid_(x * F.softplus(self.beta))).div_(1.1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.nn.nets.activations.Swish.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.nn.nets.activations.Swish.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.nn.nets.activations.Swish.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.nn.nets.activations.Swish.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="flowcon.nn.nets.activations.SwishFn"><code class="flex name class">
<span>class <span class="ident">SwishFn</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class to create custom <code>autograd.Function</code>.</p>
<p>To create a custom <code>autograd.Function</code>, subclass this class and implement
the :meth:<code>forward</code> and :meth:<code>backward</code> static methods. Then, to use your custom
op in the forward pass, call the class method <code>apply</code>. Do not call
:meth:<code>forward</code> directly.</p>
<p>To ensure correctness and best performance, make sure you are calling the
correct methods on <code>ctx</code> and validating your backward function using
:func:<code>torch.autograd.gradcheck</code>.</p>
<p>See :ref:<code>extending-autograd</code> for more details on how to use this class.</p>
<p>Examples::</p>
<pre><code>&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)
&gt;&gt;&gt; class Exp(Function):
&gt;&gt;&gt;     @staticmethod
&gt;&gt;&gt;     def forward(ctx, i):
&gt;&gt;&gt;         result = i.exp()
&gt;&gt;&gt;         ctx.save_for_backward(result)
&gt;&gt;&gt;         return result
&gt;&gt;&gt;
&gt;&gt;&gt;     @staticmethod
&gt;&gt;&gt;     def backward(ctx, grad_output):
&gt;&gt;&gt;         result, = ctx.saved_tensors
&gt;&gt;&gt;         return grad_output * result
&gt;&gt;&gt;
&gt;&gt;&gt; # Use it by calling the apply method:
&gt;&gt;&gt; # xdoctest: +SKIP
&gt;&gt;&gt; output = Exp.apply(input)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SwishFn(torch.autograd.Function):

    @staticmethod
    def forward(ctx, x, beta):
        beta_sigm = torch.sigmoid(beta * x)
        output = x * beta_sigm
        ctx.save_for_backward(x, output, beta)
        return output / 1.1

    @staticmethod
    def backward(ctx, grad_output):
        x, output, beta = ctx.saved_tensors
        beta_sigm = output / x
        grad_x = grad_output * (beta * output + beta_sigm * (1 - beta * output))
        grad_beta = torch.sum(grad_output * (x * output - output * output)).expand_as(beta)
        return grad_x / 1.1, grad_beta / 1.1</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.autograd.function.Function</li>
<li>torch.autograd.function._SingleLevelFunction</li>
<li>torch._C._FunctionBase</li>
<li>torch.autograd.function.FunctionCtx</li>
<li>torch.autograd.function._HookMixin</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="flowcon.nn.nets.activations.SwishFn.backward"><code class="name flex">
<span>def <span class="ident">backward</span></span>(<span>ctx, grad_output)</span>
</code></dt>
<dd>
<div class="desc"><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code>vjp</code> function.)</p>
<p>It must accept a context :attr:<code>ctx</code> as the first argument, followed by
as many outputs as the :func:<code>forward</code> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
:func:<code>forward</code>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute :attr:<code>ctx.needs_input_grad</code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
:func:<code>backward</code> will have <code>ctx.needs_input_grad[0] = True</code> if the
first input to :func:<code>forward</code> needs gradient computed w.r.t. the
output.</p></div>
</dd>
<dt id="flowcon.nn.nets.activations.SwishFn.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>ctx, x, beta)</span>
</code></dt>
<dd>
<div class="desc"><p>Define the forward of the custom autograd Function.</p>
<p>This function is to be overridden by all subclasses.
There are two ways to define forward:</p>
<p>Usage 1 (Combined forward and ctx)::</p>
<pre><code>@staticmethod
def forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:
    pass
</code></pre>
<ul>
<li>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</li>
<li>See :ref:<code>combining-forward-context</code> for more details</li>
</ul>
<p>Usage 2 (Separate forward and ctx)::</p>
<pre><code>@staticmethod
def forward(*args: Any, **kwargs: Any) -&gt; Any:
    pass

@staticmethod
def setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:
    pass
</code></pre>
<ul>
<li>The forward no longer accepts a ctx argument.</li>
<li>Instead, you must also override the :meth:<code>torch.autograd.Function.setup_context</code>
staticmethod to handle setting up the <code>ctx</code> object.
<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs
to the forward.</li>
<li>See :ref:<code>extending-autograd</code> for more details</li>
</ul>
<p>The context can be used to store arbitrary data that can be then
retrieved during the backward pass. Tensors should not be stored
directly on <code>ctx</code> (though this is not currently enforced for
backward compatibility). Instead, tensors should be saved either with
:func:<code>ctx.save_for_backward</code> if they are intended to be used in
<code>backward</code> (equivalently, <code>vjp</code>) or :func:<code>ctx.save_for_forward</code>
if they are intended to be used for in <code>jvp</code>.</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="flowcon.nn.nets" href="index.html">flowcon.nn.nets</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="flowcon.nn.nets.activations.CLipSwish" href="#flowcon.nn.nets.activations.CLipSwish">CLipSwish</a></code></h4>
<ul class="">
<li><code><a title="flowcon.nn.nets.activations.CLipSwish.call_super_init" href="#flowcon.nn.nets.activations.CLipSwish.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.CLipSwish.dump_patches" href="#flowcon.nn.nets.activations.CLipSwish.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.CLipSwish.forward" href="#flowcon.nn.nets.activations.CLipSwish.forward">forward</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.CLipSwish.training" href="#flowcon.nn.nets.activations.CLipSwish.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.nn.nets.activations.CSin" href="#flowcon.nn.nets.activations.CSin">CSin</a></code></h4>
<ul class="">
<li><code><a title="flowcon.nn.nets.activations.CSin.build_clone" href="#flowcon.nn.nets.activations.CSin.build_clone">build_clone</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.CSin.call_super_init" href="#flowcon.nn.nets.activations.CSin.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.CSin.dump_patches" href="#flowcon.nn.nets.activations.CSin.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.CSin.forward" href="#flowcon.nn.nets.activations.CSin.forward">forward</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.CSin.training" href="#flowcon.nn.nets.activations.CSin.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.nn.nets.activations.FullSort" href="#flowcon.nn.nets.activations.FullSort">FullSort</a></code></h4>
<ul class="">
<li><code><a title="flowcon.nn.nets.activations.FullSort.call_super_init" href="#flowcon.nn.nets.activations.FullSort.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.FullSort.dump_patches" href="#flowcon.nn.nets.activations.FullSort.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.FullSort.forward" href="#flowcon.nn.nets.activations.FullSort.forward">forward</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.FullSort.training" href="#flowcon.nn.nets.activations.FullSort.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.nn.nets.activations.LeakyLSwish" href="#flowcon.nn.nets.activations.LeakyLSwish">LeakyLSwish</a></code></h4>
<ul class="">
<li><code><a title="flowcon.nn.nets.activations.LeakyLSwish.call_super_init" href="#flowcon.nn.nets.activations.LeakyLSwish.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.LeakyLSwish.dump_patches" href="#flowcon.nn.nets.activations.LeakyLSwish.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.LeakyLSwish.forward" href="#flowcon.nn.nets.activations.LeakyLSwish.forward">forward</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.LeakyLSwish.training" href="#flowcon.nn.nets.activations.LeakyLSwish.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.nn.nets.activations.LipSwish" href="#flowcon.nn.nets.activations.LipSwish">LipSwish</a></code></h4>
<ul class="">
<li><code><a title="flowcon.nn.nets.activations.LipSwish.call_super_init" href="#flowcon.nn.nets.activations.LipSwish.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.LipSwish.dump_patches" href="#flowcon.nn.nets.activations.LipSwish.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.LipSwish.forward" href="#flowcon.nn.nets.activations.LipSwish.forward">forward</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.LipSwish.training" href="#flowcon.nn.nets.activations.LipSwish.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.nn.nets.activations.LipschitzCube" href="#flowcon.nn.nets.activations.LipschitzCube">LipschitzCube</a></code></h4>
<ul class="">
<li><code><a title="flowcon.nn.nets.activations.LipschitzCube.call_super_init" href="#flowcon.nn.nets.activations.LipschitzCube.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.LipschitzCube.dump_patches" href="#flowcon.nn.nets.activations.LipschitzCube.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.LipschitzCube.forward" href="#flowcon.nn.nets.activations.LipschitzCube.forward">forward</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.LipschitzCube.training" href="#flowcon.nn.nets.activations.LipschitzCube.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.nn.nets.activations.MaxMin" href="#flowcon.nn.nets.activations.MaxMin">MaxMin</a></code></h4>
<ul class="">
<li><code><a title="flowcon.nn.nets.activations.MaxMin.call_super_init" href="#flowcon.nn.nets.activations.MaxMin.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.MaxMin.dump_patches" href="#flowcon.nn.nets.activations.MaxMin.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.MaxMin.forward" href="#flowcon.nn.nets.activations.MaxMin.forward">forward</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.MaxMin.training" href="#flowcon.nn.nets.activations.MaxMin.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.nn.nets.activations.Sin" href="#flowcon.nn.nets.activations.Sin">Sin</a></code></h4>
<ul class="">
<li><code><a title="flowcon.nn.nets.activations.Sin.build_clone" href="#flowcon.nn.nets.activations.Sin.build_clone">build_clone</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.Sin.call_super_init" href="#flowcon.nn.nets.activations.Sin.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.Sin.dump_patches" href="#flowcon.nn.nets.activations.Sin.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.Sin.forward" href="#flowcon.nn.nets.activations.Sin.forward">forward</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.Sin.training" href="#flowcon.nn.nets.activations.Sin.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.nn.nets.activations.Swish" href="#flowcon.nn.nets.activations.Swish">Swish</a></code></h4>
<ul class="">
<li><code><a title="flowcon.nn.nets.activations.Swish.call_super_init" href="#flowcon.nn.nets.activations.Swish.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.Swish.dump_patches" href="#flowcon.nn.nets.activations.Swish.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.Swish.forward" href="#flowcon.nn.nets.activations.Swish.forward">forward</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.Swish.training" href="#flowcon.nn.nets.activations.Swish.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.nn.nets.activations.SwishFn" href="#flowcon.nn.nets.activations.SwishFn">SwishFn</a></code></h4>
<ul class="">
<li><code><a title="flowcon.nn.nets.activations.SwishFn.backward" href="#flowcon.nn.nets.activations.SwishFn.backward">backward</a></code></li>
<li><code><a title="flowcon.nn.nets.activations.SwishFn.forward" href="#flowcon.nn.nets.activations.SwishFn.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.0</a>.</p>
</footer>
</body>
</html>
