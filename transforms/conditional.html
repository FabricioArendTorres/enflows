<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>flowcon.transforms.conditional API documentation</title>
<meta name="description" content="Implementations of autoregressive transforms." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>flowcon.transforms.conditional</code></h1>
</header>
<section id="section-intro">
<p>Implementations of autoregressive transforms.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Implementations of autoregressive transforms.&#34;&#34;&#34;

import numpy as np
import torch
from torch.nn import functional as F

from flowcon.transforms.base import Transform
from flowcon.transforms.splines.linear import linear_spline
from flowcon.transforms.splines import rational_quadratic
from flowcon.transforms.splines.rational_quadratic import (
    rational_quadratic_spline,
    unconstrained_rational_quadratic_spline,
)
from flowcon.transforms.UMNN import *

from flowcon.nn.nets import ResidualNet, MLP
from flowcon.utils import torchutils
from flowcon.transforms.orthogonal import ParametrizedHouseHolder, _apply_batchwise_transforms_nodet
from flowcon.transforms.adaptive_sigmoids import SumOfSigmoids
from typing import *


class ConditionalTransform(Transform):
    &#34;&#34;&#34;Transforms each input variable with an invertible transformation, conditioned on some given input.
    &#34;&#34;&#34;

    def __init__(self,
                 features,
                 hidden_features=64,
                 context_features=1,
                 num_blocks=2,
                 use_residual_blocks=True,
                 activation=F.relu,
                 dropout_probability=0.0,
                 use_batch_norm=False,
                 conditional_net: torch.nn.Module = None, ):

        super(ConditionalTransform, self).__init__()
        self.features = features

        if conditional_net is not None:
            assert isinstance(conditional_net, torch.nn.Module)
            self.conditional_net = conditional_net
        else:

            self.conditional_net = self.set_default_network(activation, context_features, dropout_probability,
                                                            hidden_features, num_blocks,
                                                            use_batch_norm, use_residual_blocks)

    def set_default_network(self, activation, context_features, dropout_probability, hidden_features, num_blocks,
                            use_batch_norm, use_residual_blocks):
        if use_residual_blocks:
            conditional_net = ResidualNet(in_features=context_features,
                                          out_features=self._num_parameters(),
                                          hidden_features=hidden_features,
                                          activation=activation,
                                          num_blocks=num_blocks,
                                          dropout_probability=dropout_probability,
                                          use_batch_norm=use_batch_norm
                                          )
        else:
            conditional_net = MLP(in_shape=(context_features,),
                                  out_shape=(self._num_parameters(),),
                                  hidden_sizes=[hidden_features] * num_blocks)
            if dropout_probability &gt; 1e-12:
                raise NotImplementedError(&#34;No dropout for MLP&#34;)
            if use_batch_norm:
                raise NotImplementedError(&#34;No batch norm for MLP&#34;)
        return conditional_net

    def _num_parameters(self):
        return self.features * self._output_dim_multiplier()

    def forward(self, inputs, context=None):
        if context is None:
            raise TypeError(&#34;Conditional transforms require a context.&#34;)
        conditional_params = self.conditional_net(context)
        outputs, logabsdet = self._forward_given_params(inputs, conditional_params)
        return outputs, logabsdet

    def inverse(self, inputs, context=None):
        if context is None:
            raise TypeError(&#34;Conditional transforms require a context.&#34;)
        conditional_params = self.conditional_net(context)
        outputs, logabsdet = self._inverse_given_params(inputs, conditional_params)
        return outputs, logabsdet

    def _output_dim_multiplier(self):
        raise NotImplementedError()

    def _forward_given_params(self, inputs, autoregressive_params):
        raise NotImplementedError()

    def _inverse_given_params(self, inputs, autoregressive_params):
        raise NotImplementedError()


class AffineConditionalTransform(ConditionalTransform):
    def __init__(
            self,
            features,
            hidden_features,
            context_features,
            num_blocks=2,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
    ):
        super(AffineConditionalTransform, self).__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm
        )

    def _output_dim_multiplier(self):
        return 2

    def _forward_given_params(self, inputs, conditional_params):
        unconstrained_scale, shift = self._unconstrained_scale_and_shift(
            conditional_params
        )
        # scale = torch.sigmoid(unconstrained_scale + 2.0) + self._epsilon
        scale = F.softplus(unconstrained_scale) + self._epsilon
        log_scale = torch.log(scale)
        outputs = scale * inputs + shift
        logabsdet = torchutils.sum_except_batch(log_scale, num_batch_dims=1)
        return outputs, logabsdet

    def _inverse_given_params(self, inputs, conditional_params):
        unconstrained_scale, shift = self._unconstrained_scale_and_shift(
            conditional_params
        )
        # scale = torch.sigmoid(unconstrained_scale + 2.0) + self._epsilon
        scale = F.softplus(unconstrained_scale) + self._epsilon
        log_scale = torch.log(scale)
        outputs = (inputs - shift) / scale
        logabsdet = -torchutils.sum_except_batch(log_scale, num_batch_dims=1)
        return outputs, logabsdet

    def _unconstrained_scale_and_shift(self, conditional_params):
        conditional_params = conditional_params.view(
            -1, self.features, self._output_dim_multiplier()
        )
        unconstrained_scale = conditional_params[..., 0]
        shift = conditional_params[..., 1]
        return unconstrained_scale, shift


class ConditionalShiftTransform(ConditionalTransform):
    def     __init__(
            self,
            features,
            hidden_features,
            context_features,
            num_blocks=2,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
            **kwargs
    ):
        self.features = features
        super(ConditionalShiftTransform, self).__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm,
            **kwargs
        )

    def _output_dim_multiplier(self):
        return 1

    def _forward_given_params(self, inputs, conditional_params):
        shift = self._unconstrained_shift(
            conditional_params
        )
        # scale = torch.sigmoid(unconstrained_scale + 2.0) + self._epsilon
        outputs = inputs + shift.view(inputs.shape)
        logabsdet = inputs.new_zeros(inputs.shape[0])
        return outputs, logabsdet

    def _inverse_given_params(self, inputs, conditional_params):
        shift = self._unconstrained_shift(
            conditional_params
        )
        # scale = torch.sigmoid(unconstrained_scale + 2.0) + self._epsilon
        outputs = (inputs - shift.view(inputs.shape))
        logabsdet = inputs.new_zeros(inputs.shape[0])
        return outputs, logabsdet

    def _unconstrained_shift(self, conditional_params):
        # split_idx = autoregressive_params.size(1) // 2
        # unconstrained_scale = autoregressive_params[..., :split_idx]
        # shift = autoregressive_params[..., split_idx:]
        # return unconstrained_scale, shift
        conditional_params = conditional_params.view(
            -1, self.features
        )
        shift = conditional_params  # [..., None]
        return shift


class ConditionalScaleTransform(ConditionalTransform):
    def __init__(
            self,
            features,
            hidden_features,
            context_features,
            num_blocks=2,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
            **kwargs
    ):
        self.features = features
        super(ConditionalScaleTransform, self).__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm,
            **kwargs
        )
        self.eps = 1e-5

    def _output_dim_multiplier(self):
        return 1


    def _forward_given_params(self, inputs, conditional_params):
        scale = self._constrained_scale(
            conditional_params
        )
        outputs = inputs * scale.view(inputs.shape)
        logabsdet = torch.log(scale).sum(-1).view(inputs.shape[0])

        return outputs, logabsdet

    def _inverse_given_params(self, inputs, conditional_params):
        scale = self._constrained_scale(
            conditional_params
        )
        outputs = inputs / scale.view(inputs.shape)
        logabsdet = -torch.log(scale).sum(-1).view(inputs.shape[0])

        return outputs, logabsdet

    def _constrained_scale(self, conditional_params):
        # split_idx = autoregressive_params.size(1) // 2
        # unconstrained_scale = autoregressive_params[..., :split_idx]
        # shift = autoregressive_params[..., split_idx:]
        # return unconstrained_scale, shift
        conditional_params = conditional_params.view(
            -1, self.features
        )
        scale = torch.nn.functional.softplus(conditional_params) + self.eps
        return scale


class ConditionalLUTransform(ConditionalTransform):
    def __init__(
            self,
            features,
            hidden_features,
            context_features,
            num_blocks=2,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
            eps=1e-7
    ):
        super(ConditionalLUTransform, self).__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm
        )

        self.eps = eps

        self.lower_indices = np.tril_indices(features, k=-1)
        self.upper_indices = np.triu_indices(features, k=1)
        self.diag_indices = np.diag_indices(features)

        self.softplus = torch.nn.Softplus()
        self.diag_entries = torch.nn.Parameter(torch.eye(features), requires_grad=False)
        self.default_pivot = torch.nn.Parameter(torch.arange(1, self.features + 1, dtype=torch.int32).unsqueeze(0),
                                                requires_grad=False)
        self.scale_non_diag = torch.nn.Parameter(- 2 * torch.ones(()), requires_grad=True)

    def _output_dim_multiplier(self):
        return self.features

    def _forward_given_params(self, inputs, conditional_params):
        lower, upper = self._create_lower_upper(
            conditional_params
        )
        outputs = upper @ inputs.unsqueeze(-1)
        outputs = (lower @ outputs).view(inputs.shape)
        logabsdet = upper.diagonal(0, -1, -2).log().sum(-1)  # ...#inputs.new_ones(inputs.shape[0])
        return outputs, logabsdet

    def _inverse_given_params(self, inputs, conditional_params):
        lower, upper = self._create_lower_upper(
            conditional_params
        )
        outputs = torch.linalg.lu_solve(torch.tril(lower, -1) + upper,
                                        torch.broadcast_to(self.default_pivot, inputs.shape),
                                        inputs.unsqueeze(-1))

        logabsdet = -upper.diagonal(0, -1, -2).log().sum(-1)  # ...#inputs.new_ones(inputs.shape[0])
        return outputs.view(inputs.shape), logabsdet

    def _unconstrained_entries(self, conditional_params):
        conditional_params = conditional_params.view(
            -1, self.features, self._output_dim_multiplier()
        )
        return conditional_params

    def _create_lower_upper(self, conditional_params):
        unconstrained_matrix_vals = self._unconstrained_entries(conditional_params)

        lower = torch.nn.functional.softplus(self.scale_non_diag) * torch.tril(unconstrained_matrix_vals, diagonal=-1) + self.diag_entries
        upper_diag = torch.diag_embed(self.softplus(unconstrained_matrix_vals.diagonal(0, -1, -2)) + self.eps)
        upper = torch.nn.functional.softplus(self.scale_non_diag) * torch.triu(unconstrained_matrix_vals, diagonal=1) + upper_diag
        return lower, upper


class ConditionalRotationTransform(ConditionalTransform):
    def _output_dim_multiplier(self):
        pass

    def __init__(
            self,
            features,
            hidden_features,
            context_features,
            num_blocks=1,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
    ):
        assert features == 2, &#34;Only available for 2D rotations.&#34;

        super(ConditionalRotationTransform, self).__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm
        )

    def _num_parameters(self):
        return 1

    def build_matrix(self, conditional_params):
        theta = conditional_params
        m1 = torch.cos(theta)
        m2 = -torch.sin(theta)
        m3 = -m2
        m4 = m1
        matrix = torch.concatenate([m1, m2, m3, m4], -1).view(-1, self.features, self.features)
        return matrix

    def _forward_given_params(self, inputs: torch.Tensor, conditional_params):
        matrix = self.build_matrix(conditional_params)

        outputs = (matrix @ inputs.unsqueeze(-1)).squeeze()
        logabsdet = inputs.new_zeros(inputs.shape[0])  # ...#inputs.new_ones(inputs.shape[0])
        return outputs, logabsdet

    def _inverse_given_params(self, inputs, conditional_params):
        matrix = self.build_matrix(conditional_params)

        outputs = (torch.transpose(matrix, dim0=-2, dim1=-1) @ inputs.unsqueeze(-1)).squeeze()
        logabsdet = inputs.new_zeros(inputs.shape[0])  # ...#inputs.new_ones(inputs.shape[0])
        return outputs, logabsdet


class ConditionalOrthogonalTransform(ConditionalTransform):
    def __init__(
            self,
            features,
            hidden_features,
            context_features,
            num_blocks=2,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
    ):
        super(ConditionalOrthogonalTransform, self).__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm
        )

    def _output_dim_multiplier(self):
        return self.features

    def _forward_given_params(self, inputs, conditional_params):
        householder = self._get_matrices(conditional_params)
        outputs, logabsdet = householder.forward(inputs)
        return outputs, logabsdet

    def _inverse_given_params(self, inputs, conditional_params):
        householder = self._get_matrices(conditional_params)
        outputs, logabsdet = householder.inverse(inputs)

        return outputs.squeeze(), logabsdet

    def _get_matrices(self, conditional_params) -&gt; ParametrizedHouseHolder:
        q_vectors = self._unconstrained_entries(
            conditional_params
        )
        householder = ParametrizedHouseHolder(q_vectors)
        return householder

    def _unconstrained_entries(self, conditional_params):
        conditional_params = conditional_params.view(
            -1, self.features, self._output_dim_multiplier()
        )
        return conditional_params


class ConditionalSVDTransform(ConditionalTransform):
    def __init__(
            self,
            features,
            hidden_features,
            context_features,
            use_bias=True,
            num_blocks=2,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
            eps=1e-3,
            lipschitz_constant_limit=None
    ):
        self.use_bias = use_bias

        super(ConditionalSVDTransform, self).__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm
        )
        self.eps = eps
        self.lipschitz_constant = lipschitz_constant_limit

        self._epsilon = 1e-2

    def _output_dim_multiplier(self):
        multiplier_orthogonal_matrices = self.features * 2
        multiplier_diagonal_matrix = 1

        multiplier_bias = 1 if self.use_bias else 0

        return multiplier_orthogonal_matrices + multiplier_diagonal_matrix + multiplier_bias

    def _forward_given_params(self, inputs, conditional_params):
        householder_U, diag_entries_S, householder_Vt, bias = self._get_matrices(conditional_params)

        VtX, _ = householder_Vt.forward(inputs)
        SVtX = VtX * diag_entries_S
        USVtX, _ = householder_U.forward(SVtX)
        outputs = USVtX + bias if self.use_bias else USVtX

        logabsdet = diag_entries_S.log().sum(-1)  # |det(SVD)| = product of singular values

        return outputs, logabsdet

    def _inverse_given_params(self, inputs, conditional_params):
        householder_U, diag_entries_S, householder_Vt, bias = self._get_matrices(conditional_params)

        y = inputs - bias if self.use_bias else inputs
        Uty, _ = householder_U.inverse(y)
        SinvUty = Uty / diag_entries_S
        outputs, _ = householder_Vt.inverse(SinvUty)

        logabsdet = - diag_entries_S.log().sum(-1)  # |det(SVD)| = product of singular values

        return outputs.squeeze(), logabsdet

    def _get_matrices(self, conditional_params):
        q_vectors_U, q_vectors_V, diag_entries_S_unconstrained, bias = self._unconstrained_params(
            conditional_params
        )
        householder_U = ParametrizedHouseHolder(q_vectors_U)
        householder_Vt = ParametrizedHouseHolder(q_vectors_V)
        if self.lipschitz_constant is not None:
            diag_entries_S = torch.sigmoid(diag_entries_S_unconstrained) * (
                    self.lipschitz_constant - self.eps) + self.eps
        else:
            diag_entries_S = torch.exp(diag_entries_S_unconstrained) + self.eps

        return householder_U, diag_entries_S, householder_Vt, bias

    def _unconstrained_params(self, conditional_params):
        output_shapes = [self.features ** 2, self.features ** 2, self.features, self.features]

        if self.use_bias:
            q_vectors_U, q_vectors_Vt, diag_entries_S, bias = torch.split(conditional_params, output_shapes, -1)
        else:
            q_vectors_U, q_vectors_Vt, diag_entries_S = torch.split(conditional_params, output_shapes[:-1], -1)
            bias = None

        return q_vectors_U.view(-1, self.features, self.features), q_vectors_Vt.view(-1, self.features,
                                                                                     self.features), diag_entries_S, bias


class ConditionalUMNNTransform(ConditionalTransform):
    &#34;&#34;&#34;An unconstrained monotonic neural networks autoregressive layer that transforms the variables.

        Reference:
        &gt; A. Wehenkel and G. Louppe, Unconstrained Monotonic Neural Networks, NeurIPS2019.

        ---- Specific arguments ----
        integrand_net_layers: the layers dimension to put in the integrand network.
        cond_size: The embedding size for the conditioning factors.
        nb_steps: The number of integration steps.
        solver: The quadrature algorithm - CC or CCParallel. Both implements Clenshaw-Curtis quadrature with
        Leibniz rule for backward computation. CCParallel pass all the evaluation points (nb_steps) at once, it is faster
        but requires more memory.
        &#34;&#34;&#34;

    def __init__(
            self,
            features,
            hidden_features,
            context_features=None,
            num_blocks=2,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
            integrand_net_layers=[50, 50, 50],
            cond_size=20,
            nb_steps=20,
            solver=&#34;CCParallel&#34;,
    ):
        self.cond_size = cond_size

        super().__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm
        )
        self.transformer = MonotonicNormalizer(integrand_net_layers, cond_size, nb_steps, solver)

    def _output_dim_multiplier(self):
        return self.cond_size

    def _forward_given_params(self, inputs, conditional_params):
        z, jac = self.transformer(inputs, conditional_params.reshape(inputs.shape[0], inputs.shape[1], -1))
        log_det_jac = jac.log().sum(1)
        return z, log_det_jac

    def _inverse_given_params(self, inputs, conditional_params):
        x = self.transformer.inverse_transform(inputs,
                                               conditional_params.reshape(inputs.shape[0], inputs.shape[1], -1))
        z, jac = self.transformer(x, conditional_params.reshape(inputs.shape[0], inputs.shape[1], -1))
        log_det_jac = -jac.log().sum(1)
        return x, log_det_jac


class PiecewiseLinearConditionalTransform(ConditionalTransform):
    def __init__(
            self,
            num_bins,
            features,
            hidden_features,
            context_features=None,
            num_blocks=2,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
    ):
        self.num_bins = num_bins

        super().__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm
        )

    def _output_dim_multiplier(self):
        return self.num_bins

    def _elementwise(self, inputs, autoregressive_params, inverse=False):
        batch_size = inputs.shape[0]

        unnormalized_pdf = autoregressive_params.view(
            batch_size, self.features, self._output_dim_multiplier()
        )

        outputs, logabsdet = linear_spline(
            inputs=inputs, unnormalized_pdf=unnormalized_pdf, inverse=inverse,
            left=-4.0, right=4.0, bottom=-4.0, top=4.0
        )

        return outputs, torchutils.sum_except_batch(logabsdet)

    def _forward_given_params(self, inputs, autoregressive_params):
        return self._elementwise(inputs, autoregressive_params)

    def _inverse_given_params(self, inputs, autoregressive_params):
        return self._elementwise(inputs, autoregressive_params, inverse=True)


class ConditionalPiecewiseRationalQuadraticTransform(ConditionalTransform):
    def __init__(
            self,
            features,
            hidden_features,
            context_features=None,
            num_bins=10,
            tails=None,
            tail_bound=1.0,
            num_blocks=2,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
            min_bin_width=rational_quadratic.DEFAULT_MIN_BIN_WIDTH,
            min_bin_height=rational_quadratic.DEFAULT_MIN_BIN_HEIGHT,
            min_derivative=rational_quadratic.DEFAULT_MIN_DERIVATIVE,
    ):
        self.num_bins = num_bins
        self.min_bin_width = min_bin_width
        self.min_bin_height = min_bin_height
        self.min_derivative = min_derivative
        self.tails = tails
        self.tail_bound = tail_bound

        super().__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm
        )

    def _output_dim_multiplier(self):
        if self.tails == &#34;linear&#34;:
            return self.num_bins * 3 - 1
        elif self.tails is None:
            return self.num_bins * 3 + 1
        else:
            raise ValueError

    def _elementwise(self, inputs, autoregressive_params, inverse=False):
        batch_size, features = inputs.shape[0], inputs.shape[1]

        transform_params = autoregressive_params.view(
            batch_size, features, self._output_dim_multiplier()
        )

        unnormalized_widths = transform_params[..., : self.num_bins]
        unnormalized_heights = transform_params[..., self.num_bins: 2 * self.num_bins]
        unnormalized_derivatives = transform_params[..., 2 * self.num_bins:]

        if hasattr(self.conditional_net, &#34;hidden_features&#34;):
            unnormalized_widths /= np.sqrt(self.conditional_net.hidden_features)
            unnormalized_heights /= np.sqrt(self.conditional_net.hidden_features)

        if self.tails is None:
            spline_fn = rational_quadratic_spline
            spline_kwargs = {&#34;left&#34;: -1.2, &#34;right&#34;: 1.2, &#34;bottom&#34;: -1.2, &#34;top&#34;: 1.2}
        elif self.tails == &#34;linear&#34;:
            spline_fn = unconstrained_rational_quadratic_spline
            spline_kwargs = {&#34;tails&#34;: self.tails, &#34;tail_bound&#34;: self.tail_bound}
        else:
            raise ValueError

        outputs, logabsdet = spline_fn(
            inputs=inputs,
            unnormalized_widths=unnormalized_widths,
            unnormalized_heights=unnormalized_heights,
            unnormalized_derivatives=unnormalized_derivatives,
            inverse=inverse,
            min_bin_width=self.min_bin_width,
            min_bin_height=self.min_bin_height,
            min_derivative=self.min_derivative,
            enable_identity_init=True,
            **spline_kwargs
        )

        return outputs, torchutils.sum_except_batch(logabsdet)

    def _forward_given_params(self, inputs, autoregressive_params):
        return self._elementwise(inputs, autoregressive_params)

    def _inverse_given_params(self, inputs, autoregressive_params):
        return self._elementwise(inputs, autoregressive_params, inverse=True)


class ConditionalSumOfSigmoidsTransform(ConditionalTransform):
    def __init__(
            self,
            features,
            hidden_features,
            context_features=None,
            n_sigmoids=10,
            num_blocks=2,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
    ):
        self.n_sigmoids = n_sigmoids
        super().__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm
        )

    def _output_dim_multiplier(self):
        return 3 * self.n_sigmoids + 1

    def _forward_given_params(self, inputs, autoregressive_params):
        transformer = SumOfSigmoids(n_sigmoids=self.n_sigmoids, features=self.features,
                                    raw_params=autoregressive_params.view(inputs.shape[0], self.features,
                                                                          self._output_dim_multiplier()))

        z, logabsdet = transformer(inputs)
        return z, logabsdet

    def _inverse_given_params(self, inputs, autoregressive_params):
        transformer = SumOfSigmoids(n_sigmoids=self.n_sigmoids, features=self.features,
                                    raw_params=autoregressive_params.view(inputs.shape[0], self.features,
                                                                          self._output_dim_multiplier()))
        x, logabsdet = transformer.inverse(inputs)
        return x, logabsdet


class ConditionalPlanarTransform(ConditionalTransform):
    def __init__(
            self,
            features,
            hidden_features,
            context_features,
            num_blocks=2,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
    ):
        super(ConditionalPlanarTransform, self).__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm
        )

        self.softplus = torch.nn.Softplus()
        self.diag_entries = torch.nn.Parameter(torch.eye(features, device=&#34;cuda&#34;), requires_grad=False)

        self.default_pivot = torch.nn.Parameter(torch.arange(1, self.features + 1, dtype=torch.int32).unsqueeze(0),
                                                requires_grad=False)

    def _num_parameters(self):
        return self.features * self._output_dim_multiplier() + self._constant_dim_addition()

    def _output_dim_multiplier(self):
        return 2

    def _constant_dim_addition(self):
        return 1

    def _forward_given_params(self, inputs, conditional_params):
        u_, w_, b_ = self._create_uwb(
            conditional_params
        )
        pre_act = torch.bmm(inputs.view(-1, 1, self.features), torch.transpose(w_, dim0=-2, dim1=-1)).squeeze() + b_.squeeze()
        outputs = inputs + (u_ * torch.tanh(pre_act).view(-1, 1, 1)).squeeze()

        # calc logabsdet
        psi = (1 - torch.tanh(pre_act) ** 2).view(-1, 1, 1) * w_
        abs_det = (1 + torch.bmm(u_, torch.transpose(psi, dim0=-2, dim1=-1))).abs()
        logabsdet = torch.log(1e-7 + abs_det).squeeze()
        return outputs, logabsdet

    def _inverse_given_params(self, inputs, conditional_params):
        raise NotImplementedError()

    def get_u_hat(self, _u, _w):
        &#34;&#34;&#34;Enforce w^T u &gt;= -1. When using h(.) = tanh(.), this is a sufficient condition
        for invertibility of the transformation f(z). See Appendix A.1.
        &#34;&#34;&#34;
        wtu = torch.bmm(_u, torch.transpose(_w, dim0=-2, dim1=-1))

        m_wtu = -1 + torch.log(1 + torch.exp(wtu))
        return _u + (m_wtu - wtu) * _w / torch.norm(_w, p=2, dim=-1, keepdim=True) ** 2
        # self.u.data = (
        #         self.u + (m_wtu - wtu) * self.w / torch.norm(self.w, p=2, dim=1) ** 2
        # )

    def _create_uwb(self, conditional_params):
        _b = conditional_params[..., -1:]
        unconstrained_matrix_vals = conditional_params[..., :-1].view(
            -1, self.features, self._output_dim_multiplier()
        )
        _u, _w = unconstrained_matrix_vals[..., 0], unconstrained_matrix_vals[..., 1]
        _u = _u[:, None, :]
        _w = _w[:, None, :]
        _u = self.get_u_hat(_u, _w)
        return _u, _w, _b


def h(x):
    return torch.tanh(x)


def dh_dx(x):
    return 1 - torch.tanh(x) ** 2


class ConditionalSylvesterTransform(ConditionalTransform):
    def __init__(
            self,
            features: int,
            hidden_features: int,
            context_features: int,
            num_blocks=2,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
            eps=1e-3
    ):

        self.eps = eps

        self.features = features
        self.__output_splits = self._output_splits()
        super(ConditionalSylvesterTransform, self).__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm
        )
        self.features = torch.tensor(features)
        self.features = torch.nn.Parameter(self.features, requires_grad=False)
        self.reverse_idx = torch.nn.Parameter(torch.arange(self.features - 1, -1, -1), requires_grad=False)
        self.triu_mask = torch.nn.Parameter(
            torch.triu(torch.ones(self.features, self.features).unsqueeze(0), diagonal=1), requires_grad=False)
        self.identity = torch.nn.Parameter(torch.eye(features, features).unsqueeze(0), requires_grad=False)

    def _output_splits(self) -&gt; List[int]:
        # R1, R2
        # splits = [self.n_diag_entries + self.n_triangular_entries] * 2
        splits = [self.features ** 2, self.features]
        # Q via ParameterizedHouseholder
        splits += [self.features ** 2]
        # b
        splits += [self.features]

        # R1_diag, R1_triu, R2_diag, R2_triu, q_unconstrained, b
        return splits

    # def _num_parameters(self):

    def _num_parameters(self):
        return sum(self.__output_splits)

    def _forward_given_params(self, inputs, conditional_params):
        R1, R2, Q, bias = self._create_mats(
            conditional_params
        )

        logabsdet, outputs = self.forward_jit(Q, R1, R2, bias, inputs)
        return outputs, logabsdet

    @staticmethod
    @torch.jit.script
    def forward_jit(orth_Q: torch.Tensor, triu_R1: torch.Tensor, triu_R2: torch.Tensor, bias: torch.Tensor,
                    inputs: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        Qtz = torch.transpose(orth_Q, dim0=-2, dim1=-1) @ inputs.unsqueeze(-1)
        # Qtz = Q_orthogonal.inverse(inputs)[0].unsqueeze(-1)  # (n, d, 1)
        RQtz = triu_R1 @ Qtz  # (n, d, 1)
        preact = RQtz + bias
        # QR2act = Q_orthogonal.forward((R2 @ h(preact)).squeeze())[0]
        QR2act = orth_Q @ (triu_R2 @ h(preact))
        outputs = inputs + QR2act.squeeze()
        # calc logabsdet
        deriv_act = dh_dx(preact).squeeze()
        R_sq_diag = (torch.diagonal(triu_R1, dim1=-2, dim2=-1) * torch.diagonal(triu_R2, dim1=-2,
                                                                                dim2=-1)).squeeze()  # (n, d)
        diag = R_sq_diag.new_ones(inputs.shape[-1]) + deriv_act * R_sq_diag
        logabsdet = torch.log(diag).sum(-1)
        return logabsdet, outputs

    def _inverse_given_params(self, inputs, conditional_params):
        raise NotImplementedError()

    def _create_mats(self, conditional_params) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        R_full, R2_diag, q_unconstrained, b = torch.split(conditional_params, self.__output_splits, -1)

        R1, R2 = self._create_upper(R_full.view(-1, self.features, self.features), R2_diag, self.triu_mask)
        b: torch.Tensor = b.view(-1, self.features, 1)
        q_vectors: torch.Tensor = q_unconstrained.view(-1, self.features, self.features)

        # Q_householder = ParametrizedHouseHolder(q_vectors=q_vectors)
        # Q = Q_householder.matrix()
        Q = self.build_orthogonal_matrix(q_vectors)
        return R1, R2, Q, b

    def build_orthogonal_matrix(self, q_vectors: torch.Tensor):
        # identity = torch.repeat_interleave(identity[None, ...], conditional_params.shape[0], 0)
        outputs1 = _apply_batchwise_transforms_nodet(self.identity[:, 0, :], q_vectors[:, self.reverse_idx, :])
        outputs2 = _apply_batchwise_transforms_nodet(self.identity[:, 1, :], q_vectors[:, self.reverse_idx, :])
        Q = torch.stack([outputs1, outputs2], 1)
        return Q

    @staticmethod
    @torch.jit.script
    def _create_upper(full_matr_r, diag_vals, triu_mask) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        masked_1 = full_matr_r * triu_mask
        masked_2 = torch.transpose(full_matr_r, dim0=-2, dim1=-1) * triu_mask

        diag_1 = torch.diag_embed(torch.tanh(torch.diagonal(full_matr_r, dim1=-2, dim2=-1)), dim1=-2, dim2=-1)
        diag_2 = torch.diag_embed(torch.tanh(diag_vals), dim1=-2, dim2=-1)

        R1 = masked_1 + diag_1
        R2 = masked_2 + diag_2

        return R1, R2</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="flowcon.transforms.conditional.dh_dx"><code class="name flex">
<span>def <span class="ident">dh_dx</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dh_dx(x):
    return 1 - torch.tanh(x) ** 2</code></pre>
</details>
</dd>
<dt id="flowcon.transforms.conditional.h"><code class="name flex">
<span>def <span class="ident">h</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def h(x):
    return torch.tanh(x)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="flowcon.transforms.conditional.AffineConditionalTransform"><code class="flex name class">
<span>class <span class="ident">AffineConditionalTransform</span></span>
<span>(</span><span>features, hidden_features, context_features, num_blocks=2, use_residual_blocks=True, activation=&lt;function relu&gt;, dropout_probability=0.0, use_batch_norm=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms each input variable with an invertible transformation, conditioned on some given input.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AffineConditionalTransform(ConditionalTransform):
    def __init__(
            self,
            features,
            hidden_features,
            context_features,
            num_blocks=2,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
    ):
        super(AffineConditionalTransform, self).__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm
        )

    def _output_dim_multiplier(self):
        return 2

    def _forward_given_params(self, inputs, conditional_params):
        unconstrained_scale, shift = self._unconstrained_scale_and_shift(
            conditional_params
        )
        # scale = torch.sigmoid(unconstrained_scale + 2.0) + self._epsilon
        scale = F.softplus(unconstrained_scale) + self._epsilon
        log_scale = torch.log(scale)
        outputs = scale * inputs + shift
        logabsdet = torchutils.sum_except_batch(log_scale, num_batch_dims=1)
        return outputs, logabsdet

    def _inverse_given_params(self, inputs, conditional_params):
        unconstrained_scale, shift = self._unconstrained_scale_and_shift(
            conditional_params
        )
        # scale = torch.sigmoid(unconstrained_scale + 2.0) + self._epsilon
        scale = F.softplus(unconstrained_scale) + self._epsilon
        log_scale = torch.log(scale)
        outputs = (inputs - shift) / scale
        logabsdet = -torchutils.sum_except_batch(log_scale, num_batch_dims=1)
        return outputs, logabsdet

    def _unconstrained_scale_and_shift(self, conditional_params):
        conditional_params = conditional_params.view(
            -1, self.features, self._output_dim_multiplier()
        )
        unconstrained_scale = conditional_params[..., 0]
        shift = conditional_params[..., 1]
        return unconstrained_scale, shift</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.conditional.AffineConditionalTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.AffineConditionalTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.AffineConditionalTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.conditional.ConditionalTransform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalLUTransform"><code class="flex name class">
<span>class <span class="ident">ConditionalLUTransform</span></span>
<span>(</span><span>features, hidden_features, context_features, num_blocks=2, use_residual_blocks=True, activation=&lt;function relu&gt;, dropout_probability=0.0, use_batch_norm=False, eps=1e-07)</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms each input variable with an invertible transformation, conditioned on some given input.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConditionalLUTransform(ConditionalTransform):
    def __init__(
            self,
            features,
            hidden_features,
            context_features,
            num_blocks=2,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
            eps=1e-7
    ):
        super(ConditionalLUTransform, self).__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm
        )

        self.eps = eps

        self.lower_indices = np.tril_indices(features, k=-1)
        self.upper_indices = np.triu_indices(features, k=1)
        self.diag_indices = np.diag_indices(features)

        self.softplus = torch.nn.Softplus()
        self.diag_entries = torch.nn.Parameter(torch.eye(features), requires_grad=False)
        self.default_pivot = torch.nn.Parameter(torch.arange(1, self.features + 1, dtype=torch.int32).unsqueeze(0),
                                                requires_grad=False)
        self.scale_non_diag = torch.nn.Parameter(- 2 * torch.ones(()), requires_grad=True)

    def _output_dim_multiplier(self):
        return self.features

    def _forward_given_params(self, inputs, conditional_params):
        lower, upper = self._create_lower_upper(
            conditional_params
        )
        outputs = upper @ inputs.unsqueeze(-1)
        outputs = (lower @ outputs).view(inputs.shape)
        logabsdet = upper.diagonal(0, -1, -2).log().sum(-1)  # ...#inputs.new_ones(inputs.shape[0])
        return outputs, logabsdet

    def _inverse_given_params(self, inputs, conditional_params):
        lower, upper = self._create_lower_upper(
            conditional_params
        )
        outputs = torch.linalg.lu_solve(torch.tril(lower, -1) + upper,
                                        torch.broadcast_to(self.default_pivot, inputs.shape),
                                        inputs.unsqueeze(-1))

        logabsdet = -upper.diagonal(0, -1, -2).log().sum(-1)  # ...#inputs.new_ones(inputs.shape[0])
        return outputs.view(inputs.shape), logabsdet

    def _unconstrained_entries(self, conditional_params):
        conditional_params = conditional_params.view(
            -1, self.features, self._output_dim_multiplier()
        )
        return conditional_params

    def _create_lower_upper(self, conditional_params):
        unconstrained_matrix_vals = self._unconstrained_entries(conditional_params)

        lower = torch.nn.functional.softplus(self.scale_non_diag) * torch.tril(unconstrained_matrix_vals, diagonal=-1) + self.diag_entries
        upper_diag = torch.diag_embed(self.softplus(unconstrained_matrix_vals.diagonal(0, -1, -2)) + self.eps)
        upper = torch.nn.functional.softplus(self.scale_non_diag) * torch.triu(unconstrained_matrix_vals, diagonal=1) + upper_diag
        return lower, upper</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.conditional.ConditionalLUTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalLUTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalLUTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.conditional.ConditionalTransform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalOrthogonalTransform"><code class="flex name class">
<span>class <span class="ident">ConditionalOrthogonalTransform</span></span>
<span>(</span><span>features, hidden_features, context_features, num_blocks=2, use_residual_blocks=True, activation=&lt;function relu&gt;, dropout_probability=0.0, use_batch_norm=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms each input variable with an invertible transformation, conditioned on some given input.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConditionalOrthogonalTransform(ConditionalTransform):
    def __init__(
            self,
            features,
            hidden_features,
            context_features,
            num_blocks=2,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
    ):
        super(ConditionalOrthogonalTransform, self).__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm
        )

    def _output_dim_multiplier(self):
        return self.features

    def _forward_given_params(self, inputs, conditional_params):
        householder = self._get_matrices(conditional_params)
        outputs, logabsdet = householder.forward(inputs)
        return outputs, logabsdet

    def _inverse_given_params(self, inputs, conditional_params):
        householder = self._get_matrices(conditional_params)
        outputs, logabsdet = householder.inverse(inputs)

        return outputs.squeeze(), logabsdet

    def _get_matrices(self, conditional_params) -&gt; ParametrizedHouseHolder:
        q_vectors = self._unconstrained_entries(
            conditional_params
        )
        householder = ParametrizedHouseHolder(q_vectors)
        return householder

    def _unconstrained_entries(self, conditional_params):
        conditional_params = conditional_params.view(
            -1, self.features, self._output_dim_multiplier()
        )
        return conditional_params</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.conditional.ConditionalOrthogonalTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalOrthogonalTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalOrthogonalTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.conditional.ConditionalTransform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalPiecewiseRationalQuadraticTransform"><code class="flex name class">
<span>class <span class="ident">ConditionalPiecewiseRationalQuadraticTransform</span></span>
<span>(</span><span>features, hidden_features, context_features=None, num_bins=10, tails=None, tail_bound=1.0, num_blocks=2, use_residual_blocks=True, activation=&lt;function relu&gt;, dropout_probability=0.0, use_batch_norm=False, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001)</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms each input variable with an invertible transformation, conditioned on some given input.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConditionalPiecewiseRationalQuadraticTransform(ConditionalTransform):
    def __init__(
            self,
            features,
            hidden_features,
            context_features=None,
            num_bins=10,
            tails=None,
            tail_bound=1.0,
            num_blocks=2,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
            min_bin_width=rational_quadratic.DEFAULT_MIN_BIN_WIDTH,
            min_bin_height=rational_quadratic.DEFAULT_MIN_BIN_HEIGHT,
            min_derivative=rational_quadratic.DEFAULT_MIN_DERIVATIVE,
    ):
        self.num_bins = num_bins
        self.min_bin_width = min_bin_width
        self.min_bin_height = min_bin_height
        self.min_derivative = min_derivative
        self.tails = tails
        self.tail_bound = tail_bound

        super().__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm
        )

    def _output_dim_multiplier(self):
        if self.tails == &#34;linear&#34;:
            return self.num_bins * 3 - 1
        elif self.tails is None:
            return self.num_bins * 3 + 1
        else:
            raise ValueError

    def _elementwise(self, inputs, autoregressive_params, inverse=False):
        batch_size, features = inputs.shape[0], inputs.shape[1]

        transform_params = autoregressive_params.view(
            batch_size, features, self._output_dim_multiplier()
        )

        unnormalized_widths = transform_params[..., : self.num_bins]
        unnormalized_heights = transform_params[..., self.num_bins: 2 * self.num_bins]
        unnormalized_derivatives = transform_params[..., 2 * self.num_bins:]

        if hasattr(self.conditional_net, &#34;hidden_features&#34;):
            unnormalized_widths /= np.sqrt(self.conditional_net.hidden_features)
            unnormalized_heights /= np.sqrt(self.conditional_net.hidden_features)

        if self.tails is None:
            spline_fn = rational_quadratic_spline
            spline_kwargs = {&#34;left&#34;: -1.2, &#34;right&#34;: 1.2, &#34;bottom&#34;: -1.2, &#34;top&#34;: 1.2}
        elif self.tails == &#34;linear&#34;:
            spline_fn = unconstrained_rational_quadratic_spline
            spline_kwargs = {&#34;tails&#34;: self.tails, &#34;tail_bound&#34;: self.tail_bound}
        else:
            raise ValueError

        outputs, logabsdet = spline_fn(
            inputs=inputs,
            unnormalized_widths=unnormalized_widths,
            unnormalized_heights=unnormalized_heights,
            unnormalized_derivatives=unnormalized_derivatives,
            inverse=inverse,
            min_bin_width=self.min_bin_width,
            min_bin_height=self.min_bin_height,
            min_derivative=self.min_derivative,
            enable_identity_init=True,
            **spline_kwargs
        )

        return outputs, torchutils.sum_except_batch(logabsdet)

    def _forward_given_params(self, inputs, autoregressive_params):
        return self._elementwise(inputs, autoregressive_params)

    def _inverse_given_params(self, inputs, autoregressive_params):
        return self._elementwise(inputs, autoregressive_params, inverse=True)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.conditional.ConditionalPiecewiseRationalQuadraticTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalPiecewiseRationalQuadraticTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalPiecewiseRationalQuadraticTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.conditional.ConditionalTransform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalPlanarTransform"><code class="flex name class">
<span>class <span class="ident">ConditionalPlanarTransform</span></span>
<span>(</span><span>features, hidden_features, context_features, num_blocks=2, use_residual_blocks=True, activation=&lt;function relu&gt;, dropout_probability=0.0, use_batch_norm=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms each input variable with an invertible transformation, conditioned on some given input.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConditionalPlanarTransform(ConditionalTransform):
    def __init__(
            self,
            features,
            hidden_features,
            context_features,
            num_blocks=2,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
    ):
        super(ConditionalPlanarTransform, self).__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm
        )

        self.softplus = torch.nn.Softplus()
        self.diag_entries = torch.nn.Parameter(torch.eye(features, device=&#34;cuda&#34;), requires_grad=False)

        self.default_pivot = torch.nn.Parameter(torch.arange(1, self.features + 1, dtype=torch.int32).unsqueeze(0),
                                                requires_grad=False)

    def _num_parameters(self):
        return self.features * self._output_dim_multiplier() + self._constant_dim_addition()

    def _output_dim_multiplier(self):
        return 2

    def _constant_dim_addition(self):
        return 1

    def _forward_given_params(self, inputs, conditional_params):
        u_, w_, b_ = self._create_uwb(
            conditional_params
        )
        pre_act = torch.bmm(inputs.view(-1, 1, self.features), torch.transpose(w_, dim0=-2, dim1=-1)).squeeze() + b_.squeeze()
        outputs = inputs + (u_ * torch.tanh(pre_act).view(-1, 1, 1)).squeeze()

        # calc logabsdet
        psi = (1 - torch.tanh(pre_act) ** 2).view(-1, 1, 1) * w_
        abs_det = (1 + torch.bmm(u_, torch.transpose(psi, dim0=-2, dim1=-1))).abs()
        logabsdet = torch.log(1e-7 + abs_det).squeeze()
        return outputs, logabsdet

    def _inverse_given_params(self, inputs, conditional_params):
        raise NotImplementedError()

    def get_u_hat(self, _u, _w):
        &#34;&#34;&#34;Enforce w^T u &gt;= -1. When using h(.) = tanh(.), this is a sufficient condition
        for invertibility of the transformation f(z). See Appendix A.1.
        &#34;&#34;&#34;
        wtu = torch.bmm(_u, torch.transpose(_w, dim0=-2, dim1=-1))

        m_wtu = -1 + torch.log(1 + torch.exp(wtu))
        return _u + (m_wtu - wtu) * _w / torch.norm(_w, p=2, dim=-1, keepdim=True) ** 2
        # self.u.data = (
        #         self.u + (m_wtu - wtu) * self.w / torch.norm(self.w, p=2, dim=1) ** 2
        # )

    def _create_uwb(self, conditional_params):
        _b = conditional_params[..., -1:]
        unconstrained_matrix_vals = conditional_params[..., :-1].view(
            -1, self.features, self._output_dim_multiplier()
        )
        _u, _w = unconstrained_matrix_vals[..., 0], unconstrained_matrix_vals[..., 1]
        _u = _u[:, None, :]
        _w = _w[:, None, :]
        _u = self.get_u_hat(_u, _w)
        return _u, _w, _b</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.conditional.ConditionalPlanarTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalPlanarTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalPlanarTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.conditional.ConditionalPlanarTransform.get_u_hat"><code class="name flex">
<span>def <span class="ident">get_u_hat</span></span>(<span>self, _u, _w)</span>
</code></dt>
<dd>
<div class="desc"><p>Enforce w^T u &gt;= -1. When using h(.) = tanh(.), this is a sufficient condition
for invertibility of the transformation f(z). See Appendix A.1.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_u_hat(self, _u, _w):
    &#34;&#34;&#34;Enforce w^T u &gt;= -1. When using h(.) = tanh(.), this is a sufficient condition
    for invertibility of the transformation f(z). See Appendix A.1.
    &#34;&#34;&#34;
    wtu = torch.bmm(_u, torch.transpose(_w, dim0=-2, dim1=-1))

    m_wtu = -1 + torch.log(1 + torch.exp(wtu))
    return _u + (m_wtu - wtu) * _w / torch.norm(_w, p=2, dim=-1, keepdim=True) ** 2
    # self.u.data = (
    #         self.u + (m_wtu - wtu) * self.w / torch.norm(self.w, p=2, dim=1) ** 2
    # )</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.conditional.ConditionalTransform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalRotationTransform"><code class="flex name class">
<span>class <span class="ident">ConditionalRotationTransform</span></span>
<span>(</span><span>features, hidden_features, context_features, num_blocks=1, use_residual_blocks=True, activation=&lt;function relu&gt;, dropout_probability=0.0, use_batch_norm=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms each input variable with an invertible transformation, conditioned on some given input.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConditionalRotationTransform(ConditionalTransform):
    def _output_dim_multiplier(self):
        pass

    def __init__(
            self,
            features,
            hidden_features,
            context_features,
            num_blocks=1,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
    ):
        assert features == 2, &#34;Only available for 2D rotations.&#34;

        super(ConditionalRotationTransform, self).__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm
        )

    def _num_parameters(self):
        return 1

    def build_matrix(self, conditional_params):
        theta = conditional_params
        m1 = torch.cos(theta)
        m2 = -torch.sin(theta)
        m3 = -m2
        m4 = m1
        matrix = torch.concatenate([m1, m2, m3, m4], -1).view(-1, self.features, self.features)
        return matrix

    def _forward_given_params(self, inputs: torch.Tensor, conditional_params):
        matrix = self.build_matrix(conditional_params)

        outputs = (matrix @ inputs.unsqueeze(-1)).squeeze()
        logabsdet = inputs.new_zeros(inputs.shape[0])  # ...#inputs.new_ones(inputs.shape[0])
        return outputs, logabsdet

    def _inverse_given_params(self, inputs, conditional_params):
        matrix = self.build_matrix(conditional_params)

        outputs = (torch.transpose(matrix, dim0=-2, dim1=-1) @ inputs.unsqueeze(-1)).squeeze()
        logabsdet = inputs.new_zeros(inputs.shape[0])  # ...#inputs.new_ones(inputs.shape[0])
        return outputs, logabsdet</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.conditional.ConditionalRotationTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalRotationTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalRotationTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.conditional.ConditionalRotationTransform.build_matrix"><code class="name flex">
<span>def <span class="ident">build_matrix</span></span>(<span>self, conditional_params)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_matrix(self, conditional_params):
    theta = conditional_params
    m1 = torch.cos(theta)
    m2 = -torch.sin(theta)
    m3 = -m2
    m4 = m1
    matrix = torch.concatenate([m1, m2, m3, m4], -1).view(-1, self.features, self.features)
    return matrix</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.conditional.ConditionalTransform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalSVDTransform"><code class="flex name class">
<span>class <span class="ident">ConditionalSVDTransform</span></span>
<span>(</span><span>features, hidden_features, context_features, use_bias=True, num_blocks=2, use_residual_blocks=True, activation=&lt;function relu&gt;, dropout_probability=0.0, use_batch_norm=False, eps=0.001, lipschitz_constant_limit=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms each input variable with an invertible transformation, conditioned on some given input.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConditionalSVDTransform(ConditionalTransform):
    def __init__(
            self,
            features,
            hidden_features,
            context_features,
            use_bias=True,
            num_blocks=2,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
            eps=1e-3,
            lipschitz_constant_limit=None
    ):
        self.use_bias = use_bias

        super(ConditionalSVDTransform, self).__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm
        )
        self.eps = eps
        self.lipschitz_constant = lipschitz_constant_limit

        self._epsilon = 1e-2

    def _output_dim_multiplier(self):
        multiplier_orthogonal_matrices = self.features * 2
        multiplier_diagonal_matrix = 1

        multiplier_bias = 1 if self.use_bias else 0

        return multiplier_orthogonal_matrices + multiplier_diagonal_matrix + multiplier_bias

    def _forward_given_params(self, inputs, conditional_params):
        householder_U, diag_entries_S, householder_Vt, bias = self._get_matrices(conditional_params)

        VtX, _ = householder_Vt.forward(inputs)
        SVtX = VtX * diag_entries_S
        USVtX, _ = householder_U.forward(SVtX)
        outputs = USVtX + bias if self.use_bias else USVtX

        logabsdet = diag_entries_S.log().sum(-1)  # |det(SVD)| = product of singular values

        return outputs, logabsdet

    def _inverse_given_params(self, inputs, conditional_params):
        householder_U, diag_entries_S, householder_Vt, bias = self._get_matrices(conditional_params)

        y = inputs - bias if self.use_bias else inputs
        Uty, _ = householder_U.inverse(y)
        SinvUty = Uty / diag_entries_S
        outputs, _ = householder_Vt.inverse(SinvUty)

        logabsdet = - diag_entries_S.log().sum(-1)  # |det(SVD)| = product of singular values

        return outputs.squeeze(), logabsdet

    def _get_matrices(self, conditional_params):
        q_vectors_U, q_vectors_V, diag_entries_S_unconstrained, bias = self._unconstrained_params(
            conditional_params
        )
        householder_U = ParametrizedHouseHolder(q_vectors_U)
        householder_Vt = ParametrizedHouseHolder(q_vectors_V)
        if self.lipschitz_constant is not None:
            diag_entries_S = torch.sigmoid(diag_entries_S_unconstrained) * (
                    self.lipschitz_constant - self.eps) + self.eps
        else:
            diag_entries_S = torch.exp(diag_entries_S_unconstrained) + self.eps

        return householder_U, diag_entries_S, householder_Vt, bias

    def _unconstrained_params(self, conditional_params):
        output_shapes = [self.features ** 2, self.features ** 2, self.features, self.features]

        if self.use_bias:
            q_vectors_U, q_vectors_Vt, diag_entries_S, bias = torch.split(conditional_params, output_shapes, -1)
        else:
            q_vectors_U, q_vectors_Vt, diag_entries_S = torch.split(conditional_params, output_shapes[:-1], -1)
            bias = None

        return q_vectors_U.view(-1, self.features, self.features), q_vectors_Vt.view(-1, self.features,
                                                                                     self.features), diag_entries_S, bias</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.conditional.ConditionalSVDTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalSVDTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalSVDTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.conditional.ConditionalTransform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalScaleTransform"><code class="flex name class">
<span>class <span class="ident">ConditionalScaleTransform</span></span>
<span>(</span><span>features, hidden_features, context_features, num_blocks=2, use_residual_blocks=True, activation=&lt;function relu&gt;, dropout_probability=0.0, use_batch_norm=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms each input variable with an invertible transformation, conditioned on some given input.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConditionalScaleTransform(ConditionalTransform):
    def __init__(
            self,
            features,
            hidden_features,
            context_features,
            num_blocks=2,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
            **kwargs
    ):
        self.features = features
        super(ConditionalScaleTransform, self).__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm,
            **kwargs
        )
        self.eps = 1e-5

    def _output_dim_multiplier(self):
        return 1


    def _forward_given_params(self, inputs, conditional_params):
        scale = self._constrained_scale(
            conditional_params
        )
        outputs = inputs * scale.view(inputs.shape)
        logabsdet = torch.log(scale).sum(-1).view(inputs.shape[0])

        return outputs, logabsdet

    def _inverse_given_params(self, inputs, conditional_params):
        scale = self._constrained_scale(
            conditional_params
        )
        outputs = inputs / scale.view(inputs.shape)
        logabsdet = -torch.log(scale).sum(-1).view(inputs.shape[0])

        return outputs, logabsdet

    def _constrained_scale(self, conditional_params):
        # split_idx = autoregressive_params.size(1) // 2
        # unconstrained_scale = autoregressive_params[..., :split_idx]
        # shift = autoregressive_params[..., split_idx:]
        # return unconstrained_scale, shift
        conditional_params = conditional_params.view(
            -1, self.features
        )
        scale = torch.nn.functional.softplus(conditional_params) + self.eps
        return scale</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.conditional.ConditionalScaleTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalScaleTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalScaleTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.conditional.ConditionalTransform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalShiftTransform"><code class="flex name class">
<span>class <span class="ident">ConditionalShiftTransform</span></span>
<span>(</span><span>features, hidden_features, context_features, num_blocks=2, use_residual_blocks=True, activation=&lt;function relu&gt;, dropout_probability=0.0, use_batch_norm=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms each input variable with an invertible transformation, conditioned on some given input.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConditionalShiftTransform(ConditionalTransform):
    def     __init__(
            self,
            features,
            hidden_features,
            context_features,
            num_blocks=2,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
            **kwargs
    ):
        self.features = features
        super(ConditionalShiftTransform, self).__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm,
            **kwargs
        )

    def _output_dim_multiplier(self):
        return 1

    def _forward_given_params(self, inputs, conditional_params):
        shift = self._unconstrained_shift(
            conditional_params
        )
        # scale = torch.sigmoid(unconstrained_scale + 2.0) + self._epsilon
        outputs = inputs + shift.view(inputs.shape)
        logabsdet = inputs.new_zeros(inputs.shape[0])
        return outputs, logabsdet

    def _inverse_given_params(self, inputs, conditional_params):
        shift = self._unconstrained_shift(
            conditional_params
        )
        # scale = torch.sigmoid(unconstrained_scale + 2.0) + self._epsilon
        outputs = (inputs - shift.view(inputs.shape))
        logabsdet = inputs.new_zeros(inputs.shape[0])
        return outputs, logabsdet

    def _unconstrained_shift(self, conditional_params):
        # split_idx = autoregressive_params.size(1) // 2
        # unconstrained_scale = autoregressive_params[..., :split_idx]
        # shift = autoregressive_params[..., split_idx:]
        # return unconstrained_scale, shift
        conditional_params = conditional_params.view(
            -1, self.features
        )
        shift = conditional_params  # [..., None]
        return shift</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.conditional.ConditionalShiftTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalShiftTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalShiftTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.conditional.ConditionalTransform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalSumOfSigmoidsTransform"><code class="flex name class">
<span>class <span class="ident">ConditionalSumOfSigmoidsTransform</span></span>
<span>(</span><span>features, hidden_features, context_features=None, n_sigmoids=10, num_blocks=2, use_residual_blocks=True, activation=&lt;function relu&gt;, dropout_probability=0.0, use_batch_norm=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms each input variable with an invertible transformation, conditioned on some given input.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConditionalSumOfSigmoidsTransform(ConditionalTransform):
    def __init__(
            self,
            features,
            hidden_features,
            context_features=None,
            n_sigmoids=10,
            num_blocks=2,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
    ):
        self.n_sigmoids = n_sigmoids
        super().__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm
        )

    def _output_dim_multiplier(self):
        return 3 * self.n_sigmoids + 1

    def _forward_given_params(self, inputs, autoregressive_params):
        transformer = SumOfSigmoids(n_sigmoids=self.n_sigmoids, features=self.features,
                                    raw_params=autoregressive_params.view(inputs.shape[0], self.features,
                                                                          self._output_dim_multiplier()))

        z, logabsdet = transformer(inputs)
        return z, logabsdet

    def _inverse_given_params(self, inputs, autoregressive_params):
        transformer = SumOfSigmoids(n_sigmoids=self.n_sigmoids, features=self.features,
                                    raw_params=autoregressive_params.view(inputs.shape[0], self.features,
                                                                          self._output_dim_multiplier()))
        x, logabsdet = transformer.inverse(inputs)
        return x, logabsdet</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.conditional.ConditionalSumOfSigmoidsTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalSumOfSigmoidsTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalSumOfSigmoidsTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.conditional.ConditionalTransform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalSylvesterTransform"><code class="flex name class">
<span>class <span class="ident">ConditionalSylvesterTransform</span></span>
<span>(</span><span>features: int, hidden_features: int, context_features: int, num_blocks=2, use_residual_blocks=True, activation=&lt;function relu&gt;, dropout_probability=0.0, use_batch_norm=False, eps=0.001)</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms each input variable with an invertible transformation, conditioned on some given input.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConditionalSylvesterTransform(ConditionalTransform):
    def __init__(
            self,
            features: int,
            hidden_features: int,
            context_features: int,
            num_blocks=2,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
            eps=1e-3
    ):

        self.eps = eps

        self.features = features
        self.__output_splits = self._output_splits()
        super(ConditionalSylvesterTransform, self).__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm
        )
        self.features = torch.tensor(features)
        self.features = torch.nn.Parameter(self.features, requires_grad=False)
        self.reverse_idx = torch.nn.Parameter(torch.arange(self.features - 1, -1, -1), requires_grad=False)
        self.triu_mask = torch.nn.Parameter(
            torch.triu(torch.ones(self.features, self.features).unsqueeze(0), diagonal=1), requires_grad=False)
        self.identity = torch.nn.Parameter(torch.eye(features, features).unsqueeze(0), requires_grad=False)

    def _output_splits(self) -&gt; List[int]:
        # R1, R2
        # splits = [self.n_diag_entries + self.n_triangular_entries] * 2
        splits = [self.features ** 2, self.features]
        # Q via ParameterizedHouseholder
        splits += [self.features ** 2]
        # b
        splits += [self.features]

        # R1_diag, R1_triu, R2_diag, R2_triu, q_unconstrained, b
        return splits

    # def _num_parameters(self):

    def _num_parameters(self):
        return sum(self.__output_splits)

    def _forward_given_params(self, inputs, conditional_params):
        R1, R2, Q, bias = self._create_mats(
            conditional_params
        )

        logabsdet, outputs = self.forward_jit(Q, R1, R2, bias, inputs)
        return outputs, logabsdet

    @staticmethod
    @torch.jit.script
    def forward_jit(orth_Q: torch.Tensor, triu_R1: torch.Tensor, triu_R2: torch.Tensor, bias: torch.Tensor,
                    inputs: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        Qtz = torch.transpose(orth_Q, dim0=-2, dim1=-1) @ inputs.unsqueeze(-1)
        # Qtz = Q_orthogonal.inverse(inputs)[0].unsqueeze(-1)  # (n, d, 1)
        RQtz = triu_R1 @ Qtz  # (n, d, 1)
        preact = RQtz + bias
        # QR2act = Q_orthogonal.forward((R2 @ h(preact)).squeeze())[0]
        QR2act = orth_Q @ (triu_R2 @ h(preact))
        outputs = inputs + QR2act.squeeze()
        # calc logabsdet
        deriv_act = dh_dx(preact).squeeze()
        R_sq_diag = (torch.diagonal(triu_R1, dim1=-2, dim2=-1) * torch.diagonal(triu_R2, dim1=-2,
                                                                                dim2=-1)).squeeze()  # (n, d)
        diag = R_sq_diag.new_ones(inputs.shape[-1]) + deriv_act * R_sq_diag
        logabsdet = torch.log(diag).sum(-1)
        return logabsdet, outputs

    def _inverse_given_params(self, inputs, conditional_params):
        raise NotImplementedError()

    def _create_mats(self, conditional_params) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        R_full, R2_diag, q_unconstrained, b = torch.split(conditional_params, self.__output_splits, -1)

        R1, R2 = self._create_upper(R_full.view(-1, self.features, self.features), R2_diag, self.triu_mask)
        b: torch.Tensor = b.view(-1, self.features, 1)
        q_vectors: torch.Tensor = q_unconstrained.view(-1, self.features, self.features)

        # Q_householder = ParametrizedHouseHolder(q_vectors=q_vectors)
        # Q = Q_householder.matrix()
        Q = self.build_orthogonal_matrix(q_vectors)
        return R1, R2, Q, b

    def build_orthogonal_matrix(self, q_vectors: torch.Tensor):
        # identity = torch.repeat_interleave(identity[None, ...], conditional_params.shape[0], 0)
        outputs1 = _apply_batchwise_transforms_nodet(self.identity[:, 0, :], q_vectors[:, self.reverse_idx, :])
        outputs2 = _apply_batchwise_transforms_nodet(self.identity[:, 1, :], q_vectors[:, self.reverse_idx, :])
        Q = torch.stack([outputs1, outputs2], 1)
        return Q

    @staticmethod
    @torch.jit.script
    def _create_upper(full_matr_r, diag_vals, triu_mask) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        masked_1 = full_matr_r * triu_mask
        masked_2 = torch.transpose(full_matr_r, dim0=-2, dim1=-1) * triu_mask

        diag_1 = torch.diag_embed(torch.tanh(torch.diagonal(full_matr_r, dim1=-2, dim2=-1)), dim1=-2, dim2=-1)
        diag_2 = torch.diag_embed(torch.tanh(diag_vals), dim1=-2, dim2=-1)

        R1 = masked_1 + diag_1
        R2 = masked_2 + diag_2

        return R1, R2</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.conditional.ConditionalSylvesterTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalSylvesterTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalSylvesterTransform.forward_jit"><code class="name">var <span class="ident">forward_jit</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalSylvesterTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.conditional.ConditionalSylvesterTransform.build_orthogonal_matrix"><code class="name flex">
<span>def <span class="ident">build_orthogonal_matrix</span></span>(<span>self, q_vectors: torch.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_orthogonal_matrix(self, q_vectors: torch.Tensor):
    # identity = torch.repeat_interleave(identity[None, ...], conditional_params.shape[0], 0)
    outputs1 = _apply_batchwise_transforms_nodet(self.identity[:, 0, :], q_vectors[:, self.reverse_idx, :])
    outputs2 = _apply_batchwise_transforms_nodet(self.identity[:, 1, :], q_vectors[:, self.reverse_idx, :])
    Q = torch.stack([outputs1, outputs2], 1)
    return Q</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.conditional.ConditionalTransform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalTransform"><code class="flex name class">
<span>class <span class="ident">ConditionalTransform</span></span>
<span>(</span><span>features, hidden_features=64, context_features=1, num_blocks=2, use_residual_blocks=True, activation=&lt;function relu&gt;, dropout_probability=0.0, use_batch_norm=False, conditional_net: torch.nn.modules.module.Module = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms each input variable with an invertible transformation, conditioned on some given input.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConditionalTransform(Transform):
    &#34;&#34;&#34;Transforms each input variable with an invertible transformation, conditioned on some given input.
    &#34;&#34;&#34;

    def __init__(self,
                 features,
                 hidden_features=64,
                 context_features=1,
                 num_blocks=2,
                 use_residual_blocks=True,
                 activation=F.relu,
                 dropout_probability=0.0,
                 use_batch_norm=False,
                 conditional_net: torch.nn.Module = None, ):

        super(ConditionalTransform, self).__init__()
        self.features = features

        if conditional_net is not None:
            assert isinstance(conditional_net, torch.nn.Module)
            self.conditional_net = conditional_net
        else:

            self.conditional_net = self.set_default_network(activation, context_features, dropout_probability,
                                                            hidden_features, num_blocks,
                                                            use_batch_norm, use_residual_blocks)

    def set_default_network(self, activation, context_features, dropout_probability, hidden_features, num_blocks,
                            use_batch_norm, use_residual_blocks):
        if use_residual_blocks:
            conditional_net = ResidualNet(in_features=context_features,
                                          out_features=self._num_parameters(),
                                          hidden_features=hidden_features,
                                          activation=activation,
                                          num_blocks=num_blocks,
                                          dropout_probability=dropout_probability,
                                          use_batch_norm=use_batch_norm
                                          )
        else:
            conditional_net = MLP(in_shape=(context_features,),
                                  out_shape=(self._num_parameters(),),
                                  hidden_sizes=[hidden_features] * num_blocks)
            if dropout_probability &gt; 1e-12:
                raise NotImplementedError(&#34;No dropout for MLP&#34;)
            if use_batch_norm:
                raise NotImplementedError(&#34;No batch norm for MLP&#34;)
        return conditional_net

    def _num_parameters(self):
        return self.features * self._output_dim_multiplier()

    def forward(self, inputs, context=None):
        if context is None:
            raise TypeError(&#34;Conditional transforms require a context.&#34;)
        conditional_params = self.conditional_net(context)
        outputs, logabsdet = self._forward_given_params(inputs, conditional_params)
        return outputs, logabsdet

    def inverse(self, inputs, context=None):
        if context is None:
            raise TypeError(&#34;Conditional transforms require a context.&#34;)
        conditional_params = self.conditional_net(context)
        outputs, logabsdet = self._inverse_given_params(inputs, conditional_params)
        return outputs, logabsdet

    def _output_dim_multiplier(self):
        raise NotImplementedError()

    def _forward_given_params(self, inputs, autoregressive_params):
        raise NotImplementedError()

    def _inverse_given_params(self, inputs, autoregressive_params):
        raise NotImplementedError()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.conditional.AffineConditionalTransform" href="#flowcon.transforms.conditional.AffineConditionalTransform">AffineConditionalTransform</a></li>
<li><a title="flowcon.transforms.conditional.ConditionalLUTransform" href="#flowcon.transforms.conditional.ConditionalLUTransform">ConditionalLUTransform</a></li>
<li><a title="flowcon.transforms.conditional.ConditionalOrthogonalTransform" href="#flowcon.transforms.conditional.ConditionalOrthogonalTransform">ConditionalOrthogonalTransform</a></li>
<li><a title="flowcon.transforms.conditional.ConditionalPiecewiseRationalQuadraticTransform" href="#flowcon.transforms.conditional.ConditionalPiecewiseRationalQuadraticTransform">ConditionalPiecewiseRationalQuadraticTransform</a></li>
<li><a title="flowcon.transforms.conditional.ConditionalPlanarTransform" href="#flowcon.transforms.conditional.ConditionalPlanarTransform">ConditionalPlanarTransform</a></li>
<li><a title="flowcon.transforms.conditional.ConditionalRotationTransform" href="#flowcon.transforms.conditional.ConditionalRotationTransform">ConditionalRotationTransform</a></li>
<li><a title="flowcon.transforms.conditional.ConditionalSVDTransform" href="#flowcon.transforms.conditional.ConditionalSVDTransform">ConditionalSVDTransform</a></li>
<li><a title="flowcon.transforms.conditional.ConditionalScaleTransform" href="#flowcon.transforms.conditional.ConditionalScaleTransform">ConditionalScaleTransform</a></li>
<li><a title="flowcon.transforms.conditional.ConditionalShiftTransform" href="#flowcon.transforms.conditional.ConditionalShiftTransform">ConditionalShiftTransform</a></li>
<li><a title="flowcon.transforms.conditional.ConditionalSumOfSigmoidsTransform" href="#flowcon.transforms.conditional.ConditionalSumOfSigmoidsTransform">ConditionalSumOfSigmoidsTransform</a></li>
<li><a title="flowcon.transforms.conditional.ConditionalSylvesterTransform" href="#flowcon.transforms.conditional.ConditionalSylvesterTransform">ConditionalSylvesterTransform</a></li>
<li><a title="flowcon.transforms.conditional.ConditionalUMNNTransform" href="#flowcon.transforms.conditional.ConditionalUMNNTransform">ConditionalUMNNTransform</a></li>
<li><a title="flowcon.transforms.conditional.PiecewiseLinearConditionalTransform" href="#flowcon.transforms.conditional.PiecewiseLinearConditionalTransform">PiecewiseLinearConditionalTransform</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.conditional.ConditionalTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.conditional.ConditionalTransform.inverse"><code class="name flex">
<span>def <span class="ident">inverse</span></span>(<span>self, inputs, context=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inverse(self, inputs, context=None):
    if context is None:
        raise TypeError(&#34;Conditional transforms require a context.&#34;)
    conditional_params = self.conditional_net(context)
    outputs, logabsdet = self._inverse_given_params(inputs, conditional_params)
    return outputs, logabsdet</code></pre>
</details>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalTransform.set_default_network"><code class="name flex">
<span>def <span class="ident">set_default_network</span></span>(<span>self, activation, context_features, dropout_probability, hidden_features, num_blocks, use_batch_norm, use_residual_blocks)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_default_network(self, activation, context_features, dropout_probability, hidden_features, num_blocks,
                        use_batch_norm, use_residual_blocks):
    if use_residual_blocks:
        conditional_net = ResidualNet(in_features=context_features,
                                      out_features=self._num_parameters(),
                                      hidden_features=hidden_features,
                                      activation=activation,
                                      num_blocks=num_blocks,
                                      dropout_probability=dropout_probability,
                                      use_batch_norm=use_batch_norm
                                      )
    else:
        conditional_net = MLP(in_shape=(context_features,),
                              out_shape=(self._num_parameters(),),
                              hidden_sizes=[hidden_features] * num_blocks)
        if dropout_probability &gt; 1e-12:
            raise NotImplementedError(&#34;No dropout for MLP&#34;)
        if use_batch_norm:
            raise NotImplementedError(&#34;No batch norm for MLP&#34;)
    return conditional_net</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.base.Transform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalUMNNTransform"><code class="flex name class">
<span>class <span class="ident">ConditionalUMNNTransform</span></span>
<span>(</span><span>features, hidden_features, context_features=None, num_blocks=2, use_residual_blocks=True, activation=&lt;function relu&gt;, dropout_probability=0.0, use_batch_norm=False, integrand_net_layers=[50, 50, 50], cond_size=20, nb_steps=20, solver='CCParallel')</span>
</code></dt>
<dd>
<div class="desc"><p>An unconstrained monotonic neural networks autoregressive layer that transforms the variables.</p>
<p>Reference:</p>
<blockquote>
<p>A. Wehenkel and G. Louppe, Unconstrained Monotonic Neural Networks, NeurIPS2019.</p>
</blockquote>
<p>---- Specific arguments ----
integrand_net_layers: the layers dimension to put in the integrand network.
cond_size: The embedding size for the conditioning factors.
nb_steps: The number of integration steps.
solver: The quadrature algorithm - CC or CCParallel. Both implements Clenshaw-Curtis quadrature with
Leibniz rule for backward computation. CCParallel pass all the evaluation points (nb_steps) at once, it is faster
but requires more memory.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConditionalUMNNTransform(ConditionalTransform):
    &#34;&#34;&#34;An unconstrained monotonic neural networks autoregressive layer that transforms the variables.

        Reference:
        &gt; A. Wehenkel and G. Louppe, Unconstrained Monotonic Neural Networks, NeurIPS2019.

        ---- Specific arguments ----
        integrand_net_layers: the layers dimension to put in the integrand network.
        cond_size: The embedding size for the conditioning factors.
        nb_steps: The number of integration steps.
        solver: The quadrature algorithm - CC or CCParallel. Both implements Clenshaw-Curtis quadrature with
        Leibniz rule for backward computation. CCParallel pass all the evaluation points (nb_steps) at once, it is faster
        but requires more memory.
        &#34;&#34;&#34;

    def __init__(
            self,
            features,
            hidden_features,
            context_features=None,
            num_blocks=2,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
            integrand_net_layers=[50, 50, 50],
            cond_size=20,
            nb_steps=20,
            solver=&#34;CCParallel&#34;,
    ):
        self.cond_size = cond_size

        super().__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm
        )
        self.transformer = MonotonicNormalizer(integrand_net_layers, cond_size, nb_steps, solver)

    def _output_dim_multiplier(self):
        return self.cond_size

    def _forward_given_params(self, inputs, conditional_params):
        z, jac = self.transformer(inputs, conditional_params.reshape(inputs.shape[0], inputs.shape[1], -1))
        log_det_jac = jac.log().sum(1)
        return z, log_det_jac

    def _inverse_given_params(self, inputs, conditional_params):
        x = self.transformer.inverse_transform(inputs,
                                               conditional_params.reshape(inputs.shape[0], inputs.shape[1], -1))
        z, jac = self.transformer(x, conditional_params.reshape(inputs.shape[0], inputs.shape[1], -1))
        log_det_jac = -jac.log().sum(1)
        return x, log_det_jac</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.conditional.ConditionalUMNNTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalUMNNTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.ConditionalUMNNTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.conditional.ConditionalTransform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.conditional.PiecewiseLinearConditionalTransform"><code class="flex name class">
<span>class <span class="ident">PiecewiseLinearConditionalTransform</span></span>
<span>(</span><span>num_bins, features, hidden_features, context_features=None, num_blocks=2, use_residual_blocks=True, activation=&lt;function relu&gt;, dropout_probability=0.0, use_batch_norm=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms each input variable with an invertible transformation, conditioned on some given input.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PiecewiseLinearConditionalTransform(ConditionalTransform):
    def __init__(
            self,
            num_bins,
            features,
            hidden_features,
            context_features=None,
            num_blocks=2,
            use_residual_blocks=True,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
    ):
        self.num_bins = num_bins

        super().__init__(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            use_residual_blocks=use_residual_blocks,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm
        )

    def _output_dim_multiplier(self):
        return self.num_bins

    def _elementwise(self, inputs, autoregressive_params, inverse=False):
        batch_size = inputs.shape[0]

        unnormalized_pdf = autoregressive_params.view(
            batch_size, self.features, self._output_dim_multiplier()
        )

        outputs, logabsdet = linear_spline(
            inputs=inputs, unnormalized_pdf=unnormalized_pdf, inverse=inverse,
            left=-4.0, right=4.0, bottom=-4.0, top=4.0
        )

        return outputs, torchutils.sum_except_batch(logabsdet)

    def _forward_given_params(self, inputs, autoregressive_params):
        return self._elementwise(inputs, autoregressive_params)

    def _inverse_given_params(self, inputs, autoregressive_params):
        return self._elementwise(inputs, autoregressive_params, inverse=True)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.conditional.PiecewiseLinearConditionalTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.PiecewiseLinearConditionalTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.conditional.PiecewiseLinearConditionalTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.conditional.ConditionalTransform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="flowcon.transforms" href="index.html">flowcon.transforms</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="flowcon.transforms.conditional.dh_dx" href="#flowcon.transforms.conditional.dh_dx">dh_dx</a></code></li>
<li><code><a title="flowcon.transforms.conditional.h" href="#flowcon.transforms.conditional.h">h</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="flowcon.transforms.conditional.AffineConditionalTransform" href="#flowcon.transforms.conditional.AffineConditionalTransform">AffineConditionalTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.conditional.AffineConditionalTransform.call_super_init" href="#flowcon.transforms.conditional.AffineConditionalTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.conditional.AffineConditionalTransform.dump_patches" href="#flowcon.transforms.conditional.AffineConditionalTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.conditional.AffineConditionalTransform.training" href="#flowcon.transforms.conditional.AffineConditionalTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.conditional.ConditionalLUTransform" href="#flowcon.transforms.conditional.ConditionalLUTransform">ConditionalLUTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.conditional.ConditionalLUTransform.call_super_init" href="#flowcon.transforms.conditional.ConditionalLUTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalLUTransform.dump_patches" href="#flowcon.transforms.conditional.ConditionalLUTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalLUTransform.training" href="#flowcon.transforms.conditional.ConditionalLUTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.conditional.ConditionalOrthogonalTransform" href="#flowcon.transforms.conditional.ConditionalOrthogonalTransform">ConditionalOrthogonalTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.conditional.ConditionalOrthogonalTransform.call_super_init" href="#flowcon.transforms.conditional.ConditionalOrthogonalTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalOrthogonalTransform.dump_patches" href="#flowcon.transforms.conditional.ConditionalOrthogonalTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalOrthogonalTransform.training" href="#flowcon.transforms.conditional.ConditionalOrthogonalTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.conditional.ConditionalPiecewiseRationalQuadraticTransform" href="#flowcon.transforms.conditional.ConditionalPiecewiseRationalQuadraticTransform">ConditionalPiecewiseRationalQuadraticTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.conditional.ConditionalPiecewiseRationalQuadraticTransform.call_super_init" href="#flowcon.transforms.conditional.ConditionalPiecewiseRationalQuadraticTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalPiecewiseRationalQuadraticTransform.dump_patches" href="#flowcon.transforms.conditional.ConditionalPiecewiseRationalQuadraticTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalPiecewiseRationalQuadraticTransform.training" href="#flowcon.transforms.conditional.ConditionalPiecewiseRationalQuadraticTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.conditional.ConditionalPlanarTransform" href="#flowcon.transforms.conditional.ConditionalPlanarTransform">ConditionalPlanarTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.conditional.ConditionalPlanarTransform.call_super_init" href="#flowcon.transforms.conditional.ConditionalPlanarTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalPlanarTransform.dump_patches" href="#flowcon.transforms.conditional.ConditionalPlanarTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalPlanarTransform.get_u_hat" href="#flowcon.transforms.conditional.ConditionalPlanarTransform.get_u_hat">get_u_hat</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalPlanarTransform.training" href="#flowcon.transforms.conditional.ConditionalPlanarTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.conditional.ConditionalRotationTransform" href="#flowcon.transforms.conditional.ConditionalRotationTransform">ConditionalRotationTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.conditional.ConditionalRotationTransform.build_matrix" href="#flowcon.transforms.conditional.ConditionalRotationTransform.build_matrix">build_matrix</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalRotationTransform.call_super_init" href="#flowcon.transforms.conditional.ConditionalRotationTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalRotationTransform.dump_patches" href="#flowcon.transforms.conditional.ConditionalRotationTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalRotationTransform.training" href="#flowcon.transforms.conditional.ConditionalRotationTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.conditional.ConditionalSVDTransform" href="#flowcon.transforms.conditional.ConditionalSVDTransform">ConditionalSVDTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.conditional.ConditionalSVDTransform.call_super_init" href="#flowcon.transforms.conditional.ConditionalSVDTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalSVDTransform.dump_patches" href="#flowcon.transforms.conditional.ConditionalSVDTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalSVDTransform.training" href="#flowcon.transforms.conditional.ConditionalSVDTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.conditional.ConditionalScaleTransform" href="#flowcon.transforms.conditional.ConditionalScaleTransform">ConditionalScaleTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.conditional.ConditionalScaleTransform.call_super_init" href="#flowcon.transforms.conditional.ConditionalScaleTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalScaleTransform.dump_patches" href="#flowcon.transforms.conditional.ConditionalScaleTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalScaleTransform.training" href="#flowcon.transforms.conditional.ConditionalScaleTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.conditional.ConditionalShiftTransform" href="#flowcon.transforms.conditional.ConditionalShiftTransform">ConditionalShiftTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.conditional.ConditionalShiftTransform.call_super_init" href="#flowcon.transforms.conditional.ConditionalShiftTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalShiftTransform.dump_patches" href="#flowcon.transforms.conditional.ConditionalShiftTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalShiftTransform.training" href="#flowcon.transforms.conditional.ConditionalShiftTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.conditional.ConditionalSumOfSigmoidsTransform" href="#flowcon.transforms.conditional.ConditionalSumOfSigmoidsTransform">ConditionalSumOfSigmoidsTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.conditional.ConditionalSumOfSigmoidsTransform.call_super_init" href="#flowcon.transforms.conditional.ConditionalSumOfSigmoidsTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalSumOfSigmoidsTransform.dump_patches" href="#flowcon.transforms.conditional.ConditionalSumOfSigmoidsTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalSumOfSigmoidsTransform.training" href="#flowcon.transforms.conditional.ConditionalSumOfSigmoidsTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.conditional.ConditionalSylvesterTransform" href="#flowcon.transforms.conditional.ConditionalSylvesterTransform">ConditionalSylvesterTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.conditional.ConditionalSylvesterTransform.build_orthogonal_matrix" href="#flowcon.transforms.conditional.ConditionalSylvesterTransform.build_orthogonal_matrix">build_orthogonal_matrix</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalSylvesterTransform.call_super_init" href="#flowcon.transforms.conditional.ConditionalSylvesterTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalSylvesterTransform.dump_patches" href="#flowcon.transforms.conditional.ConditionalSylvesterTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalSylvesterTransform.forward_jit" href="#flowcon.transforms.conditional.ConditionalSylvesterTransform.forward_jit">forward_jit</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalSylvesterTransform.training" href="#flowcon.transforms.conditional.ConditionalSylvesterTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.conditional.ConditionalTransform" href="#flowcon.transforms.conditional.ConditionalTransform">ConditionalTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.conditional.ConditionalTransform.call_super_init" href="#flowcon.transforms.conditional.ConditionalTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalTransform.dump_patches" href="#flowcon.transforms.conditional.ConditionalTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalTransform.inverse" href="#flowcon.transforms.conditional.ConditionalTransform.inverse">inverse</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalTransform.set_default_network" href="#flowcon.transforms.conditional.ConditionalTransform.set_default_network">set_default_network</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalTransform.training" href="#flowcon.transforms.conditional.ConditionalTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.conditional.ConditionalUMNNTransform" href="#flowcon.transforms.conditional.ConditionalUMNNTransform">ConditionalUMNNTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.conditional.ConditionalUMNNTransform.call_super_init" href="#flowcon.transforms.conditional.ConditionalUMNNTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalUMNNTransform.dump_patches" href="#flowcon.transforms.conditional.ConditionalUMNNTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.conditional.ConditionalUMNNTransform.training" href="#flowcon.transforms.conditional.ConditionalUMNNTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.conditional.PiecewiseLinearConditionalTransform" href="#flowcon.transforms.conditional.PiecewiseLinearConditionalTransform">PiecewiseLinearConditionalTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.conditional.PiecewiseLinearConditionalTransform.call_super_init" href="#flowcon.transforms.conditional.PiecewiseLinearConditionalTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.conditional.PiecewiseLinearConditionalTransform.dump_patches" href="#flowcon.transforms.conditional.PiecewiseLinearConditionalTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.conditional.PiecewiseLinearConditionalTransform.training" href="#flowcon.transforms.conditional.PiecewiseLinearConditionalTransform.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>