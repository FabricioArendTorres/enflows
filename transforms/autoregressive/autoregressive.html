<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.0">
<title>flowcon.transforms.autoregressive.autoregressive API documentation</title>
<meta name="description" content="Implementations of autoregressive transforms.">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>flowcon.transforms.autoregressive.autoregressive</code></h1>
</header>
<section id="section-intro">
<p>Implementations of autoregressive transforms.</p>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="flowcon.transforms.autoregressive.autoregressive.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform"><code class="flex name class">
<span>class <span class="ident">AutoregressiveTransform</span></span>
<span>(</span><span>autoregressive_net)</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms each input variable with an invertible elementwise transformation.</p>
<p>The parameters of each invertible elementwise transformation can be functions of previous input
variables, but they must not depend on the current or any following input variables.</p>
<p>NOTE: Calculating the inverse transform is D times slower than calculating the
forward transform, where D is the dimensionality of the input to the transform.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AutoregressiveTransform(Transform):
    &#34;&#34;&#34;Transforms each input variable with an invertible elementwise transformation.

    The parameters of each invertible elementwise transformation can be functions of previous input
    variables, but they must not depend on the current or any following input variables.

    NOTE: Calculating the inverse transform is D times slower than calculating the
    forward transform, where D is the dimensionality of the input to the transform.
    &#34;&#34;&#34;

    def __init__(self, autoregressive_net):
        super(AutoregressiveTransform, self).__init__()
        self.autoregressive_net = autoregressive_net

    def forward(self, inputs, context=None):
        autoregressive_params = self.autoregressive_net(inputs, context)
        outputs, logabsdet = self._elementwise_forward(inputs, autoregressive_params)
        return outputs, logabsdet

    def inverse(self, inputs, context=None):
        num_inputs = int(np.prod(inputs.shape[1:]))
        outputs = torch.zeros_like(inputs)
        logabsdet = None
        for _ in range(num_inputs):
            autoregressive_params = self.autoregressive_net(outputs, context)
            outputs, logabsdet = self._elementwise_inverse(
                inputs, autoregressive_params
            )
        return outputs, logabsdet

    def _output_dim_multiplier(self):
        raise NotImplementedError()

    def _elementwise_forward(self, inputs, autoregressive_params):
        raise NotImplementedError()

    def _elementwise_inverse(self, inputs, autoregressive_params):
        raise NotImplementedError()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.base.Transform" href="../base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.autoregressive.autoregressive.MaskedAffineAutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.MaskedAffineAutoregressiveTransform">MaskedAffineAutoregressiveTransform</a></li>
<li><a title="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseCubicAutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseCubicAutoregressiveTransform">MaskedPiecewiseCubicAutoregressiveTransform</a></li>
<li><a title="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseLinearAutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseLinearAutoregressiveTransform">MaskedPiecewiseLinearAutoregressiveTransform</a></li>
<li><a title="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseQuadraticAutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseQuadraticAutoregressiveTransform">MaskedPiecewiseQuadraticAutoregressiveTransform</a></li>
<li><a title="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseRationalQuadraticAutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseRationalQuadraticAutoregressiveTransform">MaskedPiecewiseRationalQuadraticAutoregressiveTransform</a></li>
<li><a title="flowcon.transforms.autoregressive.autoregressive.MaskedShiftAutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.MaskedShiftAutoregressiveTransform">MaskedShiftAutoregressiveTransform</a></li>
<li><a title="flowcon.transforms.autoregressive.autoregressive.MaskedSumOfSigmoidsTransform" href="#flowcon.transforms.autoregressive.autoregressive.MaskedSumOfSigmoidsTransform">MaskedSumOfSigmoidsTransform</a></li>
<li><a title="flowcon.transforms.autoregressive.autoregressive.MaskedUMNNAutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.MaskedUMNNAutoregressiveTransform">MaskedUMNNAutoregressiveTransform</a></li>
<li><a title="flowcon.transforms.autoregressive.deep_sigmoid.MaskedDeepSigmoidTransform" href="deep_sigmoid.html#flowcon.transforms.autoregressive.deep_sigmoid.MaskedDeepSigmoidTransform">MaskedDeepSigmoidTransform</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform.inverse"><code class="name flex">
<span>def <span class="ident">inverse</span></span>(<span>self, inputs, context=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.base.Transform" href="../base.html#flowcon.transforms.base.Transform">Transform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.base.Transform.forward" href="../base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedAffineAutoregressiveTransform"><code class="flex name class">
<span>class <span class="ident">MaskedAffineAutoregressiveTransform</span></span>
<span>(</span><span>features, hidden_features, context_features=None, num_blocks=2, use_residual_blocks=True, random_mask=False, activation=&lt;function relu&gt;, dropout_probability=0.0, use_batch_norm=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms each input variable with an invertible elementwise transformation.</p>
<p>The parameters of each invertible elementwise transformation can be functions of previous input
variables, but they must not depend on the current or any following input variables.</p>
<p>NOTE: Calculating the inverse transform is D times slower than calculating the
forward transform, where D is the dimensionality of the input to the transform.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MaskedAffineAutoregressiveTransform(AutoregressiveTransform):
    def __init__(
            self,
            features,
            hidden_features,
            context_features=None,
            num_blocks=2,
            use_residual_blocks=True,
            random_mask=False,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
    ):
        self.features = features
        made = made_module.MADE(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            output_multiplier=self._output_dim_multiplier(),
            use_residual_blocks=use_residual_blocks,
            random_mask=random_mask,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm,
        )
        self._epsilon = 1e-3
        super(MaskedAffineAutoregressiveTransform, self).__init__(made)

    def _output_dim_multiplier(self):
        return 2

    def _elementwise_forward(self, inputs, autoregressive_params):
        unconstrained_scale, shift = self._unconstrained_scale_and_shift(
            autoregressive_params
        )
        # scale = torch.sigmoid(unconstrained_scale + 2.0) + self._epsilon
        scale = F.softplus(unconstrained_scale) + self._epsilon
        log_scale = torch.log(scale)
        outputs = scale * inputs + shift
        logabsdet = torchutils.sum_except_batch(log_scale, num_batch_dims=1)
        return outputs, logabsdet

    def _elementwise_inverse(self, inputs, autoregressive_params):
        unconstrained_scale, shift = self._unconstrained_scale_and_shift(
            autoregressive_params
        )
        # scale = torch.sigmoid(unconstrained_scale + 2.0) + self._epsilon
        scale = F.softplus(unconstrained_scale) + self._epsilon
        log_scale = torch.log(scale)
        outputs = (inputs - shift) / scale
        logabsdet = -torchutils.sum_except_batch(log_scale, num_batch_dims=1)
        return outputs, logabsdet

    def _unconstrained_scale_and_shift(self, autoregressive_params):
        # split_idx = autoregressive_params.size(1) // 2
        # unconstrained_scale = autoregressive_params[..., :split_idx]
        # shift = autoregressive_params[..., split_idx:]
        # return unconstrained_scale, shift
        autoregressive_params = autoregressive_params.view(
            -1, self.features, self._output_dim_multiplier()
        )
        unconstrained_scale = autoregressive_params[..., 0]
        shift = autoregressive_params[..., 1]
        return unconstrained_scale, shift</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform">AutoregressiveTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="../base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedAffineAutoregressiveTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedAffineAutoregressiveTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedAffineAutoregressiveTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform">AutoregressiveTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform.forward" href="../base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseCubicAutoregressiveTransform"><code class="flex name class">
<span>class <span class="ident">MaskedPiecewiseCubicAutoregressiveTransform</span></span>
<span>(</span><span>num_bins, features, hidden_features, context_features=None, num_blocks=2, use_residual_blocks=True, random_mask=False, activation=&lt;function relu&gt;, dropout_probability=0.0, use_batch_norm=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms each input variable with an invertible elementwise transformation.</p>
<p>The parameters of each invertible elementwise transformation can be functions of previous input
variables, but they must not depend on the current or any following input variables.</p>
<p>NOTE: Calculating the inverse transform is D times slower than calculating the
forward transform, where D is the dimensionality of the input to the transform.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MaskedPiecewiseCubicAutoregressiveTransform(AutoregressiveTransform):
    def __init__(
            self,
            num_bins,
            features,
            hidden_features,
            context_features=None,
            num_blocks=2,
            use_residual_blocks=True,
            random_mask=False,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
    ):
        self.num_bins = num_bins
        self.features = features
        made = made_module.MADE(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            output_multiplier=self._output_dim_multiplier(),
            use_residual_blocks=use_residual_blocks,
            random_mask=random_mask,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm,
        )
        super(MaskedPiecewiseCubicAutoregressiveTransform, self).__init__(made)

    def _output_dim_multiplier(self):
        return self.num_bins * 2 + 2

    def _elementwise(self, inputs, autoregressive_params, inverse=False):
        batch_size = inputs.shape[0]

        transform_params = autoregressive_params.view(
            batch_size, self.features, self.num_bins * 2 + 2
        )

        unnormalized_widths = transform_params[..., : self.num_bins]
        unnormalized_heights = transform_params[..., self.num_bins: 2 * self.num_bins]
        derivatives = transform_params[..., 2 * self.num_bins:]
        unnorm_derivatives_left = derivatives[..., 0][..., None]
        unnorm_derivatives_right = derivatives[..., 1][..., None]

        if hasattr(self.autoregressive_net, &#34;hidden_features&#34;):
            unnormalized_widths /= np.sqrt(self.autoregressive_net.hidden_features)
            unnormalized_heights /= np.sqrt(self.autoregressive_net.hidden_features)

        outputs, logabsdet = cubic_spline(
            inputs=inputs,
            unnormalized_widths=unnormalized_widths,
            unnormalized_heights=unnormalized_heights,
            unnorm_derivatives_left=unnorm_derivatives_left,
            unnorm_derivatives_right=unnorm_derivatives_right,
            inverse=inverse,
        )
        return outputs, torchutils.sum_except_batch(logabsdet)

    def _elementwise_forward(self, inputs, autoregressive_params):
        return self._elementwise(inputs, autoregressive_params)

    def _elementwise_inverse(self, inputs, autoregressive_params):
        return self._elementwise(inputs, autoregressive_params, inverse=True)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform">AutoregressiveTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="../base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseCubicAutoregressiveTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseCubicAutoregressiveTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseCubicAutoregressiveTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform">AutoregressiveTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform.forward" href="../base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseLinearAutoregressiveTransform"><code class="flex name class">
<span>class <span class="ident">MaskedPiecewiseLinearAutoregressiveTransform</span></span>
<span>(</span><span>num_bins, features, hidden_features, context_features=None, num_blocks=2, use_residual_blocks=True, random_mask=False, activation=&lt;function relu&gt;, dropout_probability=0.0, use_batch_norm=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms each input variable with an invertible elementwise transformation.</p>
<p>The parameters of each invertible elementwise transformation can be functions of previous input
variables, but they must not depend on the current or any following input variables.</p>
<p>NOTE: Calculating the inverse transform is D times slower than calculating the
forward transform, where D is the dimensionality of the input to the transform.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MaskedPiecewiseLinearAutoregressiveTransform(AutoregressiveTransform):
    def __init__(
            self,
            num_bins,
            features,
            hidden_features,
            context_features=None,
            num_blocks=2,
            use_residual_blocks=True,
            random_mask=False,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
    ):
        self.num_bins = num_bins
        self.features = features
        made = made_module.MADE(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            output_multiplier=self._output_dim_multiplier(),
            use_residual_blocks=use_residual_blocks,
            random_mask=random_mask,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm,
        )
        super().__init__(made)

    def _output_dim_multiplier(self):
        return self.num_bins

    def _elementwise(self, inputs, autoregressive_params, inverse=False):
        batch_size = inputs.shape[0]

        unnormalized_pdf = autoregressive_params.view(
            batch_size, self.features, self._output_dim_multiplier()
        )

        outputs, logabsdet = linear_spline(
            inputs=inputs, unnormalized_pdf=unnormalized_pdf, inverse=inverse
        )

        return outputs, torchutils.sum_except_batch(logabsdet)

    def _elementwise_forward(self, inputs, autoregressive_params):
        return self._elementwise(inputs, autoregressive_params)

    def _elementwise_inverse(self, inputs, autoregressive_params):
        return self._elementwise(inputs, autoregressive_params, inverse=True)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform">AutoregressiveTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="../base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseLinearAutoregressiveTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseLinearAutoregressiveTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseLinearAutoregressiveTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform">AutoregressiveTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform.forward" href="../base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseQuadraticAutoregressiveTransform"><code class="flex name class">
<span>class <span class="ident">MaskedPiecewiseQuadraticAutoregressiveTransform</span></span>
<span>(</span><span>features, hidden_features, context_features=None, num_bins=10, num_blocks=2, tails=None, tail_bound=1.0, use_residual_blocks=True, random_mask=False, activation=&lt;function relu&gt;, dropout_probability=0.0, use_batch_norm=False, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001)</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms each input variable with an invertible elementwise transformation.</p>
<p>The parameters of each invertible elementwise transformation can be functions of previous input
variables, but they must not depend on the current or any following input variables.</p>
<p>NOTE: Calculating the inverse transform is D times slower than calculating the
forward transform, where D is the dimensionality of the input to the transform.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MaskedPiecewiseQuadraticAutoregressiveTransform(AutoregressiveTransform):
    def __init__(
            self,
            features,
            hidden_features,
            context_features=None,
            num_bins=10,
            num_blocks=2,
            tails=None,
            tail_bound=1.0,
            use_residual_blocks=True,
            random_mask=False,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
            min_bin_width=rational_quadratic.DEFAULT_MIN_BIN_WIDTH,
            min_bin_height=rational_quadratic.DEFAULT_MIN_BIN_HEIGHT,
            min_derivative=rational_quadratic.DEFAULT_MIN_DERIVATIVE,
    ):
        self.num_bins = num_bins
        self.min_bin_width = min_bin_width
        self.min_bin_height = min_bin_height
        self.min_derivative = min_derivative
        self.tails = tails
        self.tail_bound = tail_bound
        self.features = features
        made = made_module.MADE(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            output_multiplier=self._output_dim_multiplier(),
            use_residual_blocks=use_residual_blocks,
            random_mask=random_mask,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm,
        )
        super().__init__(made)

    def _output_dim_multiplier(self):
        if self.tails == &#34;linear&#34;:
            return self.num_bins * 2 - 1
        else:
            return self.num_bins * 2 + 1

    def _elementwise(self, inputs, autoregressive_params, inverse=False):
        batch_size = inputs.shape[0]

        transform_params = autoregressive_params.view(
            batch_size, self.features, self._output_dim_multiplier()
        )

        unnormalized_widths = transform_params[..., : self.num_bins]
        unnormalized_heights = transform_params[..., self.num_bins:]

        if hasattr(self.autoregressive_net, &#34;hidden_features&#34;):
            unnormalized_widths /= np.sqrt(self.autoregressive_net.hidden_features)
            # unnormalized_heights /= np.sqrt(self.autoregressive_net.hidden_features)

        if self.tails is None:
            spline_fn = quadratic_spline
            spline_kwargs = {}
        elif self.tails == &#34;linear&#34;:
            spline_fn = unconstrained_quadratic_spline
            spline_kwargs = {&#34;tails&#34;: self.tails, &#34;tail_bound&#34;: self.tail_bound}
        else:
            raise ValueError

        outputs, logabsdet = spline_fn(
            inputs=inputs,
            unnormalized_heights=unnormalized_heights,
            unnormalized_widths=unnormalized_widths,
            inverse=inverse,
            min_bin_width=self.min_bin_width,
            min_bin_height=self.min_bin_height,
            **spline_kwargs
        )

        return outputs, torchutils.sum_except_batch(logabsdet)

    def _elementwise_forward(self, inputs, autoregressive_params):
        return self._elementwise(inputs, autoregressive_params)

    def _elementwise_inverse(self, inputs, autoregressive_params):
        return self._elementwise(inputs, autoregressive_params, inverse=True)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform">AutoregressiveTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="../base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseQuadraticAutoregressiveTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseQuadraticAutoregressiveTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseQuadraticAutoregressiveTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform">AutoregressiveTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform.forward" href="../base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseRationalQuadraticAutoregressiveTransform"><code class="flex name class">
<span>class <span class="ident">MaskedPiecewiseRationalQuadraticAutoregressiveTransform</span></span>
<span>(</span><span>features, hidden_features, context_features=None, num_bins=10, tails=None, tail_bound=1.0, num_blocks=2, use_residual_blocks=True, random_mask=False, activation=&lt;function relu&gt;, dropout_probability=0.0, use_batch_norm=False, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001)</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms each input variable with an invertible elementwise transformation.</p>
<p>The parameters of each invertible elementwise transformation can be functions of previous input
variables, but they must not depend on the current or any following input variables.</p>
<p>NOTE: Calculating the inverse transform is D times slower than calculating the
forward transform, where D is the dimensionality of the input to the transform.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MaskedPiecewiseRationalQuadraticAutoregressiveTransform(AutoregressiveTransform):
    def __init__(
            self,
            features,
            hidden_features,
            context_features=None,
            num_bins=10,
            tails=None,
            tail_bound=1.0,
            num_blocks=2,
            use_residual_blocks=True,
            random_mask=False,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
            min_bin_width=rational_quadratic.DEFAULT_MIN_BIN_WIDTH,
            min_bin_height=rational_quadratic.DEFAULT_MIN_BIN_HEIGHT,
            min_derivative=rational_quadratic.DEFAULT_MIN_DERIVATIVE,
    ):
        self.num_bins = num_bins
        self.min_bin_width = min_bin_width
        self.min_bin_height = min_bin_height
        self.min_derivative = min_derivative
        self.tails = tails
        self.tail_bound = tail_bound

        autoregressive_net = made_module.MADE(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            output_multiplier=self._output_dim_multiplier(),
            use_residual_blocks=use_residual_blocks,
            random_mask=random_mask,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm,
        )

        super().__init__(autoregressive_net)

    def _output_dim_multiplier(self):
        if self.tails == &#34;linear&#34;:
            return self.num_bins * 3 - 1
        elif self.tails is None:
            return self.num_bins * 3 + 1
        else:
            raise ValueError

    def _elementwise(self, inputs, autoregressive_params, inverse=False):
        batch_size, features = inputs.shape[0], inputs.shape[1]

        transform_params = autoregressive_params.view(
            batch_size, features, self._output_dim_multiplier()
        )

        unnormalized_widths = transform_params[..., : self.num_bins]
        unnormalized_heights = transform_params[..., self.num_bins: 2 * self.num_bins]
        unnormalized_derivatives = transform_params[..., 2 * self.num_bins:]

        if hasattr(self.autoregressive_net, &#34;hidden_features&#34;):
            unnormalized_widths /= np.sqrt(self.autoregressive_net.hidden_features)
            unnormalized_heights /= np.sqrt(self.autoregressive_net.hidden_features)

        if self.tails is None:
            spline_fn = rational_quadratic_spline
            spline_kwargs = {&#34;left&#34;: -1.2, &#34;right&#34;: 1.2, &#34;bottom&#34;: -1.2, &#34;top&#34;: 1.2}
        elif self.tails == &#34;linear&#34;:
            spline_fn = unconstrained_rational_quadratic_spline
            spline_kwargs = {&#34;tails&#34;: self.tails, &#34;tail_bound&#34;: self.tail_bound}
        else:
            raise ValueError

        outputs, logabsdet = spline_fn(
            inputs=inputs,
            unnormalized_widths=unnormalized_widths,
            unnormalized_heights=unnormalized_heights,
            unnormalized_derivatives=unnormalized_derivatives,
            inverse=inverse,
            min_bin_width=self.min_bin_width,
            min_bin_height=self.min_bin_height,
            min_derivative=self.min_derivative,
            enable_identity_init=True,
            **spline_kwargs
        )

        return outputs, torchutils.sum_except_batch(logabsdet)

    def _elementwise_forward(self, inputs, autoregressive_params):
        return self._elementwise(inputs, autoregressive_params)

    def _elementwise_inverse(self, inputs, autoregressive_params):
        return self._elementwise(inputs, autoregressive_params, inverse=True)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform">AutoregressiveTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="../base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseRationalQuadraticAutoregressiveTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseRationalQuadraticAutoregressiveTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseRationalQuadraticAutoregressiveTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform">AutoregressiveTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform.forward" href="../base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedShiftAutoregressiveTransform"><code class="flex name class">
<span>class <span class="ident">MaskedShiftAutoregressiveTransform</span></span>
<span>(</span><span>features, hidden_features, context_features=None, num_blocks=2, use_residual_blocks=True, random_mask=False, activation=&lt;function relu&gt;, dropout_probability=0.0, use_batch_norm=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms each input variable with an invertible elementwise transformation.</p>
<p>The parameters of each invertible elementwise transformation can be functions of previous input
variables, but they must not depend on the current or any following input variables.</p>
<p>NOTE: Calculating the inverse transform is D times slower than calculating the
forward transform, where D is the dimensionality of the input to the transform.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MaskedShiftAutoregressiveTransform(AutoregressiveTransform):
    def __init__(
            self,
            features,
            hidden_features,
            context_features=None,
            num_blocks=2,
            use_residual_blocks=True,
            random_mask=False,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
    ):
        self.features = features
        made = made_module.MADE(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            output_multiplier=self._output_dim_multiplier(),
            use_residual_blocks=use_residual_blocks,
            random_mask=random_mask,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm,
        )
        self._epsilon = 1e-3
        self.shift_scale = 1.
        super(MaskedShiftAutoregressiveTransform, self).__init__(made)

    def _output_dim_multiplier(self):
        return 1

    def _elementwise_forward(self, inputs, autoregressive_params):
        shift = self._unconstrained_shift(
            autoregressive_params
        )
        # scale = torch.sigmoid(unconstrained_scale + 2.0) + self._epsilon
        # scale = F.softplus(unconstrained_scale) + self._epsilon
        # log_scale = torch.log(scale)
        outputs = inputs + torch.tanh(shift) * 2
        # logabsdet = torchutils.sum_except_batch(log_scale, num_batch_dims=1)
        logabsdet = torch.zeros(inputs.shape[0], device=inputs.device)
        return outputs, logabsdet

    def _elementwise_inverse(self, inputs, autoregressive_params):
        shift = self._unconstrained_shift(
            autoregressive_params
        )
        # scale = torch.sigmoid(unconstrained_scale + 2.0) + self._epsilon
        # scale = F.softplus(unconstrained_scale) + self._epsilon
        # log_scale = torch.log(scale)
        outputs = (inputs - shift) # / scale
        logabsdet = torch.zeros(inputs.shape[0], device=inputs.device)
        return outputs, logabsdet

    def _unconstrained_shift(self, autoregressive_params):
        # split_idx = autoregressive_params.size(1) // 2
        # unconstrained_scale = autoregressive_params[..., :split_idx]
        # shift = autoregressive_params[..., split_idx:]
        # return unconstrained_scale, shift
        shift = autoregressive_params.view(
            -1, self.features
        )
        # unconstrained_scale = autoregressive_params[..., 0]
        return shift * self.shift_scale</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform">AutoregressiveTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="../base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedShiftAutoregressiveTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedShiftAutoregressiveTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedShiftAutoregressiveTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform">AutoregressiveTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform.forward" href="../base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedSumOfSigmoidsTransform"><code class="flex name class">
<span>class <span class="ident">MaskedSumOfSigmoidsTransform</span></span>
<span>(</span><span>features, hidden_features, n_sigmoids=30, context_features=None, num_blocks=2, use_residual_blocks=True, random_mask=False, activation=&lt;function relu&gt;, dropout_probability=0.0, use_batch_norm=False)</span>
</code></dt>
<dd>
<div class="desc"><p>An unconstrained monotonic neural networks autoregressive layer that transforms the variables.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MaskedSumOfSigmoidsTransform(AutoregressiveTransform):
    &#34;&#34;&#34;An unconstrained monotonic neural networks autoregressive layer that transforms the variables.
        &#34;&#34;&#34;

    def __init__(
            self,
            features,
            hidden_features,
            n_sigmoids=30,
            context_features=None,
            num_blocks=2,
            use_residual_blocks=True,
            random_mask=False,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
    ):
        self.features = features
        self.n_sigmoids = n_sigmoids

        made = made_module.MADE(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            output_multiplier=self._output_dim_multiplier(),
            use_residual_blocks=use_residual_blocks,
            random_mask=random_mask,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm,
        )
        super().__init__(made)

    def _output_dim_multiplier(self):
        return 3 * self.n_sigmoids + 1

    def _elementwise_forward(self, inputs, autoregressive_params):
        transformer = SumOfSigmoids(n_sigmoids=self.n_sigmoids, features=self.features,
                                    raw_params=autoregressive_params.view(inputs.shape[0], self.features,
                                                                            self._output_dim_multiplier()))

        z, logabsdet = transformer(inputs)
        return z - 0.5, logabsdet

    def _elementwise_inverse(self, inputs, autoregressive_params):
        # self.transformer.set_raw_params(self.features, autoregressive_params.reshape(inputs.shape[0], -1))
        inputs = inputs + 0.5
        transformer = SumOfSigmoids(n_sigmoids=self.n_sigmoids, features=self.features,
                                    raw_params=autoregressive_params.view(inputs.shape[0], self.features,
                                                                            self._output_dim_multiplier()))
        x, logabsdet = transformer.inverse(inputs)
        return x, logabsdet</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform">AutoregressiveTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="../base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedSumOfSigmoidsTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedSumOfSigmoidsTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedSumOfSigmoidsTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform">AutoregressiveTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform.forward" href="../base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedUMNNAutoregressiveTransform"><code class="flex name class">
<span>class <span class="ident">MaskedUMNNAutoregressiveTransform</span></span>
<span>(</span><span>features, hidden_features, context_features=None, num_blocks=2, use_residual_blocks=True, random_mask=False, activation=&lt;function relu&gt;, dropout_probability=0.0, use_batch_norm=False, integrand_net_layers=[50, 50, 50], cond_size=20, nb_steps=20, solver='CCParallel')</span>
</code></dt>
<dd>
<div class="desc"><p>An unconstrained monotonic neural networks autoregressive layer that transforms the variables.</p>
<p>Reference:</p>
<blockquote>
<p>A. Wehenkel and G. Louppe, Unconstrained Monotonic Neural Networks, NeurIPS2019.</p>
</blockquote>
<p>---- Specific arguments ----
integrand_net_layers: the layers dimension to put in the integrand network.
cond_size: The embedding size for the conditioning factors.
nb_steps: The number of integration steps.
solver: The quadrature algorithm - CC or CCParallel. Both implements Clenshaw-Curtis quadrature with
Leibniz rule for backward computation. CCParallel pass all the evaluation points (nb_steps) at once, it is faster
but requires more memory.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MaskedUMNNAutoregressiveTransform(AutoregressiveTransform):
    &#34;&#34;&#34;An unconstrained monotonic neural networks autoregressive layer that transforms the variables.

        Reference:
        &gt; A. Wehenkel and G. Louppe, Unconstrained Monotonic Neural Networks, NeurIPS2019.

        ---- Specific arguments ----
        integrand_net_layers: the layers dimension to put in the integrand network.
        cond_size: The embedding size for the conditioning factors.
        nb_steps: The number of integration steps.
        solver: The quadrature algorithm - CC or CCParallel. Both implements Clenshaw-Curtis quadrature with
        Leibniz rule for backward computation. CCParallel pass all the evaluation points (nb_steps) at once, it is faster
        but requires more memory.
        &#34;&#34;&#34;

    def __init__(
            self,
            features,
            hidden_features,
            context_features=None,
            num_blocks=2,
            use_residual_blocks=True,
            random_mask=False,
            activation=F.relu,
            dropout_probability=0.0,
            use_batch_norm=False,
            integrand_net_layers=[50, 50, 50],
            cond_size=20,
            nb_steps=20,
            solver=&#34;CCParallel&#34;,
    ):
        self.features = features
        self.cond_size = cond_size
        made = made_module.MADE(
            features=features,
            hidden_features=hidden_features,
            context_features=context_features,
            num_blocks=num_blocks,
            output_multiplier=self._output_dim_multiplier(),
            use_residual_blocks=use_residual_blocks,
            random_mask=random_mask,
            activation=activation,
            dropout_probability=dropout_probability,
            use_batch_norm=use_batch_norm,
        )
        self._epsilon = 1e-3
        super().__init__(made)
        self.transformer = MonotonicNormalizer(integrand_net_layers, cond_size, nb_steps, solver)

    def _output_dim_multiplier(self):
        return self.cond_size

    def _elementwise_forward(self, inputs, autoregressive_params):
        z, jac = self.transformer(inputs, autoregressive_params.reshape(inputs.shape[0], inputs.shape[1], -1))
        log_det_jac = jac.log().sum(1)
        return z, log_det_jac

    def _elementwise_inverse(self, inputs, autoregressive_params):
        x = self.transformer.inverse_transform(inputs,
                                               autoregressive_params.reshape(inputs.shape[0], inputs.shape[1], -1))
        z, jac = self.transformer(x, autoregressive_params.reshape(inputs.shape[0], inputs.shape[1], -1))
        log_det_jac = -jac.log().sum(1)
        return x, log_det_jac</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform">AutoregressiveTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="../base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedUMNNAutoregressiveTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedUMNNAutoregressiveTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.autoregressive.autoregressive.MaskedUMNNAutoregressiveTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform">AutoregressiveTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform.forward" href="../base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="flowcon.transforms.autoregressive" href="index.html">flowcon.transforms.autoregressive</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.main" href="#flowcon.transforms.autoregressive.autoregressive.main">main</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform">AutoregressiveTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform.call_super_init" href="#flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform.dump_patches" href="#flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform.inverse" href="#flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform.inverse">inverse</a></code></li>
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform.training" href="#flowcon.transforms.autoregressive.autoregressive.AutoregressiveTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedAffineAutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.MaskedAffineAutoregressiveTransform">MaskedAffineAutoregressiveTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedAffineAutoregressiveTransform.call_super_init" href="#flowcon.transforms.autoregressive.autoregressive.MaskedAffineAutoregressiveTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedAffineAutoregressiveTransform.dump_patches" href="#flowcon.transforms.autoregressive.autoregressive.MaskedAffineAutoregressiveTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedAffineAutoregressiveTransform.training" href="#flowcon.transforms.autoregressive.autoregressive.MaskedAffineAutoregressiveTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseCubicAutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseCubicAutoregressiveTransform">MaskedPiecewiseCubicAutoregressiveTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseCubicAutoregressiveTransform.call_super_init" href="#flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseCubicAutoregressiveTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseCubicAutoregressiveTransform.dump_patches" href="#flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseCubicAutoregressiveTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseCubicAutoregressiveTransform.training" href="#flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseCubicAutoregressiveTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseLinearAutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseLinearAutoregressiveTransform">MaskedPiecewiseLinearAutoregressiveTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseLinearAutoregressiveTransform.call_super_init" href="#flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseLinearAutoregressiveTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseLinearAutoregressiveTransform.dump_patches" href="#flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseLinearAutoregressiveTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseLinearAutoregressiveTransform.training" href="#flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseLinearAutoregressiveTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseQuadraticAutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseQuadraticAutoregressiveTransform">MaskedPiecewiseQuadraticAutoregressiveTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseQuadraticAutoregressiveTransform.call_super_init" href="#flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseQuadraticAutoregressiveTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseQuadraticAutoregressiveTransform.dump_patches" href="#flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseQuadraticAutoregressiveTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseQuadraticAutoregressiveTransform.training" href="#flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseQuadraticAutoregressiveTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseRationalQuadraticAutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseRationalQuadraticAutoregressiveTransform">MaskedPiecewiseRationalQuadraticAutoregressiveTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseRationalQuadraticAutoregressiveTransform.call_super_init" href="#flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseRationalQuadraticAutoregressiveTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseRationalQuadraticAutoregressiveTransform.dump_patches" href="#flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseRationalQuadraticAutoregressiveTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseRationalQuadraticAutoregressiveTransform.training" href="#flowcon.transforms.autoregressive.autoregressive.MaskedPiecewiseRationalQuadraticAutoregressiveTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedShiftAutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.MaskedShiftAutoregressiveTransform">MaskedShiftAutoregressiveTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedShiftAutoregressiveTransform.call_super_init" href="#flowcon.transforms.autoregressive.autoregressive.MaskedShiftAutoregressiveTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedShiftAutoregressiveTransform.dump_patches" href="#flowcon.transforms.autoregressive.autoregressive.MaskedShiftAutoregressiveTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedShiftAutoregressiveTransform.training" href="#flowcon.transforms.autoregressive.autoregressive.MaskedShiftAutoregressiveTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedSumOfSigmoidsTransform" href="#flowcon.transforms.autoregressive.autoregressive.MaskedSumOfSigmoidsTransform">MaskedSumOfSigmoidsTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedSumOfSigmoidsTransform.call_super_init" href="#flowcon.transforms.autoregressive.autoregressive.MaskedSumOfSigmoidsTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedSumOfSigmoidsTransform.dump_patches" href="#flowcon.transforms.autoregressive.autoregressive.MaskedSumOfSigmoidsTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedSumOfSigmoidsTransform.training" href="#flowcon.transforms.autoregressive.autoregressive.MaskedSumOfSigmoidsTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedUMNNAutoregressiveTransform" href="#flowcon.transforms.autoregressive.autoregressive.MaskedUMNNAutoregressiveTransform">MaskedUMNNAutoregressiveTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedUMNNAutoregressiveTransform.call_super_init" href="#flowcon.transforms.autoregressive.autoregressive.MaskedUMNNAutoregressiveTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedUMNNAutoregressiveTransform.dump_patches" href="#flowcon.transforms.autoregressive.autoregressive.MaskedUMNNAutoregressiveTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.autoregressive.autoregressive.MaskedUMNNAutoregressiveTransform.training" href="#flowcon.transforms.autoregressive.autoregressive.MaskedUMNNAutoregressiveTransform.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.0</a>.</p>
</footer>
</body>
</html>
