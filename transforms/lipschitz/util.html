<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>flowcon.transforms.lipschitz.util API documentation</title>
<meta name="description" content="Adapted from Implicit Normalizing Flow (ICLR 2021).
Link: https://github.com/thu-ml/implicit-normalizing-flows/blob/master/lib/layers/broyden.py" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>flowcon.transforms.lipschitz.util</code></h1>
</header>
<section id="section-intro">
<p>Adapted from Implicit Normalizing Flow (ICLR 2021).
Link: <a href="https://github.com/thu-ml/implicit-normalizing-flows/blob/master/lib/layers/broyden.py">https://github.com/thu-ml/implicit-normalizing-flows/blob/master/lib/layers/broyden.py</a></p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;
Adapted from Implicit Normalizing Flow (ICLR 2021).
Link: https://github.com/thu-ml/implicit-normalizing-flows/blob/master/lib/layers/broyden.py
&#39;&#39;&#39;

import torch
from torch import nn, nn as nn
import torch.nn.functional as functional
from torch.autograd import Function
import numpy as np
import math
import pickle
import sys
import os
from typing import *
from scipy.optimize import root
from scipy.special import expit
import time

import logging

logger = logging.getLogger()


def find_fixed_point_noaccel(f, x0, threshold=1000, eps=1e-5):
    b = x0.size(0)
    b_shape = (b,)
    for _ in range(1, len(x0.shape)):
        b_shape = b_shape + (1,)
    alpha = 0.5 * torch.ones(b_shape, device=x0.device)
    x, x_prev = (1 - alpha) * x0 + (alpha) * f(x0), x0
    i = 0
    tol = eps + eps * x0.abs()

    best_err = 1e9 * torch.ones(b_shape, device=x0.device)
    best_iter = torch.zeros(b_shape, dtype=torch.int64, device=x0.device)
    while True:
        fx = f(x)
        err_values = torch.abs(fx - x) / tol
        cur_err = torch.max(err_values.view(b, -1), dim=1)[0].view(b_shape)

        if torch.all(cur_err &lt; 1.):
            break
        alpha = torch.where(torch.logical_and(cur_err &gt;= best_err, i &gt;= best_iter + 30),
                            alpha * 0.9,
                            alpha)
        alpha = torch.max(alpha, 0.1 * torch.ones_like(alpha))
        best_iter = torch.where(torch.logical_or(cur_err &lt; best_err, i &gt;= best_iter + 30),
                                i * torch.ones(b_shape, dtype=torch.int64, device=x0.device),
                                best_iter)
        best_err = torch.min(best_err, cur_err)

        x, x_prev = (1 - alpha) * x + (alpha) * fx, x
        i += 1
        if i &gt; threshold:
            dx = torch.abs(f(x) - x)
            rel_err = torch.max(dx / tol).item()
            abs_err = torch.max(dx).item()
            if rel_err &gt; 3 or abs_err &gt; 3 * max(eps, 1e-9):
                logger.info(&#39;Relative/Absolute error maximum: %.10f/%.10f&#39; % (rel_err, abs_err))
                logger.info(&#39;Iterations exceeded %d for fixed point noaccel.&#39; % (threshold))
            break
    return x


def find_fixed_point(f, x0, threshold=1000, eps=1e-5):
    b = x0.size(0)

    def g(w):
        return f(w.view(x0.shape)).view(b, -1)

    with torch.no_grad():
        X0 = x0.view(b, -1)
        X1 = g(X0)
        Gnm1 = X1
        dXnm1 = X1 - X0
        Xn = X1

        tol = eps + eps * X0.abs()
        best_err = math.inf
        best_iter = 0
        n = 1
        while n &lt; threshold:
            Gn = g(Xn)
            dXn = Gn - Xn
            cur_err = torch.max(torch.abs(dXn) / tol).item()
            if cur_err &lt;= 1.:
                break
            if cur_err &lt; best_err:
                best_err = cur_err
                best_iter = n
            elif n &gt;= best_iter + 10:
                break

            d2Xn = dXn - dXnm1
            d2Xn_norm = torch.linalg.vector_norm(d2Xn, dim=1)
            mult = (d2Xn * dXn).sum(dim=1) / (d2Xn_norm ** 2 + 1e-8)
            mult = mult.view(b, 1)
            Xnp1 = Gn - mult * (Gn - Gnm1)

            dXnm1 = dXn
            Gnm1 = Gn
            Xn = Xnp1
            n = n + 1

        rel_err = torch.max(torch.abs(dXn) / tol).item()
        if rel_err &gt; 1:
            abs_err = torch.max(torch.abs(dXn)).item()
            if rel_err &gt; 10:
                return find_fixed_point_noaccel(f, x0, threshold=threshold, eps=eps)
            else:
                return find_fixed_point_noaccel(f, Xn.view(x0.shape), threshold=threshold, eps=eps)
        else:
            return Xn.view(x0.shape)




class ParameterGenerator(torch.nn.Module):
    def sample_parameters(self, training=True) -&gt; Tuple[Callable, int]:
        pass



class Sampler():
    def rcdf_fn(self, k, offset):
        pass

    def sample_fn(self, m):
        pass

    @classmethod
    def build_sampler(cls, n_dist, **kwargs):
        if n_dist == &#39;geometric&#39;:
            return GeometricSampler(kwargs.get(&#34;geom_p&#34;))
        elif n_dist == &#39;poisson&#39;:
            return GeometricSampler(kwargs.get(&#34;lamb&#34;))
        else:
            raise NotImplementedError(f&#34;Unknown sampler &#39;{n_dist}&#39;.&#34;)


class GeometricSampler(Sampler):
    def __init__(self, geom_p):
        self.geom_p = expit(geom_p)

    def sample_fn(self, m):
        return self.geometric_sample(self.geom_p, m)

    def rcdf_fn(self, k, offset):
        return self.geometric_1mcdf(self.geom_p, k, offset)

    @staticmethod
    def geometric_sample(p, n_samples):
        return np.random.geometric(p, n_samples)

    @staticmethod
    def geometric_1mcdf(p, k, offset):
        if k &lt;= offset:
            return 1.
        else:
            k = k - offset
        &#34;&#34;&#34;P(n &gt;= k)&#34;&#34;&#34;
        return (1 - p) ** max(k - 1, 0)


class UnbiasedParameterGenerator(ParameterGenerator):
    geom_p = 0.5
    geom_p = np.log(geom_p) - np.log(1. - geom_p)

    def __init__(self, n_exact_terms, n_samples):
        super().__init__()
        self.sampler = GeometricSampler(self.geom_p)
        self.n_exact_terms = n_exact_terms
        self.n_samples = n_samples

        # store the samples of n.
        # self.register_buffer(&#39;last_n_samples&#39;, torch.zeros(self.n_samples))

    def sample_parameters(self, training=True):
        n_samples = self.sampler.sample_fn(m=self.n_samples)
        # n_samples = sample_fn(self.n_samples)
        n_power_series = max(n_samples) + self.n_exact_terms

        if not training:
            n_power_series += 20

        def coeff_fn(k):
            rcdf_term = self.sampler.rcdf_fn(k, self.n_exact_terms)
            return 1 / rcdf_term * sum(n_samples &gt;= k - self.n_exact_terms) / len(n_samples)

        return coeff_fn, n_power_series


class BiasedParameterGenerator(ParameterGenerator):
    def __init__(self, n_power_series):
        super().__init__()

        self.n_power_series = n_power_series

    def sample_parameters(self, training=True):
        def coeff_fn(k):
            return 1

        return coeff_fn, self.n_power_series


class Sine(nn.Module):
    def __init__(self, w0=1.):
        super().__init__()
        self.w0 = w0

    def forward(self, x):
        return torch.sin(self.w0 * x) / self.w0


def exists(val):
    return val is not None</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="flowcon.transforms.lipschitz.util.exists"><code class="name flex">
<span>def <span class="ident">exists</span></span>(<span>val)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def exists(val):
    return val is not None</code></pre>
</details>
</dd>
<dt id="flowcon.transforms.lipschitz.util.find_fixed_point"><code class="name flex">
<span>def <span class="ident">find_fixed_point</span></span>(<span>f, x0, threshold=1000, eps=1e-05)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_fixed_point(f, x0, threshold=1000, eps=1e-5):
    b = x0.size(0)

    def g(w):
        return f(w.view(x0.shape)).view(b, -1)

    with torch.no_grad():
        X0 = x0.view(b, -1)
        X1 = g(X0)
        Gnm1 = X1
        dXnm1 = X1 - X0
        Xn = X1

        tol = eps + eps * X0.abs()
        best_err = math.inf
        best_iter = 0
        n = 1
        while n &lt; threshold:
            Gn = g(Xn)
            dXn = Gn - Xn
            cur_err = torch.max(torch.abs(dXn) / tol).item()
            if cur_err &lt;= 1.:
                break
            if cur_err &lt; best_err:
                best_err = cur_err
                best_iter = n
            elif n &gt;= best_iter + 10:
                break

            d2Xn = dXn - dXnm1
            d2Xn_norm = torch.linalg.vector_norm(d2Xn, dim=1)
            mult = (d2Xn * dXn).sum(dim=1) / (d2Xn_norm ** 2 + 1e-8)
            mult = mult.view(b, 1)
            Xnp1 = Gn - mult * (Gn - Gnm1)

            dXnm1 = dXn
            Gnm1 = Gn
            Xn = Xnp1
            n = n + 1

        rel_err = torch.max(torch.abs(dXn) / tol).item()
        if rel_err &gt; 1:
            abs_err = torch.max(torch.abs(dXn)).item()
            if rel_err &gt; 10:
                return find_fixed_point_noaccel(f, x0, threshold=threshold, eps=eps)
            else:
                return find_fixed_point_noaccel(f, Xn.view(x0.shape), threshold=threshold, eps=eps)
        else:
            return Xn.view(x0.shape)</code></pre>
</details>
</dd>
<dt id="flowcon.transforms.lipschitz.util.find_fixed_point_noaccel"><code class="name flex">
<span>def <span class="ident">find_fixed_point_noaccel</span></span>(<span>f, x0, threshold=1000, eps=1e-05)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_fixed_point_noaccel(f, x0, threshold=1000, eps=1e-5):
    b = x0.size(0)
    b_shape = (b,)
    for _ in range(1, len(x0.shape)):
        b_shape = b_shape + (1,)
    alpha = 0.5 * torch.ones(b_shape, device=x0.device)
    x, x_prev = (1 - alpha) * x0 + (alpha) * f(x0), x0
    i = 0
    tol = eps + eps * x0.abs()

    best_err = 1e9 * torch.ones(b_shape, device=x0.device)
    best_iter = torch.zeros(b_shape, dtype=torch.int64, device=x0.device)
    while True:
        fx = f(x)
        err_values = torch.abs(fx - x) / tol
        cur_err = torch.max(err_values.view(b, -1), dim=1)[0].view(b_shape)

        if torch.all(cur_err &lt; 1.):
            break
        alpha = torch.where(torch.logical_and(cur_err &gt;= best_err, i &gt;= best_iter + 30),
                            alpha * 0.9,
                            alpha)
        alpha = torch.max(alpha, 0.1 * torch.ones_like(alpha))
        best_iter = torch.where(torch.logical_or(cur_err &lt; best_err, i &gt;= best_iter + 30),
                                i * torch.ones(b_shape, dtype=torch.int64, device=x0.device),
                                best_iter)
        best_err = torch.min(best_err, cur_err)

        x, x_prev = (1 - alpha) * x + (alpha) * fx, x
        i += 1
        if i &gt; threshold:
            dx = torch.abs(f(x) - x)
            rel_err = torch.max(dx / tol).item()
            abs_err = torch.max(dx).item()
            if rel_err &gt; 3 or abs_err &gt; 3 * max(eps, 1e-9):
                logger.info(&#39;Relative/Absolute error maximum: %.10f/%.10f&#39; % (rel_err, abs_err))
                logger.info(&#39;Iterations exceeded %d for fixed point noaccel.&#39; % (threshold))
            break
    return x</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="flowcon.transforms.lipschitz.util.BiasedParameterGenerator"><code class="flex name class">
<span>class <span class="ident">BiasedParameterGenerator</span></span>
<span>(</span><span>n_power_series)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BiasedParameterGenerator(ParameterGenerator):
    def __init__(self, n_power_series):
        super().__init__()

        self.n_power_series = n_power_series

    def sample_parameters(self, training=True):
        def coeff_fn(k):
            return 1

        return coeff_fn, self.n_power_series</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.lipschitz.util.ParameterGenerator" href="#flowcon.transforms.lipschitz.util.ParameterGenerator">ParameterGenerator</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.lipschitz.util.BiasedParameterGenerator.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.lipschitz.util.BiasedParameterGenerator.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.lipschitz.util.BiasedParameterGenerator.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.lipschitz.util.BiasedParameterGenerator.sample_parameters"><code class="name flex">
<span>def <span class="ident">sample_parameters</span></span>(<span>self, training=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_parameters(self, training=True):
    def coeff_fn(k):
        return 1

    return coeff_fn, self.n_power_series</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.lipschitz.util.ParameterGenerator" href="#flowcon.transforms.lipschitz.util.ParameterGenerator">ParameterGenerator</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.lipschitz.util.ParameterGenerator.forward" href="#flowcon.transforms.lipschitz.util.ParameterGenerator.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.lipschitz.util.GeometricSampler"><code class="flex name class">
<span>class <span class="ident">GeometricSampler</span></span>
<span>(</span><span>geom_p)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GeometricSampler(Sampler):
    def __init__(self, geom_p):
        self.geom_p = expit(geom_p)

    def sample_fn(self, m):
        return self.geometric_sample(self.geom_p, m)

    def rcdf_fn(self, k, offset):
        return self.geometric_1mcdf(self.geom_p, k, offset)

    @staticmethod
    def geometric_sample(p, n_samples):
        return np.random.geometric(p, n_samples)

    @staticmethod
    def geometric_1mcdf(p, k, offset):
        if k &lt;= offset:
            return 1.
        else:
            k = k - offset
        &#34;&#34;&#34;P(n &gt;= k)&#34;&#34;&#34;
        return (1 - p) ** max(k - 1, 0)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.lipschitz.util.Sampler" href="#flowcon.transforms.lipschitz.util.Sampler">Sampler</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="flowcon.transforms.lipschitz.util.GeometricSampler.geometric_1mcdf"><code class="name flex">
<span>def <span class="ident">geometric_1mcdf</span></span>(<span>p, k, offset)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def geometric_1mcdf(p, k, offset):
    if k &lt;= offset:
        return 1.
    else:
        k = k - offset
    &#34;&#34;&#34;P(n &gt;= k)&#34;&#34;&#34;
    return (1 - p) ** max(k - 1, 0)</code></pre>
</details>
</dd>
<dt id="flowcon.transforms.lipschitz.util.GeometricSampler.geometric_sample"><code class="name flex">
<span>def <span class="ident">geometric_sample</span></span>(<span>p, n_samples)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def geometric_sample(p, n_samples):
    return np.random.geometric(p, n_samples)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.lipschitz.util.GeometricSampler.rcdf_fn"><code class="name flex">
<span>def <span class="ident">rcdf_fn</span></span>(<span>self, k, offset)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rcdf_fn(self, k, offset):
    return self.geometric_1mcdf(self.geom_p, k, offset)</code></pre>
</details>
</dd>
<dt id="flowcon.transforms.lipschitz.util.GeometricSampler.sample_fn"><code class="name flex">
<span>def <span class="ident">sample_fn</span></span>(<span>self, m)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_fn(self, m):
    return self.geometric_sample(self.geom_p, m)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="flowcon.transforms.lipschitz.util.ParameterGenerator"><code class="flex name class">
<span>class <span class="ident">ParameterGenerator</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ParameterGenerator(torch.nn.Module):
    def sample_parameters(self, training=True) -&gt; Tuple[Callable, int]:
        pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.lipschitz.util.BiasedParameterGenerator" href="#flowcon.transforms.lipschitz.util.BiasedParameterGenerator">BiasedParameterGenerator</a></li>
<li><a title="flowcon.transforms.lipschitz.util.UnbiasedParameterGenerator" href="#flowcon.transforms.lipschitz.util.UnbiasedParameterGenerator">UnbiasedParameterGenerator</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.lipschitz.util.ParameterGenerator.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.lipschitz.util.ParameterGenerator.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.lipschitz.util.ParameterGenerator.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.lipschitz.util.ParameterGenerator.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, *input: Any) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def _forward_unimplemented(self, *input: Any) -&gt; None:
    r&#34;&#34;&#34;Define the computation performed at every call.

    Should be overridden by all subclasses.

    .. note::
        Although the recipe for forward pass needs to be defined within
        this function, one should call the :class:`Module` instance afterwards
        instead of this since the former takes care of running the
        registered hooks while the latter silently ignores them.
    &#34;&#34;&#34;
    raise NotImplementedError(f&#34;Module [{type(self).__name__}] is missing the required \&#34;forward\&#34; function&#34;)</code></pre>
</details>
</dd>
<dt id="flowcon.transforms.lipschitz.util.ParameterGenerator.sample_parameters"><code class="name flex">
<span>def <span class="ident">sample_parameters</span></span>(<span>self, training=True) ‑> Tuple[Callable, int]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_parameters(self, training=True) -&gt; Tuple[Callable, int]:
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="flowcon.transforms.lipschitz.util.Sampler"><code class="flex name class">
<span>class <span class="ident">Sampler</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Sampler():
    def rcdf_fn(self, k, offset):
        pass

    def sample_fn(self, m):
        pass

    @classmethod
    def build_sampler(cls, n_dist, **kwargs):
        if n_dist == &#39;geometric&#39;:
            return GeometricSampler(kwargs.get(&#34;geom_p&#34;))
        elif n_dist == &#39;poisson&#39;:
            return GeometricSampler(kwargs.get(&#34;lamb&#34;))
        else:
            raise NotImplementedError(f&#34;Unknown sampler &#39;{n_dist}&#39;.&#34;)</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.lipschitz.util.GeometricSampler" href="#flowcon.transforms.lipschitz.util.GeometricSampler">GeometricSampler</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="flowcon.transforms.lipschitz.util.Sampler.build_sampler"><code class="name flex">
<span>def <span class="ident">build_sampler</span></span>(<span>n_dist, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def build_sampler(cls, n_dist, **kwargs):
    if n_dist == &#39;geometric&#39;:
        return GeometricSampler(kwargs.get(&#34;geom_p&#34;))
    elif n_dist == &#39;poisson&#39;:
        return GeometricSampler(kwargs.get(&#34;lamb&#34;))
    else:
        raise NotImplementedError(f&#34;Unknown sampler &#39;{n_dist}&#39;.&#34;)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.lipschitz.util.Sampler.rcdf_fn"><code class="name flex">
<span>def <span class="ident">rcdf_fn</span></span>(<span>self, k, offset)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rcdf_fn(self, k, offset):
    pass</code></pre>
</details>
</dd>
<dt id="flowcon.transforms.lipschitz.util.Sampler.sample_fn"><code class="name flex">
<span>def <span class="ident">sample_fn</span></span>(<span>self, m)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_fn(self, m):
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="flowcon.transforms.lipschitz.util.Sine"><code class="flex name class">
<span>class <span class="ident">Sine</span></span>
<span>(</span><span>w0=1.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Sine(nn.Module):
    def __init__(self, w0=1.):
        super().__init__()
        self.w0 = w0

    def forward(self, x):
        return torch.sin(self.w0 * x) / self.w0</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.lipschitz.util.Sine.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.lipschitz.util.Sine.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.lipschitz.util.Sine.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.lipschitz.util.Sine.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    return torch.sin(self.w0 * x) / self.w0</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="flowcon.transforms.lipschitz.util.UnbiasedParameterGenerator"><code class="flex name class">
<span>class <span class="ident">UnbiasedParameterGenerator</span></span>
<span>(</span><span>n_exact_terms, n_samples)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class UnbiasedParameterGenerator(ParameterGenerator):
    geom_p = 0.5
    geom_p = np.log(geom_p) - np.log(1. - geom_p)

    def __init__(self, n_exact_terms, n_samples):
        super().__init__()
        self.sampler = GeometricSampler(self.geom_p)
        self.n_exact_terms = n_exact_terms
        self.n_samples = n_samples

        # store the samples of n.
        # self.register_buffer(&#39;last_n_samples&#39;, torch.zeros(self.n_samples))

    def sample_parameters(self, training=True):
        n_samples = self.sampler.sample_fn(m=self.n_samples)
        # n_samples = sample_fn(self.n_samples)
        n_power_series = max(n_samples) + self.n_exact_terms

        if not training:
            n_power_series += 20

        def coeff_fn(k):
            rcdf_term = self.sampler.rcdf_fn(k, self.n_exact_terms)
            return 1 / rcdf_term * sum(n_samples &gt;= k - self.n_exact_terms) / len(n_samples)

        return coeff_fn, n_power_series</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.lipschitz.util.ParameterGenerator" href="#flowcon.transforms.lipschitz.util.ParameterGenerator">ParameterGenerator</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.lipschitz.util.UnbiasedParameterGenerator.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.lipschitz.util.UnbiasedParameterGenerator.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.lipschitz.util.UnbiasedParameterGenerator.geom_p"><code class="name">var <span class="ident">geom_p</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.lipschitz.util.UnbiasedParameterGenerator.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.lipschitz.util.UnbiasedParameterGenerator.sample_parameters"><code class="name flex">
<span>def <span class="ident">sample_parameters</span></span>(<span>self, training=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_parameters(self, training=True):
    n_samples = self.sampler.sample_fn(m=self.n_samples)
    # n_samples = sample_fn(self.n_samples)
    n_power_series = max(n_samples) + self.n_exact_terms

    if not training:
        n_power_series += 20

    def coeff_fn(k):
        rcdf_term = self.sampler.rcdf_fn(k, self.n_exact_terms)
        return 1 / rcdf_term * sum(n_samples &gt;= k - self.n_exact_terms) / len(n_samples)

    return coeff_fn, n_power_series</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.lipschitz.util.ParameterGenerator" href="#flowcon.transforms.lipschitz.util.ParameterGenerator">ParameterGenerator</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.lipschitz.util.ParameterGenerator.forward" href="#flowcon.transforms.lipschitz.util.ParameterGenerator.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="flowcon.transforms.lipschitz" href="index.html">flowcon.transforms.lipschitz</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="flowcon.transforms.lipschitz.util.exists" href="#flowcon.transforms.lipschitz.util.exists">exists</a></code></li>
<li><code><a title="flowcon.transforms.lipschitz.util.find_fixed_point" href="#flowcon.transforms.lipschitz.util.find_fixed_point">find_fixed_point</a></code></li>
<li><code><a title="flowcon.transforms.lipschitz.util.find_fixed_point_noaccel" href="#flowcon.transforms.lipschitz.util.find_fixed_point_noaccel">find_fixed_point_noaccel</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="flowcon.transforms.lipschitz.util.BiasedParameterGenerator" href="#flowcon.transforms.lipschitz.util.BiasedParameterGenerator">BiasedParameterGenerator</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.lipschitz.util.BiasedParameterGenerator.call_super_init" href="#flowcon.transforms.lipschitz.util.BiasedParameterGenerator.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.lipschitz.util.BiasedParameterGenerator.dump_patches" href="#flowcon.transforms.lipschitz.util.BiasedParameterGenerator.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.lipschitz.util.BiasedParameterGenerator.sample_parameters" href="#flowcon.transforms.lipschitz.util.BiasedParameterGenerator.sample_parameters">sample_parameters</a></code></li>
<li><code><a title="flowcon.transforms.lipschitz.util.BiasedParameterGenerator.training" href="#flowcon.transforms.lipschitz.util.BiasedParameterGenerator.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.lipschitz.util.GeometricSampler" href="#flowcon.transforms.lipschitz.util.GeometricSampler">GeometricSampler</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.lipschitz.util.GeometricSampler.geometric_1mcdf" href="#flowcon.transforms.lipschitz.util.GeometricSampler.geometric_1mcdf">geometric_1mcdf</a></code></li>
<li><code><a title="flowcon.transforms.lipschitz.util.GeometricSampler.geometric_sample" href="#flowcon.transforms.lipschitz.util.GeometricSampler.geometric_sample">geometric_sample</a></code></li>
<li><code><a title="flowcon.transforms.lipschitz.util.GeometricSampler.rcdf_fn" href="#flowcon.transforms.lipschitz.util.GeometricSampler.rcdf_fn">rcdf_fn</a></code></li>
<li><code><a title="flowcon.transforms.lipschitz.util.GeometricSampler.sample_fn" href="#flowcon.transforms.lipschitz.util.GeometricSampler.sample_fn">sample_fn</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.lipschitz.util.ParameterGenerator" href="#flowcon.transforms.lipschitz.util.ParameterGenerator">ParameterGenerator</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.lipschitz.util.ParameterGenerator.call_super_init" href="#flowcon.transforms.lipschitz.util.ParameterGenerator.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.lipschitz.util.ParameterGenerator.dump_patches" href="#flowcon.transforms.lipschitz.util.ParameterGenerator.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.lipschitz.util.ParameterGenerator.forward" href="#flowcon.transforms.lipschitz.util.ParameterGenerator.forward">forward</a></code></li>
<li><code><a title="flowcon.transforms.lipschitz.util.ParameterGenerator.sample_parameters" href="#flowcon.transforms.lipschitz.util.ParameterGenerator.sample_parameters">sample_parameters</a></code></li>
<li><code><a title="flowcon.transforms.lipschitz.util.ParameterGenerator.training" href="#flowcon.transforms.lipschitz.util.ParameterGenerator.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.lipschitz.util.Sampler" href="#flowcon.transforms.lipschitz.util.Sampler">Sampler</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.lipschitz.util.Sampler.build_sampler" href="#flowcon.transforms.lipschitz.util.Sampler.build_sampler">build_sampler</a></code></li>
<li><code><a title="flowcon.transforms.lipschitz.util.Sampler.rcdf_fn" href="#flowcon.transforms.lipschitz.util.Sampler.rcdf_fn">rcdf_fn</a></code></li>
<li><code><a title="flowcon.transforms.lipschitz.util.Sampler.sample_fn" href="#flowcon.transforms.lipschitz.util.Sampler.sample_fn">sample_fn</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.lipschitz.util.Sine" href="#flowcon.transforms.lipschitz.util.Sine">Sine</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.lipschitz.util.Sine.call_super_init" href="#flowcon.transforms.lipschitz.util.Sine.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.lipschitz.util.Sine.dump_patches" href="#flowcon.transforms.lipschitz.util.Sine.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.lipschitz.util.Sine.forward" href="#flowcon.transforms.lipschitz.util.Sine.forward">forward</a></code></li>
<li><code><a title="flowcon.transforms.lipschitz.util.Sine.training" href="#flowcon.transforms.lipschitz.util.Sine.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.lipschitz.util.UnbiasedParameterGenerator" href="#flowcon.transforms.lipschitz.util.UnbiasedParameterGenerator">UnbiasedParameterGenerator</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.lipschitz.util.UnbiasedParameterGenerator.call_super_init" href="#flowcon.transforms.lipschitz.util.UnbiasedParameterGenerator.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.lipschitz.util.UnbiasedParameterGenerator.dump_patches" href="#flowcon.transforms.lipschitz.util.UnbiasedParameterGenerator.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.lipschitz.util.UnbiasedParameterGenerator.geom_p" href="#flowcon.transforms.lipschitz.util.UnbiasedParameterGenerator.geom_p">geom_p</a></code></li>
<li><code><a title="flowcon.transforms.lipschitz.util.UnbiasedParameterGenerator.sample_parameters" href="#flowcon.transforms.lipschitz.util.UnbiasedParameterGenerator.sample_parameters">sample_parameters</a></code></li>
<li><code><a title="flowcon.transforms.lipschitz.util.UnbiasedParameterGenerator.training" href="#flowcon.transforms.lipschitz.util.UnbiasedParameterGenerator.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>