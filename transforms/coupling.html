<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>flowcon.transforms.coupling API documentation</title>
<meta name="description" content="Implementations of various coupling layers." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>flowcon.transforms.coupling</code></h1>
</header>
<section id="section-intro">
<p>Implementations of various coupling layers.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Implementations of various coupling layers.&#34;&#34;&#34;
import warnings

import numpy as np
import torch
from torch.nn.functional import softplus

from flowcon.transforms import splines
from flowcon.transforms.base import Transform
from flowcon.transforms.nonlinearities import (
    PiecewiseCubicCDF,
    PiecewiseLinearCDF,
    PiecewiseQuadraticCDF,
    PiecewiseRationalQuadraticCDF,
)
from flowcon.utils import torchutils
from flowcon.transforms.UMNN import *


class CouplingTransform(Transform):
    &#34;&#34;&#34;A base class for coupling layers. Supports 2D inputs (NxD), as well as 4D inputs for
    images (NxCxHxW). For images the splitting is done on the channel dimension, using the
    provided 1D mask.&#34;&#34;&#34;

    def __init__(self, mask, transform_net_create_fn, unconditional_transform=None):
        &#34;&#34;&#34;
        Constructor.

        Args:
            mask: a 1-dim tensor, tuple or list. It indexes inputs as follows:
                * If `mask[i] &gt; 0`, `input[i]` will be transformed.
                * If `mask[i] &lt;= 0`, `input[i]` will be passed unchanged.
        &#34;&#34;&#34;
        mask = torch.as_tensor(mask)
        if mask.dim() != 1:
            raise ValueError(&#34;Mask must be a 1-dim tensor.&#34;)
        if mask.numel() &lt;= 0:
            raise ValueError(&#34;Mask can&#39;t be empty.&#34;)

        super().__init__()
        self.features = len(mask)
        features_vector = torch.arange(self.features)

        self.register_buffer(
            &#34;identity_features&#34;, features_vector.masked_select(mask &lt;= 0)
        )
        self.register_buffer(
            &#34;transform_features&#34;, features_vector.masked_select(mask &gt; 0)
        )

        assert self.num_identity_features + self.num_transform_features == self.features

        self.transform_net = transform_net_create_fn(
            self.num_identity_features,
            self.num_transform_features * self._transform_dim_multiplier(),
        )

        if unconditional_transform is None:
            self.unconditional_transform = None
        else:
            self.unconditional_transform = unconditional_transform(
                features=self.num_identity_features
            )

    @property
    def num_identity_features(self):
        return len(self.identity_features)

    @property
    def num_transform_features(self):
        return len(self.transform_features)

    def forward(self, inputs, context=None):
        if inputs.dim() not in [2, 4]:
            raise ValueError(&#34;Inputs must be a 2D or a 4D tensor.&#34;)

        if inputs.shape[1] != self.features:
            raise ValueError(
                &#34;Expected features = {}, got {}.&#34;.format(self.features, inputs.shape[1])
            )

        identity_split = inputs[:, self.identity_features, ...]
        transform_split = inputs[:, self.transform_features, ...]

        transform_params = self.transform_net(identity_split, context)
        transform_split, logabsdet = self._coupling_transform_forward(
            inputs=transform_split, transform_params=transform_params
        )

        if self.unconditional_transform is not None:
            identity_split, logabsdet_identity = self.unconditional_transform(
                identity_split, context
            )
            logabsdet += logabsdet_identity

        outputs = torch.empty_like(inputs)
        outputs[:, self.identity_features, ...] = identity_split
        outputs[:, self.transform_features, ...] = transform_split

        return outputs, logabsdet

    def inverse(self, inputs, context=None):
        if inputs.dim() not in [2, 4]:
            raise ValueError(&#34;Inputs must be a 2D or a 4D tensor.&#34;)

        if inputs.shape[1] != self.features:
            raise ValueError(
                &#34;Expected features = {}, got {}.&#34;.format(self.features, inputs.shape[1])
            )

        identity_split = inputs[:, self.identity_features, ...]
        transform_split = inputs[:, self.transform_features, ...]

        logabsdet = 0.0
        if self.unconditional_transform is not None:
            identity_split, logabsdet = self.unconditional_transform.inverse(
                identity_split, context
            )

        transform_params = self.transform_net(identity_split, context)
        transform_split, logabsdet_split = self._coupling_transform_inverse(
            inputs=transform_split, transform_params=transform_params
        )
        logabsdet += logabsdet_split

        outputs = torch.empty_like(inputs)
        outputs[:, self.identity_features] = identity_split
        outputs[:, self.transform_features] = transform_split

        return outputs, logabsdet

    def _transform_dim_multiplier(self):
        &#34;&#34;&#34;Number of features to output for each transform dimension.&#34;&#34;&#34;
        raise NotImplementedError()

    def _coupling_transform_forward(self, inputs, transform_params):
        &#34;&#34;&#34;Forward pass of the coupling transform.&#34;&#34;&#34;
        raise NotImplementedError()

    def _coupling_transform_inverse(self, inputs, transform_params):
        &#34;&#34;&#34;Inverse of the coupling transform.&#34;&#34;&#34;
        raise NotImplementedError()


class UMNNCouplingTransform(CouplingTransform):
    &#34;&#34;&#34;An unconstrained monotonic neural networks coupling layer that transforms the variables.

    Reference:
    &gt; A. Wehenkel and G. Louppe, Unconstrained Monotonic Neural Networks, NeurIPS2019.

    ---- Specific arguments ----
        integrand_net_layers: the layers dimension to put in the integrand network.
        cond_size: The embedding size for the conditioning factors.
        nb_steps: The number of integration steps.
        solver: The quadrature algorithm - CC or CCParallel. Both implements Clenshaw-Curtis quadrature with
        Leibniz rule for backward computation. CCParallel pass all the evaluation points (nb_steps) at once, it is faster
        but requires more memory.

    &#34;&#34;&#34;
    def __init__(
        self,
        mask,
        transform_net_create_fn,
        integrand_net_layers=[50, 50, 50],
        cond_size=20,
        nb_steps=20,
        solver=&#34;CCParallel&#34;,
        apply_unconditional_transform=False
    ):

        if apply_unconditional_transform:
            unconditional_transform = lambda features: MonotonicNormalizer(integrand_net_layers, 0, nb_steps, solver)
        else:
            unconditional_transform = None
        self.cond_size = cond_size
        super().__init__(
            mask,
            transform_net_create_fn,
            unconditional_transform=unconditional_transform,
        )

        self.transformer = MonotonicNormalizer(integrand_net_layers, cond_size, nb_steps, solver)

    def _transform_dim_multiplier(self):
        return self.cond_size

    def _coupling_transform_forward(self, inputs, transform_params):
        if len(inputs.shape) == 2:
            z, jac = self.transformer(inputs, transform_params.reshape(inputs.shape[0], inputs.shape[1], -1))
            log_det_jac = jac.log().sum(1)
            return z, log_det_jac
        else:
            B, C, H, W = inputs.shape
            z, jac = self.transformer(inputs.permute(0, 2, 3, 1).reshape(-1, inputs.shape[1]), transform_params.permute(0, 2, 3, 1).reshape(-1, 1, transform_params.shape[1]))
            log_det_jac = jac.log().reshape(B, -1).sum(1)
            return z.reshape(B, H, W, C).permute(0, 3, 1, 2), log_det_jac

    def _coupling_transform_inverse(self, inputs, transform_params):
        if len(inputs.shape) == 2:
            x = self.transformer.inverse_transform(inputs, transform_params.reshape(inputs.shape[0], inputs.shape[1], -1))
            z, jac = self.transformer(x, transform_params.reshape(inputs.shape[0], inputs.shape[1], -1))
            log_det_jac = -jac.log().sum(1)
            return x, log_det_jac
        else:
            B, C, H, W = inputs.shape
            x = self.transformer.inverse_transform(inputs.permute(0, 2, 3, 1).reshape(-1, inputs.shape[1]), transform_params.permute(0, 2, 3, 1).reshape(-1, 1, transform_params.shape[1]))
            z, jac = self.transformer(x, transform_params.permute(0, 2, 3, 1).reshape(-1, 1, transform_params.shape[1]))
            log_det_jac = -jac.log().reshape(B, -1).sum(1)
            return x.reshape(B, H, W, C).permute(0, 3, 1, 2), log_det_jac


class AffineCouplingTransform(CouplingTransform):
    &#34;&#34;&#34;An affine coupling layer that scales and shifts part of the variables.

    Reference:
    &gt; L. Dinh et al., Density estimation using Real NVP, ICLR 2017.

    The user should supply `scale_activation`, the final activation function in the neural network producing the scale tensor.
    Two options are predefined in the class.
    `DEFAULT_SCALE_ACTIVATION` preserves backwards compatibility but only produces scales &lt;= 1.001.
    `GENERAL_SCALE_ACTIVATION` produces scales &lt;= 3, which is more useful in general applications.
    &#34;&#34;&#34;

    DEFAULT_SCALE_ACTIVATION = lambda x : torch.sigmoid(x + 2) + 1e-3
    GENERAL_SCALE_ACTIVATION = lambda x : (softplus(x) + 1e-3).clamp(0, 3)

    def __init__(self, mask, transform_net_create_fn, unconditional_transform=None, scale_activation=DEFAULT_SCALE_ACTIVATION):
        self.scale_activation = scale_activation
        super().__init__(mask, transform_net_create_fn, unconditional_transform)

    def _transform_dim_multiplier(self):
        return 2

    def _scale_and_shift(self, transform_params):
        unconstrained_scale = transform_params[:, self.num_transform_features:, ...]
        shift = transform_params[:, : self.num_transform_features, ...]
        scale = self.scale_activation(unconstrained_scale)
        return scale, shift

    def _coupling_transform_forward(self, inputs, transform_params):
        scale, shift = self._scale_and_shift(transform_params)
        log_scale = torch.log(scale)
        outputs = inputs * scale + shift
        logabsdet = torchutils.sum_except_batch(log_scale, num_batch_dims=1)
        return outputs, logabsdet

    def _coupling_transform_inverse(self, inputs, transform_params):
        scale, shift = self._scale_and_shift(transform_params)
        log_scale = torch.log(scale)
        outputs = (inputs - shift) / scale
        logabsdet = -torchutils.sum_except_batch(log_scale, num_batch_dims=1)
        return outputs, logabsdet


class AdditiveCouplingTransform(AffineCouplingTransform):
    &#34;&#34;&#34;An additive coupling layer, i.e. an affine coupling layer without scaling.

    Reference:
    &gt; L. Dinh et al., NICE:  Non-linear  Independent  Components  Estimation,
    &gt; arXiv:1410.8516, 2014.
    &#34;&#34;&#34;

    def _transform_dim_multiplier(self):
        return 1

    def _scale_and_shift(self, transform_params):
        shift = transform_params
        scale = torch.ones_like(shift)
        return scale, shift


class PiecewiseCouplingTransform(CouplingTransform):
    def _coupling_transform_forward(self, inputs, transform_params):
        return self._coupling_transform(inputs, transform_params, inverse=False)

    def _coupling_transform_inverse(self, inputs, transform_params):
        return self._coupling_transform(inputs, transform_params, inverse=True)

    def _coupling_transform(self, inputs, transform_params, inverse=False):
        if inputs.dim() == 4:
            b, c, h, w = inputs.shape
            # For images, reshape transform_params from Bx(C*?)xHxW to BxCxHxWx?
            transform_params = transform_params.reshape(b, c, -1, h, w).permute(
                0, 1, 3, 4, 2
            )
        elif inputs.dim() == 2:
            b, d = inputs.shape
            # For 2D data, reshape transform_params from Bx(D*?) to BxDx?
            transform_params = transform_params.reshape(b, d, -1)

        outputs, logabsdet = self._piecewise_cdf(inputs, transform_params, inverse)

        return outputs, torchutils.sum_except_batch(logabsdet)

    def _piecewise_cdf(self, inputs, transform_params, inverse=False):
        raise NotImplementedError()


class PiecewiseLinearCouplingTransform(PiecewiseCouplingTransform):
    &#34;&#34;&#34;
    Reference:
    &gt; Müller et al., Neural Importance Sampling, arXiv:1808.03856, 2018.
    &#34;&#34;&#34;

    def __init__(
        self,
        mask,
        transform_net_create_fn,
        num_bins=10,
        tails=None,
        tail_bound=1.0,
        apply_unconditional_transform=False,
        img_shape=None,
    ):
        self.num_bins = num_bins
        self.tails = tails
        self.tail_bound = tail_bound

        if apply_unconditional_transform:
            unconditional_transform = lambda features: PiecewiseLinearCDF(
                shape=[features] + (img_shape if img_shape else []),
                num_bins=num_bins,
                tails=tails,
                tail_bound=tail_bound,
            )
        else:
            unconditional_transform = None

        super().__init__(
            mask,
            transform_net_create_fn,
            unconditional_transform=unconditional_transform,
        )

    def _transform_dim_multiplier(self):
        return self.num_bins

    def _piecewise_cdf(self, inputs, transform_params, inverse=False):
        unnormalized_pdf = transform_params

        if self.tails is None:
            return splines.linear_spline(
                inputs=inputs, unnormalized_pdf=unnormalized_pdf, inverse=inverse
            )
        else:
            return splines.unconstrained_linear_spline(
                inputs=inputs,
                unnormalized_pdf=unnormalized_pdf,
                inverse=inverse,
                tails=self.tails,
                tail_bound=self.tail_bound,
            )


class PiecewiseQuadraticCouplingTransform(PiecewiseCouplingTransform):
    &#34;&#34;&#34;
    Reference:
    &gt; Müller et al., Neural Importance Sampling, arXiv:1808.03856, 2018.
    &#34;&#34;&#34;

    def __init__(
        self,
        mask,
        transform_net_create_fn,
        num_bins=10,
        tails=None,
        tail_bound=1.0,
        apply_unconditional_transform=False,
        img_shape=None,
        min_bin_width=splines.quadratic.DEFAULT_MIN_BIN_WIDTH,
        min_bin_height=splines.quadratic.DEFAULT_MIN_BIN_HEIGHT,
    ):
        self.num_bins = num_bins
        self.tails = tails
        self.tail_bound = tail_bound
        self.min_bin_width = min_bin_width
        self.min_bin_height = min_bin_height

        if apply_unconditional_transform:
            unconditional_transform = lambda features: PiecewiseQuadraticCDF(
                shape=[features] + (img_shape if img_shape else []),
                num_bins=num_bins,
                tails=tails,
                tail_bound=tail_bound,
                min_bin_width=min_bin_width,
                min_bin_height=min_bin_height,
            )
        else:
            unconditional_transform = None

        super().__init__(
            mask,
            transform_net_create_fn,
            unconditional_transform=unconditional_transform,
        )

    def _transform_dim_multiplier(self):
        if self.tails == &#34;linear&#34;:
            return self.num_bins * 2 - 1
        else:
            return self.num_bins * 2 + 1

    def _piecewise_cdf(self, inputs, transform_params, inverse=False):
        unnormalized_widths = transform_params[..., : self.num_bins]
        unnormalized_heights = transform_params[..., self.num_bins :]

        if hasattr(self.transform_net, &#34;hidden_features&#34;):
            unnormalized_widths /= np.sqrt(self.transform_net.hidden_features)
            unnormalized_heights /= np.sqrt(self.transform_net.hidden_features)

        if self.tails is None:
            spline_fn = splines.quadratic_spline
            spline_kwargs = {}
        else:
            spline_fn = splines.unconstrained_quadratic_spline
            spline_kwargs = {&#34;tails&#34;: self.tails, &#34;tail_bound&#34;: self.tail_bound}

        return spline_fn(
            inputs=inputs,
            unnormalized_widths=unnormalized_widths,
            unnormalized_heights=unnormalized_heights,
            inverse=inverse,
            min_bin_width=self.min_bin_width,
            min_bin_height=self.min_bin_height,
            **spline_kwargs
        )


class PiecewiseCubicCouplingTransform(PiecewiseCouplingTransform):
    def __init__(
        self,
        mask,
        transform_net_create_fn,
        num_bins=10,
        tails=None,
        tail_bound=1.0,
        apply_unconditional_transform=False,
        img_shape=None,
        min_bin_width=splines.cubic.DEFAULT_MIN_BIN_WIDTH,
        min_bin_height=splines.cubic.DEFAULT_MIN_BIN_HEIGHT,
    ):

        self.num_bins = num_bins
        self.min_bin_width = min_bin_width
        self.min_bin_height = min_bin_height
        self.tails = tails
        self.tail_bound = tail_bound

        if apply_unconditional_transform:
            unconditional_transform = lambda features: PiecewiseCubicCDF(
                shape=[features] + (img_shape if img_shape else []),
                num_bins=num_bins,
                tails=tails,
                tail_bound=tail_bound,
                min_bin_width=min_bin_width,
                min_bin_height=min_bin_height,
            )
        else:
            unconditional_transform = None

        super().__init__(
            mask,
            transform_net_create_fn,
            unconditional_transform=unconditional_transform,
        )

    def _transform_dim_multiplier(self):
        return self.num_bins * 2 + 2

    def _piecewise_cdf(self, inputs, transform_params, inverse=False):
        unnormalized_widths = transform_params[..., : self.num_bins]
        unnormalized_heights = transform_params[..., self.num_bins : 2 * self.num_bins]
        unnorm_derivatives_left = transform_params[..., 2 * self.num_bins][..., None]
        unnorm_derivatives_right = transform_params[..., 2 * self.num_bins + 1][
            ..., None
        ]

        if hasattr(self.transform_net, &#34;hidden_features&#34;):
            unnormalized_widths /= np.sqrt(self.transform_net.hidden_features)
            unnormalized_heights /= np.sqrt(self.transform_net.hidden_features)

        if self.tails is None:
            spline_fn = splines.cubic_spline
            spline_kwargs = {}
        else:
            spline_fn = splines.unconstrained_cubic_spline
            spline_kwargs = {&#34;tails&#34;: self.tails, &#34;tail_bound&#34;: self.tail_bound}

        return spline_fn(
            inputs=inputs,
            unnormalized_widths=unnormalized_widths,
            unnormalized_heights=unnormalized_heights,
            unnorm_derivatives_left=unnorm_derivatives_left,
            unnorm_derivatives_right=unnorm_derivatives_right,
            inverse=inverse,
            min_bin_width=self.min_bin_width,
            min_bin_height=self.min_bin_height,
            **spline_kwargs
        )


class PiecewiseRationalQuadraticCouplingTransform(PiecewiseCouplingTransform):
    def __init__(
        self,
        mask,
        transform_net_create_fn,
        num_bins=10,
        tails=None,
        tail_bound=1.0,
        apply_unconditional_transform=False,
        img_shape=None,
        min_bin_width=splines.rational_quadratic.DEFAULT_MIN_BIN_WIDTH,
        min_bin_height=splines.rational_quadratic.DEFAULT_MIN_BIN_HEIGHT,
        min_derivative=splines.rational_quadratic.DEFAULT_MIN_DERIVATIVE,
    ):

        self.num_bins = num_bins
        self.min_bin_width = min_bin_width
        self.min_bin_height = min_bin_height
        self.min_derivative = min_derivative
        self.tails = tails
        self.tail_bound = tail_bound

        if apply_unconditional_transform:
            unconditional_transform = lambda features: PiecewiseRationalQuadraticCDF(
                shape=[features] + (img_shape if img_shape else []),
                num_bins=num_bins,
                tails=tails,
                tail_bound=tail_bound,
                min_bin_width=min_bin_width,
                min_bin_height=min_bin_height,
                min_derivative=min_derivative,
            )
        else:
            unconditional_transform = None

        super().__init__(
            mask,
            transform_net_create_fn,
            unconditional_transform=unconditional_transform,
        )

    def _transform_dim_multiplier(self):
        if self.tails == &#34;linear&#34;:
            return self.num_bins * 3 - 1
        else:
            return self.num_bins * 3 + 1

    def _piecewise_cdf(self, inputs, transform_params, inverse=False):
        unnormalized_widths = transform_params[..., : self.num_bins]
        unnormalized_heights = transform_params[..., self.num_bins : 2 * self.num_bins]
        unnormalized_derivatives = transform_params[..., 2 * self.num_bins :]

        if hasattr(self.transform_net, &#34;hidden_features&#34;):
            unnormalized_widths /= np.sqrt(self.transform_net.hidden_features)
            unnormalized_heights /= np.sqrt(self.transform_net.hidden_features)
        elif hasattr(self.transform_net, &#34;hidden_channels&#34;):
            unnormalized_widths /= np.sqrt(self.transform_net.hidden_channels)
            unnormalized_heights /= np.sqrt(self.transform_net.hidden_channels)
        else:
            warnings.warn(
                &#34;Inputs to the softmax are not scaled down: initialization might be bad.&#34;
            )

        if self.tails is None:
            spline_fn = splines.rational_quadratic_spline
            spline_kwargs = {}
        else:
            spline_fn = splines.unconstrained_rational_quadratic_spline
            spline_kwargs = {&#34;tails&#34;: self.tails, &#34;tail_bound&#34;: self.tail_bound}

        return spline_fn(
            inputs=inputs,
            unnormalized_widths=unnormalized_widths,
            unnormalized_heights=unnormalized_heights,
            unnormalized_derivatives=unnormalized_derivatives,
            inverse=inverse,
            min_bin_width=self.min_bin_width,
            min_bin_height=self.min_bin_height,
            min_derivative=self.min_derivative,
            **spline_kwargs
        )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="flowcon.transforms.coupling.AdditiveCouplingTransform"><code class="flex name class">
<span>class <span class="ident">AdditiveCouplingTransform</span></span>
<span>(</span><span>mask, transform_net_create_fn, unconditional_transform=None, scale_activation=&lt;function AffineCouplingTransform.&lt;lambda&gt;&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>An additive coupling layer, i.e. an affine coupling layer without scaling.</p>
<p>Reference:</p>
<blockquote>
<p>L. Dinh et al., NICE:
Non-linear
Independent
Components
Estimation,
arXiv:1410.8516, 2014.</p>
</blockquote>
<p>Constructor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>mask</code></strong></dt>
<dd>a 1-dim tensor, tuple or list. It indexes inputs as follows:
* If <code>mask[i] &gt; 0</code>, <code>input[i]</code> will be transformed.
* If <code>mask[i] &lt;= 0</code>, <code>input[i]</code> will be passed unchanged.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AdditiveCouplingTransform(AffineCouplingTransform):
    &#34;&#34;&#34;An additive coupling layer, i.e. an affine coupling layer without scaling.

    Reference:
    &gt; L. Dinh et al., NICE:  Non-linear  Independent  Components  Estimation,
    &gt; arXiv:1410.8516, 2014.
    &#34;&#34;&#34;

    def _transform_dim_multiplier(self):
        return 1

    def _scale_and_shift(self, transform_params):
        shift = transform_params
        scale = torch.ones_like(shift)
        return scale, shift</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.coupling.AffineCouplingTransform" href="#flowcon.transforms.coupling.AffineCouplingTransform">AffineCouplingTransform</a></li>
<li><a title="flowcon.transforms.coupling.CouplingTransform" href="#flowcon.transforms.coupling.CouplingTransform">CouplingTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.coupling.AdditiveCouplingTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.coupling.AdditiveCouplingTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.coupling.AdditiveCouplingTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.coupling.AffineCouplingTransform" href="#flowcon.transforms.coupling.AffineCouplingTransform">AffineCouplingTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.coupling.AffineCouplingTransform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.coupling.AffineCouplingTransform"><code class="flex name class">
<span>class <span class="ident">AffineCouplingTransform</span></span>
<span>(</span><span>mask, transform_net_create_fn, unconditional_transform=None, scale_activation=&lt;function AffineCouplingTransform.&lt;lambda&gt;&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>An affine coupling layer that scales and shifts part of the variables.</p>
<p>Reference:</p>
<blockquote>
<p>L. Dinh et al., Density estimation using Real NVP, ICLR 2017.</p>
</blockquote>
<p>The user should supply <code>scale_activation</code>, the final activation function in the neural network producing the scale tensor.
Two options are predefined in the class.
<code>DEFAULT_SCALE_ACTIVATION</code> preserves backwards compatibility but only produces scales &lt;= 1.001.
<code>GENERAL_SCALE_ACTIVATION</code> produces scales &lt;= 3, which is more useful in general applications.</p>
<p>Constructor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>mask</code></strong></dt>
<dd>a 1-dim tensor, tuple or list. It indexes inputs as follows:
* If <code>mask[i] &gt; 0</code>, <code>input[i]</code> will be transformed.
* If <code>mask[i] &lt;= 0</code>, <code>input[i]</code> will be passed unchanged.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AffineCouplingTransform(CouplingTransform):
    &#34;&#34;&#34;An affine coupling layer that scales and shifts part of the variables.

    Reference:
    &gt; L. Dinh et al., Density estimation using Real NVP, ICLR 2017.

    The user should supply `scale_activation`, the final activation function in the neural network producing the scale tensor.
    Two options are predefined in the class.
    `DEFAULT_SCALE_ACTIVATION` preserves backwards compatibility but only produces scales &lt;= 1.001.
    `GENERAL_SCALE_ACTIVATION` produces scales &lt;= 3, which is more useful in general applications.
    &#34;&#34;&#34;

    DEFAULT_SCALE_ACTIVATION = lambda x : torch.sigmoid(x + 2) + 1e-3
    GENERAL_SCALE_ACTIVATION = lambda x : (softplus(x) + 1e-3).clamp(0, 3)

    def __init__(self, mask, transform_net_create_fn, unconditional_transform=None, scale_activation=DEFAULT_SCALE_ACTIVATION):
        self.scale_activation = scale_activation
        super().__init__(mask, transform_net_create_fn, unconditional_transform)

    def _transform_dim_multiplier(self):
        return 2

    def _scale_and_shift(self, transform_params):
        unconstrained_scale = transform_params[:, self.num_transform_features:, ...]
        shift = transform_params[:, : self.num_transform_features, ...]
        scale = self.scale_activation(unconstrained_scale)
        return scale, shift

    def _coupling_transform_forward(self, inputs, transform_params):
        scale, shift = self._scale_and_shift(transform_params)
        log_scale = torch.log(scale)
        outputs = inputs * scale + shift
        logabsdet = torchutils.sum_except_batch(log_scale, num_batch_dims=1)
        return outputs, logabsdet

    def _coupling_transform_inverse(self, inputs, transform_params):
        scale, shift = self._scale_and_shift(transform_params)
        log_scale = torch.log(scale)
        outputs = (inputs - shift) / scale
        logabsdet = -torchutils.sum_except_batch(log_scale, num_batch_dims=1)
        return outputs, logabsdet</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.coupling.CouplingTransform" href="#flowcon.transforms.coupling.CouplingTransform">CouplingTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.coupling.AdditiveCouplingTransform" href="#flowcon.transforms.coupling.AdditiveCouplingTransform">AdditiveCouplingTransform</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.coupling.AffineCouplingTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.coupling.AffineCouplingTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.coupling.AffineCouplingTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.coupling.AffineCouplingTransform.DEFAULT_SCALE_ACTIVATION"><code class="name flex">
<span>def <span class="ident">DEFAULT_SCALE_ACTIVATION</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">DEFAULT_SCALE_ACTIVATION = lambda x : torch.sigmoid(x + 2) + 1e-3</code></pre>
</details>
</dd>
<dt id="flowcon.transforms.coupling.AffineCouplingTransform.GENERAL_SCALE_ACTIVATION"><code class="name flex">
<span>def <span class="ident">GENERAL_SCALE_ACTIVATION</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">GENERAL_SCALE_ACTIVATION = lambda x : (softplus(x) + 1e-3).clamp(0, 3)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.coupling.CouplingTransform" href="#flowcon.transforms.coupling.CouplingTransform">CouplingTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.coupling.CouplingTransform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.coupling.CouplingTransform"><code class="flex name class">
<span>class <span class="ident">CouplingTransform</span></span>
<span>(</span><span>mask, transform_net_create_fn, unconditional_transform=None)</span>
</code></dt>
<dd>
<div class="desc"><p>A base class for coupling layers. Supports 2D inputs (NxD), as well as 4D inputs for
images (NxCxHxW). For images the splitting is done on the channel dimension, using the
provided 1D mask.</p>
<p>Constructor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>mask</code></strong></dt>
<dd>a 1-dim tensor, tuple or list. It indexes inputs as follows:
* If <code>mask[i] &gt; 0</code>, <code>input[i]</code> will be transformed.
* If <code>mask[i] &lt;= 0</code>, <code>input[i]</code> will be passed unchanged.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CouplingTransform(Transform):
    &#34;&#34;&#34;A base class for coupling layers. Supports 2D inputs (NxD), as well as 4D inputs for
    images (NxCxHxW). For images the splitting is done on the channel dimension, using the
    provided 1D mask.&#34;&#34;&#34;

    def __init__(self, mask, transform_net_create_fn, unconditional_transform=None):
        &#34;&#34;&#34;
        Constructor.

        Args:
            mask: a 1-dim tensor, tuple or list. It indexes inputs as follows:
                * If `mask[i] &gt; 0`, `input[i]` will be transformed.
                * If `mask[i] &lt;= 0`, `input[i]` will be passed unchanged.
        &#34;&#34;&#34;
        mask = torch.as_tensor(mask)
        if mask.dim() != 1:
            raise ValueError(&#34;Mask must be a 1-dim tensor.&#34;)
        if mask.numel() &lt;= 0:
            raise ValueError(&#34;Mask can&#39;t be empty.&#34;)

        super().__init__()
        self.features = len(mask)
        features_vector = torch.arange(self.features)

        self.register_buffer(
            &#34;identity_features&#34;, features_vector.masked_select(mask &lt;= 0)
        )
        self.register_buffer(
            &#34;transform_features&#34;, features_vector.masked_select(mask &gt; 0)
        )

        assert self.num_identity_features + self.num_transform_features == self.features

        self.transform_net = transform_net_create_fn(
            self.num_identity_features,
            self.num_transform_features * self._transform_dim_multiplier(),
        )

        if unconditional_transform is None:
            self.unconditional_transform = None
        else:
            self.unconditional_transform = unconditional_transform(
                features=self.num_identity_features
            )

    @property
    def num_identity_features(self):
        return len(self.identity_features)

    @property
    def num_transform_features(self):
        return len(self.transform_features)

    def forward(self, inputs, context=None):
        if inputs.dim() not in [2, 4]:
            raise ValueError(&#34;Inputs must be a 2D or a 4D tensor.&#34;)

        if inputs.shape[1] != self.features:
            raise ValueError(
                &#34;Expected features = {}, got {}.&#34;.format(self.features, inputs.shape[1])
            )

        identity_split = inputs[:, self.identity_features, ...]
        transform_split = inputs[:, self.transform_features, ...]

        transform_params = self.transform_net(identity_split, context)
        transform_split, logabsdet = self._coupling_transform_forward(
            inputs=transform_split, transform_params=transform_params
        )

        if self.unconditional_transform is not None:
            identity_split, logabsdet_identity = self.unconditional_transform(
                identity_split, context
            )
            logabsdet += logabsdet_identity

        outputs = torch.empty_like(inputs)
        outputs[:, self.identity_features, ...] = identity_split
        outputs[:, self.transform_features, ...] = transform_split

        return outputs, logabsdet

    def inverse(self, inputs, context=None):
        if inputs.dim() not in [2, 4]:
            raise ValueError(&#34;Inputs must be a 2D or a 4D tensor.&#34;)

        if inputs.shape[1] != self.features:
            raise ValueError(
                &#34;Expected features = {}, got {}.&#34;.format(self.features, inputs.shape[1])
            )

        identity_split = inputs[:, self.identity_features, ...]
        transform_split = inputs[:, self.transform_features, ...]

        logabsdet = 0.0
        if self.unconditional_transform is not None:
            identity_split, logabsdet = self.unconditional_transform.inverse(
                identity_split, context
            )

        transform_params = self.transform_net(identity_split, context)
        transform_split, logabsdet_split = self._coupling_transform_inverse(
            inputs=transform_split, transform_params=transform_params
        )
        logabsdet += logabsdet_split

        outputs = torch.empty_like(inputs)
        outputs[:, self.identity_features] = identity_split
        outputs[:, self.transform_features] = transform_split

        return outputs, logabsdet

    def _transform_dim_multiplier(self):
        &#34;&#34;&#34;Number of features to output for each transform dimension.&#34;&#34;&#34;
        raise NotImplementedError()

    def _coupling_transform_forward(self, inputs, transform_params):
        &#34;&#34;&#34;Forward pass of the coupling transform.&#34;&#34;&#34;
        raise NotImplementedError()

    def _coupling_transform_inverse(self, inputs, transform_params):
        &#34;&#34;&#34;Inverse of the coupling transform.&#34;&#34;&#34;
        raise NotImplementedError()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.coupling.AffineCouplingTransform" href="#flowcon.transforms.coupling.AffineCouplingTransform">AffineCouplingTransform</a></li>
<li><a title="flowcon.transforms.coupling.PiecewiseCouplingTransform" href="#flowcon.transforms.coupling.PiecewiseCouplingTransform">PiecewiseCouplingTransform</a></li>
<li><a title="flowcon.transforms.coupling.UMNNCouplingTransform" href="#flowcon.transforms.coupling.UMNNCouplingTransform">UMNNCouplingTransform</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.coupling.CouplingTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.coupling.CouplingTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.coupling.CouplingTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="flowcon.transforms.coupling.CouplingTransform.num_identity_features"><code class="name">var <span class="ident">num_identity_features</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def num_identity_features(self):
    return len(self.identity_features)</code></pre>
</details>
</dd>
<dt id="flowcon.transforms.coupling.CouplingTransform.num_transform_features"><code class="name">var <span class="ident">num_transform_features</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def num_transform_features(self):
    return len(self.transform_features)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.coupling.CouplingTransform.inverse"><code class="name flex">
<span>def <span class="ident">inverse</span></span>(<span>self, inputs, context=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inverse(self, inputs, context=None):
    if inputs.dim() not in [2, 4]:
        raise ValueError(&#34;Inputs must be a 2D or a 4D tensor.&#34;)

    if inputs.shape[1] != self.features:
        raise ValueError(
            &#34;Expected features = {}, got {}.&#34;.format(self.features, inputs.shape[1])
        )

    identity_split = inputs[:, self.identity_features, ...]
    transform_split = inputs[:, self.transform_features, ...]

    logabsdet = 0.0
    if self.unconditional_transform is not None:
        identity_split, logabsdet = self.unconditional_transform.inverse(
            identity_split, context
        )

    transform_params = self.transform_net(identity_split, context)
    transform_split, logabsdet_split = self._coupling_transform_inverse(
        inputs=transform_split, transform_params=transform_params
    )
    logabsdet += logabsdet_split

    outputs = torch.empty_like(inputs)
    outputs[:, self.identity_features] = identity_split
    outputs[:, self.transform_features] = transform_split

    return outputs, logabsdet</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.base.Transform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.coupling.PiecewiseCouplingTransform"><code class="flex name class">
<span>class <span class="ident">PiecewiseCouplingTransform</span></span>
<span>(</span><span>mask, transform_net_create_fn, unconditional_transform=None)</span>
</code></dt>
<dd>
<div class="desc"><p>A base class for coupling layers. Supports 2D inputs (NxD), as well as 4D inputs for
images (NxCxHxW). For images the splitting is done on the channel dimension, using the
provided 1D mask.</p>
<p>Constructor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>mask</code></strong></dt>
<dd>a 1-dim tensor, tuple or list. It indexes inputs as follows:
* If <code>mask[i] &gt; 0</code>, <code>input[i]</code> will be transformed.
* If <code>mask[i] &lt;= 0</code>, <code>input[i]</code> will be passed unchanged.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PiecewiseCouplingTransform(CouplingTransform):
    def _coupling_transform_forward(self, inputs, transform_params):
        return self._coupling_transform(inputs, transform_params, inverse=False)

    def _coupling_transform_inverse(self, inputs, transform_params):
        return self._coupling_transform(inputs, transform_params, inverse=True)

    def _coupling_transform(self, inputs, transform_params, inverse=False):
        if inputs.dim() == 4:
            b, c, h, w = inputs.shape
            # For images, reshape transform_params from Bx(C*?)xHxW to BxCxHxWx?
            transform_params = transform_params.reshape(b, c, -1, h, w).permute(
                0, 1, 3, 4, 2
            )
        elif inputs.dim() == 2:
            b, d = inputs.shape
            # For 2D data, reshape transform_params from Bx(D*?) to BxDx?
            transform_params = transform_params.reshape(b, d, -1)

        outputs, logabsdet = self._piecewise_cdf(inputs, transform_params, inverse)

        return outputs, torchutils.sum_except_batch(logabsdet)

    def _piecewise_cdf(self, inputs, transform_params, inverse=False):
        raise NotImplementedError()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.coupling.CouplingTransform" href="#flowcon.transforms.coupling.CouplingTransform">CouplingTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.coupling.PiecewiseCubicCouplingTransform" href="#flowcon.transforms.coupling.PiecewiseCubicCouplingTransform">PiecewiseCubicCouplingTransform</a></li>
<li><a title="flowcon.transforms.coupling.PiecewiseLinearCouplingTransform" href="#flowcon.transforms.coupling.PiecewiseLinearCouplingTransform">PiecewiseLinearCouplingTransform</a></li>
<li><a title="flowcon.transforms.coupling.PiecewiseQuadraticCouplingTransform" href="#flowcon.transforms.coupling.PiecewiseQuadraticCouplingTransform">PiecewiseQuadraticCouplingTransform</a></li>
<li><a title="flowcon.transforms.coupling.PiecewiseRationalQuadraticCouplingTransform" href="#flowcon.transforms.coupling.PiecewiseRationalQuadraticCouplingTransform">PiecewiseRationalQuadraticCouplingTransform</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.coupling.PiecewiseCouplingTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.coupling.PiecewiseCouplingTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.coupling.PiecewiseCouplingTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.coupling.CouplingTransform" href="#flowcon.transforms.coupling.CouplingTransform">CouplingTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.coupling.CouplingTransform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.coupling.PiecewiseCubicCouplingTransform"><code class="flex name class">
<span>class <span class="ident">PiecewiseCubicCouplingTransform</span></span>
<span>(</span><span>mask, transform_net_create_fn, num_bins=10, tails=None, tail_bound=1.0, apply_unconditional_transform=False, img_shape=None, min_bin_width=0.001, min_bin_height=0.001)</span>
</code></dt>
<dd>
<div class="desc"><p>A base class for coupling layers. Supports 2D inputs (NxD), as well as 4D inputs for
images (NxCxHxW). For images the splitting is done on the channel dimension, using the
provided 1D mask.</p>
<p>Constructor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>mask</code></strong></dt>
<dd>a 1-dim tensor, tuple or list. It indexes inputs as follows:
* If <code>mask[i] &gt; 0</code>, <code>input[i]</code> will be transformed.
* If <code>mask[i] &lt;= 0</code>, <code>input[i]</code> will be passed unchanged.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PiecewiseCubicCouplingTransform(PiecewiseCouplingTransform):
    def __init__(
        self,
        mask,
        transform_net_create_fn,
        num_bins=10,
        tails=None,
        tail_bound=1.0,
        apply_unconditional_transform=False,
        img_shape=None,
        min_bin_width=splines.cubic.DEFAULT_MIN_BIN_WIDTH,
        min_bin_height=splines.cubic.DEFAULT_MIN_BIN_HEIGHT,
    ):

        self.num_bins = num_bins
        self.min_bin_width = min_bin_width
        self.min_bin_height = min_bin_height
        self.tails = tails
        self.tail_bound = tail_bound

        if apply_unconditional_transform:
            unconditional_transform = lambda features: PiecewiseCubicCDF(
                shape=[features] + (img_shape if img_shape else []),
                num_bins=num_bins,
                tails=tails,
                tail_bound=tail_bound,
                min_bin_width=min_bin_width,
                min_bin_height=min_bin_height,
            )
        else:
            unconditional_transform = None

        super().__init__(
            mask,
            transform_net_create_fn,
            unconditional_transform=unconditional_transform,
        )

    def _transform_dim_multiplier(self):
        return self.num_bins * 2 + 2

    def _piecewise_cdf(self, inputs, transform_params, inverse=False):
        unnormalized_widths = transform_params[..., : self.num_bins]
        unnormalized_heights = transform_params[..., self.num_bins : 2 * self.num_bins]
        unnorm_derivatives_left = transform_params[..., 2 * self.num_bins][..., None]
        unnorm_derivatives_right = transform_params[..., 2 * self.num_bins + 1][
            ..., None
        ]

        if hasattr(self.transform_net, &#34;hidden_features&#34;):
            unnormalized_widths /= np.sqrt(self.transform_net.hidden_features)
            unnormalized_heights /= np.sqrt(self.transform_net.hidden_features)

        if self.tails is None:
            spline_fn = splines.cubic_spline
            spline_kwargs = {}
        else:
            spline_fn = splines.unconstrained_cubic_spline
            spline_kwargs = {&#34;tails&#34;: self.tails, &#34;tail_bound&#34;: self.tail_bound}

        return spline_fn(
            inputs=inputs,
            unnormalized_widths=unnormalized_widths,
            unnormalized_heights=unnormalized_heights,
            unnorm_derivatives_left=unnorm_derivatives_left,
            unnorm_derivatives_right=unnorm_derivatives_right,
            inverse=inverse,
            min_bin_width=self.min_bin_width,
            min_bin_height=self.min_bin_height,
            **spline_kwargs
        )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.coupling.PiecewiseCouplingTransform" href="#flowcon.transforms.coupling.PiecewiseCouplingTransform">PiecewiseCouplingTransform</a></li>
<li><a title="flowcon.transforms.coupling.CouplingTransform" href="#flowcon.transforms.coupling.CouplingTransform">CouplingTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.coupling.PiecewiseCubicCouplingTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.coupling.PiecewiseCubicCouplingTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.coupling.PiecewiseCubicCouplingTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.coupling.PiecewiseCouplingTransform" href="#flowcon.transforms.coupling.PiecewiseCouplingTransform">PiecewiseCouplingTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.coupling.PiecewiseCouplingTransform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.coupling.PiecewiseLinearCouplingTransform"><code class="flex name class">
<span>class <span class="ident">PiecewiseLinearCouplingTransform</span></span>
<span>(</span><span>mask, transform_net_create_fn, num_bins=10, tails=None, tail_bound=1.0, apply_unconditional_transform=False, img_shape=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Reference:</p>
<blockquote>
<p>Müller et al., Neural Importance Sampling, arXiv:1808.03856, 2018.</p>
</blockquote>
<p>Constructor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>mask</code></strong></dt>
<dd>a 1-dim tensor, tuple or list. It indexes inputs as follows:
* If <code>mask[i] &gt; 0</code>, <code>input[i]</code> will be transformed.
* If <code>mask[i] &lt;= 0</code>, <code>input[i]</code> will be passed unchanged.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PiecewiseLinearCouplingTransform(PiecewiseCouplingTransform):
    &#34;&#34;&#34;
    Reference:
    &gt; Müller et al., Neural Importance Sampling, arXiv:1808.03856, 2018.
    &#34;&#34;&#34;

    def __init__(
        self,
        mask,
        transform_net_create_fn,
        num_bins=10,
        tails=None,
        tail_bound=1.0,
        apply_unconditional_transform=False,
        img_shape=None,
    ):
        self.num_bins = num_bins
        self.tails = tails
        self.tail_bound = tail_bound

        if apply_unconditional_transform:
            unconditional_transform = lambda features: PiecewiseLinearCDF(
                shape=[features] + (img_shape if img_shape else []),
                num_bins=num_bins,
                tails=tails,
                tail_bound=tail_bound,
            )
        else:
            unconditional_transform = None

        super().__init__(
            mask,
            transform_net_create_fn,
            unconditional_transform=unconditional_transform,
        )

    def _transform_dim_multiplier(self):
        return self.num_bins

    def _piecewise_cdf(self, inputs, transform_params, inverse=False):
        unnormalized_pdf = transform_params

        if self.tails is None:
            return splines.linear_spline(
                inputs=inputs, unnormalized_pdf=unnormalized_pdf, inverse=inverse
            )
        else:
            return splines.unconstrained_linear_spline(
                inputs=inputs,
                unnormalized_pdf=unnormalized_pdf,
                inverse=inverse,
                tails=self.tails,
                tail_bound=self.tail_bound,
            )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.coupling.PiecewiseCouplingTransform" href="#flowcon.transforms.coupling.PiecewiseCouplingTransform">PiecewiseCouplingTransform</a></li>
<li><a title="flowcon.transforms.coupling.CouplingTransform" href="#flowcon.transforms.coupling.CouplingTransform">CouplingTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.coupling.PiecewiseLinearCouplingTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.coupling.PiecewiseLinearCouplingTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.coupling.PiecewiseLinearCouplingTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.coupling.PiecewiseCouplingTransform" href="#flowcon.transforms.coupling.PiecewiseCouplingTransform">PiecewiseCouplingTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.coupling.PiecewiseCouplingTransform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.coupling.PiecewiseQuadraticCouplingTransform"><code class="flex name class">
<span>class <span class="ident">PiecewiseQuadraticCouplingTransform</span></span>
<span>(</span><span>mask, transform_net_create_fn, num_bins=10, tails=None, tail_bound=1.0, apply_unconditional_transform=False, img_shape=None, min_bin_width=0.001, min_bin_height=0.001)</span>
</code></dt>
<dd>
<div class="desc"><p>Reference:</p>
<blockquote>
<p>Müller et al., Neural Importance Sampling, arXiv:1808.03856, 2018.</p>
</blockquote>
<p>Constructor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>mask</code></strong></dt>
<dd>a 1-dim tensor, tuple or list. It indexes inputs as follows:
* If <code>mask[i] &gt; 0</code>, <code>input[i]</code> will be transformed.
* If <code>mask[i] &lt;= 0</code>, <code>input[i]</code> will be passed unchanged.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PiecewiseQuadraticCouplingTransform(PiecewiseCouplingTransform):
    &#34;&#34;&#34;
    Reference:
    &gt; Müller et al., Neural Importance Sampling, arXiv:1808.03856, 2018.
    &#34;&#34;&#34;

    def __init__(
        self,
        mask,
        transform_net_create_fn,
        num_bins=10,
        tails=None,
        tail_bound=1.0,
        apply_unconditional_transform=False,
        img_shape=None,
        min_bin_width=splines.quadratic.DEFAULT_MIN_BIN_WIDTH,
        min_bin_height=splines.quadratic.DEFAULT_MIN_BIN_HEIGHT,
    ):
        self.num_bins = num_bins
        self.tails = tails
        self.tail_bound = tail_bound
        self.min_bin_width = min_bin_width
        self.min_bin_height = min_bin_height

        if apply_unconditional_transform:
            unconditional_transform = lambda features: PiecewiseQuadraticCDF(
                shape=[features] + (img_shape if img_shape else []),
                num_bins=num_bins,
                tails=tails,
                tail_bound=tail_bound,
                min_bin_width=min_bin_width,
                min_bin_height=min_bin_height,
            )
        else:
            unconditional_transform = None

        super().__init__(
            mask,
            transform_net_create_fn,
            unconditional_transform=unconditional_transform,
        )

    def _transform_dim_multiplier(self):
        if self.tails == &#34;linear&#34;:
            return self.num_bins * 2 - 1
        else:
            return self.num_bins * 2 + 1

    def _piecewise_cdf(self, inputs, transform_params, inverse=False):
        unnormalized_widths = transform_params[..., : self.num_bins]
        unnormalized_heights = transform_params[..., self.num_bins :]

        if hasattr(self.transform_net, &#34;hidden_features&#34;):
            unnormalized_widths /= np.sqrt(self.transform_net.hidden_features)
            unnormalized_heights /= np.sqrt(self.transform_net.hidden_features)

        if self.tails is None:
            spline_fn = splines.quadratic_spline
            spline_kwargs = {}
        else:
            spline_fn = splines.unconstrained_quadratic_spline
            spline_kwargs = {&#34;tails&#34;: self.tails, &#34;tail_bound&#34;: self.tail_bound}

        return spline_fn(
            inputs=inputs,
            unnormalized_widths=unnormalized_widths,
            unnormalized_heights=unnormalized_heights,
            inverse=inverse,
            min_bin_width=self.min_bin_width,
            min_bin_height=self.min_bin_height,
            **spline_kwargs
        )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.coupling.PiecewiseCouplingTransform" href="#flowcon.transforms.coupling.PiecewiseCouplingTransform">PiecewiseCouplingTransform</a></li>
<li><a title="flowcon.transforms.coupling.CouplingTransform" href="#flowcon.transforms.coupling.CouplingTransform">CouplingTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.coupling.PiecewiseQuadraticCouplingTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.coupling.PiecewiseQuadraticCouplingTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.coupling.PiecewiseQuadraticCouplingTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.coupling.PiecewiseCouplingTransform" href="#flowcon.transforms.coupling.PiecewiseCouplingTransform">PiecewiseCouplingTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.coupling.PiecewiseCouplingTransform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.coupling.PiecewiseRationalQuadraticCouplingTransform"><code class="flex name class">
<span>class <span class="ident">PiecewiseRationalQuadraticCouplingTransform</span></span>
<span>(</span><span>mask, transform_net_create_fn, num_bins=10, tails=None, tail_bound=1.0, apply_unconditional_transform=False, img_shape=None, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001)</span>
</code></dt>
<dd>
<div class="desc"><p>A base class for coupling layers. Supports 2D inputs (NxD), as well as 4D inputs for
images (NxCxHxW). For images the splitting is done on the channel dimension, using the
provided 1D mask.</p>
<p>Constructor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>mask</code></strong></dt>
<dd>a 1-dim tensor, tuple or list. It indexes inputs as follows:
* If <code>mask[i] &gt; 0</code>, <code>input[i]</code> will be transformed.
* If <code>mask[i] &lt;= 0</code>, <code>input[i]</code> will be passed unchanged.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PiecewiseRationalQuadraticCouplingTransform(PiecewiseCouplingTransform):
    def __init__(
        self,
        mask,
        transform_net_create_fn,
        num_bins=10,
        tails=None,
        tail_bound=1.0,
        apply_unconditional_transform=False,
        img_shape=None,
        min_bin_width=splines.rational_quadratic.DEFAULT_MIN_BIN_WIDTH,
        min_bin_height=splines.rational_quadratic.DEFAULT_MIN_BIN_HEIGHT,
        min_derivative=splines.rational_quadratic.DEFAULT_MIN_DERIVATIVE,
    ):

        self.num_bins = num_bins
        self.min_bin_width = min_bin_width
        self.min_bin_height = min_bin_height
        self.min_derivative = min_derivative
        self.tails = tails
        self.tail_bound = tail_bound

        if apply_unconditional_transform:
            unconditional_transform = lambda features: PiecewiseRationalQuadraticCDF(
                shape=[features] + (img_shape if img_shape else []),
                num_bins=num_bins,
                tails=tails,
                tail_bound=tail_bound,
                min_bin_width=min_bin_width,
                min_bin_height=min_bin_height,
                min_derivative=min_derivative,
            )
        else:
            unconditional_transform = None

        super().__init__(
            mask,
            transform_net_create_fn,
            unconditional_transform=unconditional_transform,
        )

    def _transform_dim_multiplier(self):
        if self.tails == &#34;linear&#34;:
            return self.num_bins * 3 - 1
        else:
            return self.num_bins * 3 + 1

    def _piecewise_cdf(self, inputs, transform_params, inverse=False):
        unnormalized_widths = transform_params[..., : self.num_bins]
        unnormalized_heights = transform_params[..., self.num_bins : 2 * self.num_bins]
        unnormalized_derivatives = transform_params[..., 2 * self.num_bins :]

        if hasattr(self.transform_net, &#34;hidden_features&#34;):
            unnormalized_widths /= np.sqrt(self.transform_net.hidden_features)
            unnormalized_heights /= np.sqrt(self.transform_net.hidden_features)
        elif hasattr(self.transform_net, &#34;hidden_channels&#34;):
            unnormalized_widths /= np.sqrt(self.transform_net.hidden_channels)
            unnormalized_heights /= np.sqrt(self.transform_net.hidden_channels)
        else:
            warnings.warn(
                &#34;Inputs to the softmax are not scaled down: initialization might be bad.&#34;
            )

        if self.tails is None:
            spline_fn = splines.rational_quadratic_spline
            spline_kwargs = {}
        else:
            spline_fn = splines.unconstrained_rational_quadratic_spline
            spline_kwargs = {&#34;tails&#34;: self.tails, &#34;tail_bound&#34;: self.tail_bound}

        return spline_fn(
            inputs=inputs,
            unnormalized_widths=unnormalized_widths,
            unnormalized_heights=unnormalized_heights,
            unnormalized_derivatives=unnormalized_derivatives,
            inverse=inverse,
            min_bin_width=self.min_bin_width,
            min_bin_height=self.min_bin_height,
            min_derivative=self.min_derivative,
            **spline_kwargs
        )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.coupling.PiecewiseCouplingTransform" href="#flowcon.transforms.coupling.PiecewiseCouplingTransform">PiecewiseCouplingTransform</a></li>
<li><a title="flowcon.transforms.coupling.CouplingTransform" href="#flowcon.transforms.coupling.CouplingTransform">CouplingTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.coupling.PiecewiseRationalQuadraticCouplingTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.coupling.PiecewiseRationalQuadraticCouplingTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.coupling.PiecewiseRationalQuadraticCouplingTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.coupling.PiecewiseCouplingTransform" href="#flowcon.transforms.coupling.PiecewiseCouplingTransform">PiecewiseCouplingTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.coupling.PiecewiseCouplingTransform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.coupling.UMNNCouplingTransform"><code class="flex name class">
<span>class <span class="ident">UMNNCouplingTransform</span></span>
<span>(</span><span>mask, transform_net_create_fn, integrand_net_layers=[50, 50, 50], cond_size=20, nb_steps=20, solver='CCParallel', apply_unconditional_transform=False)</span>
</code></dt>
<dd>
<div class="desc"><p>An unconstrained monotonic neural networks coupling layer that transforms the variables.</p>
<p>Reference:</p>
<blockquote>
<p>A. Wehenkel and G. Louppe, Unconstrained Monotonic Neural Networks, NeurIPS2019.</p>
</blockquote>
<p>---- Specific arguments ----
integrand_net_layers: the layers dimension to put in the integrand network.
cond_size: The embedding size for the conditioning factors.
nb_steps: The number of integration steps.
solver: The quadrature algorithm - CC or CCParallel. Both implements Clenshaw-Curtis quadrature with
Leibniz rule for backward computation. CCParallel pass all the evaluation points (nb_steps) at once, it is faster
but requires more memory.</p>
<p>Constructor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>mask</code></strong></dt>
<dd>a 1-dim tensor, tuple or list. It indexes inputs as follows:
* If <code>mask[i] &gt; 0</code>, <code>input[i]</code> will be transformed.
* If <code>mask[i] &lt;= 0</code>, <code>input[i]</code> will be passed unchanged.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class UMNNCouplingTransform(CouplingTransform):
    &#34;&#34;&#34;An unconstrained monotonic neural networks coupling layer that transforms the variables.

    Reference:
    &gt; A. Wehenkel and G. Louppe, Unconstrained Monotonic Neural Networks, NeurIPS2019.

    ---- Specific arguments ----
        integrand_net_layers: the layers dimension to put in the integrand network.
        cond_size: The embedding size for the conditioning factors.
        nb_steps: The number of integration steps.
        solver: The quadrature algorithm - CC or CCParallel. Both implements Clenshaw-Curtis quadrature with
        Leibniz rule for backward computation. CCParallel pass all the evaluation points (nb_steps) at once, it is faster
        but requires more memory.

    &#34;&#34;&#34;
    def __init__(
        self,
        mask,
        transform_net_create_fn,
        integrand_net_layers=[50, 50, 50],
        cond_size=20,
        nb_steps=20,
        solver=&#34;CCParallel&#34;,
        apply_unconditional_transform=False
    ):

        if apply_unconditional_transform:
            unconditional_transform = lambda features: MonotonicNormalizer(integrand_net_layers, 0, nb_steps, solver)
        else:
            unconditional_transform = None
        self.cond_size = cond_size
        super().__init__(
            mask,
            transform_net_create_fn,
            unconditional_transform=unconditional_transform,
        )

        self.transformer = MonotonicNormalizer(integrand_net_layers, cond_size, nb_steps, solver)

    def _transform_dim_multiplier(self):
        return self.cond_size

    def _coupling_transform_forward(self, inputs, transform_params):
        if len(inputs.shape) == 2:
            z, jac = self.transformer(inputs, transform_params.reshape(inputs.shape[0], inputs.shape[1], -1))
            log_det_jac = jac.log().sum(1)
            return z, log_det_jac
        else:
            B, C, H, W = inputs.shape
            z, jac = self.transformer(inputs.permute(0, 2, 3, 1).reshape(-1, inputs.shape[1]), transform_params.permute(0, 2, 3, 1).reshape(-1, 1, transform_params.shape[1]))
            log_det_jac = jac.log().reshape(B, -1).sum(1)
            return z.reshape(B, H, W, C).permute(0, 3, 1, 2), log_det_jac

    def _coupling_transform_inverse(self, inputs, transform_params):
        if len(inputs.shape) == 2:
            x = self.transformer.inverse_transform(inputs, transform_params.reshape(inputs.shape[0], inputs.shape[1], -1))
            z, jac = self.transformer(x, transform_params.reshape(inputs.shape[0], inputs.shape[1], -1))
            log_det_jac = -jac.log().sum(1)
            return x, log_det_jac
        else:
            B, C, H, W = inputs.shape
            x = self.transformer.inverse_transform(inputs.permute(0, 2, 3, 1).reshape(-1, inputs.shape[1]), transform_params.permute(0, 2, 3, 1).reshape(-1, 1, transform_params.shape[1]))
            z, jac = self.transformer(x, transform_params.permute(0, 2, 3, 1).reshape(-1, 1, transform_params.shape[1]))
            log_det_jac = -jac.log().reshape(B, -1).sum(1)
            return x.reshape(B, H, W, C).permute(0, 3, 1, 2), log_det_jac</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.coupling.CouplingTransform" href="#flowcon.transforms.coupling.CouplingTransform">CouplingTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.coupling.UMNNCouplingTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.coupling.UMNNCouplingTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.coupling.UMNNCouplingTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.coupling.CouplingTransform" href="#flowcon.transforms.coupling.CouplingTransform">CouplingTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.coupling.CouplingTransform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="flowcon.transforms" href="index.html">flowcon.transforms</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="flowcon.transforms.coupling.AdditiveCouplingTransform" href="#flowcon.transforms.coupling.AdditiveCouplingTransform">AdditiveCouplingTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.coupling.AdditiveCouplingTransform.call_super_init" href="#flowcon.transforms.coupling.AdditiveCouplingTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.coupling.AdditiveCouplingTransform.dump_patches" href="#flowcon.transforms.coupling.AdditiveCouplingTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.coupling.AdditiveCouplingTransform.training" href="#flowcon.transforms.coupling.AdditiveCouplingTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.coupling.AffineCouplingTransform" href="#flowcon.transforms.coupling.AffineCouplingTransform">AffineCouplingTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.coupling.AffineCouplingTransform.DEFAULT_SCALE_ACTIVATION" href="#flowcon.transforms.coupling.AffineCouplingTransform.DEFAULT_SCALE_ACTIVATION">DEFAULT_SCALE_ACTIVATION</a></code></li>
<li><code><a title="flowcon.transforms.coupling.AffineCouplingTransform.GENERAL_SCALE_ACTIVATION" href="#flowcon.transforms.coupling.AffineCouplingTransform.GENERAL_SCALE_ACTIVATION">GENERAL_SCALE_ACTIVATION</a></code></li>
<li><code><a title="flowcon.transforms.coupling.AffineCouplingTransform.call_super_init" href="#flowcon.transforms.coupling.AffineCouplingTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.coupling.AffineCouplingTransform.dump_patches" href="#flowcon.transforms.coupling.AffineCouplingTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.coupling.AffineCouplingTransform.training" href="#flowcon.transforms.coupling.AffineCouplingTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.coupling.CouplingTransform" href="#flowcon.transforms.coupling.CouplingTransform">CouplingTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.coupling.CouplingTransform.call_super_init" href="#flowcon.transforms.coupling.CouplingTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.coupling.CouplingTransform.dump_patches" href="#flowcon.transforms.coupling.CouplingTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.coupling.CouplingTransform.inverse" href="#flowcon.transforms.coupling.CouplingTransform.inverse">inverse</a></code></li>
<li><code><a title="flowcon.transforms.coupling.CouplingTransform.num_identity_features" href="#flowcon.transforms.coupling.CouplingTransform.num_identity_features">num_identity_features</a></code></li>
<li><code><a title="flowcon.transforms.coupling.CouplingTransform.num_transform_features" href="#flowcon.transforms.coupling.CouplingTransform.num_transform_features">num_transform_features</a></code></li>
<li><code><a title="flowcon.transforms.coupling.CouplingTransform.training" href="#flowcon.transforms.coupling.CouplingTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.coupling.PiecewiseCouplingTransform" href="#flowcon.transforms.coupling.PiecewiseCouplingTransform">PiecewiseCouplingTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.coupling.PiecewiseCouplingTransform.call_super_init" href="#flowcon.transforms.coupling.PiecewiseCouplingTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.coupling.PiecewiseCouplingTransform.dump_patches" href="#flowcon.transforms.coupling.PiecewiseCouplingTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.coupling.PiecewiseCouplingTransform.training" href="#flowcon.transforms.coupling.PiecewiseCouplingTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.coupling.PiecewiseCubicCouplingTransform" href="#flowcon.transforms.coupling.PiecewiseCubicCouplingTransform">PiecewiseCubicCouplingTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.coupling.PiecewiseCubicCouplingTransform.call_super_init" href="#flowcon.transforms.coupling.PiecewiseCubicCouplingTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.coupling.PiecewiseCubicCouplingTransform.dump_patches" href="#flowcon.transforms.coupling.PiecewiseCubicCouplingTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.coupling.PiecewiseCubicCouplingTransform.training" href="#flowcon.transforms.coupling.PiecewiseCubicCouplingTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.coupling.PiecewiseLinearCouplingTransform" href="#flowcon.transforms.coupling.PiecewiseLinearCouplingTransform">PiecewiseLinearCouplingTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.coupling.PiecewiseLinearCouplingTransform.call_super_init" href="#flowcon.transforms.coupling.PiecewiseLinearCouplingTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.coupling.PiecewiseLinearCouplingTransform.dump_patches" href="#flowcon.transforms.coupling.PiecewiseLinearCouplingTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.coupling.PiecewiseLinearCouplingTransform.training" href="#flowcon.transforms.coupling.PiecewiseLinearCouplingTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.coupling.PiecewiseQuadraticCouplingTransform" href="#flowcon.transforms.coupling.PiecewiseQuadraticCouplingTransform">PiecewiseQuadraticCouplingTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.coupling.PiecewiseQuadraticCouplingTransform.call_super_init" href="#flowcon.transforms.coupling.PiecewiseQuadraticCouplingTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.coupling.PiecewiseQuadraticCouplingTransform.dump_patches" href="#flowcon.transforms.coupling.PiecewiseQuadraticCouplingTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.coupling.PiecewiseQuadraticCouplingTransform.training" href="#flowcon.transforms.coupling.PiecewiseQuadraticCouplingTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.coupling.PiecewiseRationalQuadraticCouplingTransform" href="#flowcon.transforms.coupling.PiecewiseRationalQuadraticCouplingTransform">PiecewiseRationalQuadraticCouplingTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.coupling.PiecewiseRationalQuadraticCouplingTransform.call_super_init" href="#flowcon.transforms.coupling.PiecewiseRationalQuadraticCouplingTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.coupling.PiecewiseRationalQuadraticCouplingTransform.dump_patches" href="#flowcon.transforms.coupling.PiecewiseRationalQuadraticCouplingTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.coupling.PiecewiseRationalQuadraticCouplingTransform.training" href="#flowcon.transforms.coupling.PiecewiseRationalQuadraticCouplingTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.coupling.UMNNCouplingTransform" href="#flowcon.transforms.coupling.UMNNCouplingTransform">UMNNCouplingTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.coupling.UMNNCouplingTransform.call_super_init" href="#flowcon.transforms.coupling.UMNNCouplingTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.coupling.UMNNCouplingTransform.dump_patches" href="#flowcon.transforms.coupling.UMNNCouplingTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.coupling.UMNNCouplingTransform.training" href="#flowcon.transforms.coupling.UMNNCouplingTransform.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>