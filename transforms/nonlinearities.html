<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.0">
<title>flowcon.transforms.nonlinearities API documentation</title>
<meta name="description" content="Implementations of invertible non-linearities.">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>flowcon.transforms.nonlinearities</code></h1>
</header>
<section id="section-intro">
<p>Implementations of invertible non-linearities.</p>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="flowcon.transforms.nonlinearities.CauchyCDF"><code class="flex name class">
<span>class <span class="ident">CauchyCDF</span></span>
<span>(</span><span>location=None, scale=None, features=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all transform objects.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CauchyCDF(Transform):
    def __init__(self, location=None, scale=None, features=None):
        super().__init__()

    def forward(self, inputs, context=None):
        outputs = (1 / np.pi) * torch.atan(inputs) + 0.5
        logabsdet = torchutils.sum_except_batch(
            -np.log(np.pi) - torch.log(1 + inputs ** 2)
        )
        return outputs, logabsdet

    def inverse(self, inputs, context=None):
        if torch.min(inputs) &lt; 0 or torch.max(inputs) &gt; 1:
            raise InputOutsideDomain()

        outputs = torch.tan(np.pi * (inputs - 0.5))
        logabsdet = -torchutils.sum_except_batch(
            -np.log(np.pi) - torch.log(1 + outputs ** 2)
        )
        return outputs, logabsdet</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.CauchyCDF.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.CauchyCDF.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.CauchyCDF.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.CauchyCDF.inverse"><code class="name flex">
<span>def <span class="ident">inverse</span></span>(<span>self, inputs, context=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.base.Transform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.nonlinearities.CauchyCDFInverse"><code class="flex name class">
<span>class <span class="ident">CauchyCDFInverse</span></span>
<span>(</span><span>location=None, scale=None, features=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a transform that is the inverse of a given transform.</p>
<p>Constructor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>transform</code></strong></dt>
<dd>An object of type <code>Transform</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CauchyCDFInverse(InverseTransform):
    def __init__(self, location=None, scale=None, features=None):
        super().__init__(CauchyCDF(location=location, scale=scale, features=features))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.base.InverseTransform" href="base.html#flowcon.transforms.base.InverseTransform">InverseTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.CauchyCDFInverse.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.CauchyCDFInverse.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.CauchyCDFInverse.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.base.InverseTransform" href="base.html#flowcon.transforms.base.InverseTransform">InverseTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.base.InverseTransform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.nonlinearities.CompositeCDFTransform"><code class="flex name class">
<span>class <span class="ident">CompositeCDFTransform</span></span>
<span>(</span><span>squashing_transform, cdf_transform)</span>
</code></dt>
<dd>
<div class="desc"><p>Composes several transforms into one, in the order they are given.</p>
<p>Constructor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>transforms</code></strong></dt>
<dd>an iterable of <code>Transform</code> objects.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CompositeCDFTransform(CompositeTransform):
    def __init__(self, squashing_transform, cdf_transform):
        super().__init__(
            [squashing_transform, cdf_transform, InverseTransform(squashing_transform), ]
        )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.base.CompositeTransform" href="base.html#flowcon.transforms.base.CompositeTransform">CompositeTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.CompositeCDFTransform.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.CompositeCDFTransform.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.CompositeCDFTransform.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.base.CompositeTransform" href="base.html#flowcon.transforms.base.CompositeTransform">CompositeTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.base.CompositeTransform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.nonlinearities.Exp"><code class="flex name class">
<span>class <span class="ident">Exp</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all transform objects.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Exp(Transform):
    def forward(self, inputs, context=None):
        outputs = torch.exp(inputs)
        logabsdet = torchutils.sum_except_batch(inputs, num_batch_dims=1)

        return outputs, logabsdet

    def inverse(self, inputs, context=None):
        if torch.min(inputs) &lt;= 0.:
            raise InputOutsideDomain()

        outputs = torch.log(inputs)
        logabsdet = -torchutils.sum_except_batch(outputs, num_batch_dims=1)

        return outputs, logabsdet</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.Exp.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.Exp.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.Exp.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.Exp.inverse"><code class="name flex">
<span>def <span class="ident">inverse</span></span>(<span>self, inputs, context=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.base.Transform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.nonlinearities.ExtendedSoftplus"><code class="flex name class">
<span>class <span class="ident">ExtendedSoftplus</span></span>
<span>(</span><span>features, shift=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Combination of a (shifted and scaled) softplus and the same softplus flipped around the origin</p>
<p>Softplus(scale * (x-shift)) - Softplus(-scale * (x + shift))</p>
<p>Linear outside of origin, flat around origin.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ExtendedSoftplus(torch.nn.Module):
    &#34;&#34;&#34;
    Combination of a (shifted and scaled) softplus and the same softplus flipped around the origin

    Softplus(scale * (x-shift)) - Softplus(-scale * (x + shift))

    Linear outside of origin, flat around origin.
    &#34;&#34;&#34;

    def __init__(self, features, shift=None):
        self.features = features
        super(ExtendedSoftplus, self).__init__()
        if shift is None:
            self.shift = torch.nn.Parameter(torch.ones(1, features) * 3, requires_grad=True)
            # self.log_scale = torch.nn.Parameter(torch.zeros(1, features), requires_grad=True)
        elif torch.is_tensor(shift):
            self.shift = shift.reshape(-1, features)
            # self.log_scale = log_scale.reshape(-1, features)
        else:
            self.shift = torch.nn.Parameter(torch.tensor(shift), requires_grad=True)
            # self.log_scale = torch.nn.Parameter(torch.tensor(log_scale), requires_grad=True)

        self._softplus = torch.nn.Softplus()

    # def get_shift_and_scale(self):
    #     # return self._softplus(self.shift), torch.exp(self.log_scale)
    #     return self.shift, torch.exp(self.log_scale) + 1e-3
    #     # return 5, torch.exp(self.log_scale)

    def get_shift(self):
        return self._softplus(self.shift) + 1e-1

    def softplus(self, x, shift):
        return self._softplus((x - shift))

    def softminus(self, x, shift):
        return - self._softplus(-(x + shift))

    def diag_jacobian_pos(self, x, shift):
        # (b e^(b x))/(e^(a b) + e^(b x))
        return torch.exp(x) / (torch.exp(shift) + torch.exp(x))

    def log_diag_jacobian_pos(self, x, shift):
        # -log(e^(a b) + e^(b x)) + b x + log(b)
        log_jac = -torch.logaddexp(shift, x) + x
        return log_jac

    def diag_jacobian_neg(self, x, shift):
        return torch.sigmoid(- (shift + x))

    def log_diag_jacobian_neg(self, x, shift):
        return - self._softplus((shift + x))

    def forward(self, inputs):
        # inputs = inputs.requires_grad_()
        shift = self.get_shift()
        outputs = self.softplus(inputs, shift) + self.softminus(inputs, shift)
        # ref_batch_jacobian = torchutils.batch_jacobian(outputs, inputs)
        # ref_logabsdet = torchutils.logabsdet(ref_batch_jacobian)
        # breakpoint()
        diag_jacobian = torch.logaddexp(self.log_diag_jacobian_pos(inputs, shift),
                                        self.log_diag_jacobian_neg(inputs, shift))
        return outputs, diag_jacobian  # torch.log(diag_jacobian).sum(-1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.ExtendedSoftplus.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.ExtendedSoftplus.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.ExtendedSoftplus.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.ExtendedSoftplus.diag_jacobian_neg"><code class="name flex">
<span>def <span class="ident">diag_jacobian_neg</span></span>(<span>self, x, shift)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.ExtendedSoftplus.diag_jacobian_pos"><code class="name flex">
<span>def <span class="ident">diag_jacobian_pos</span></span>(<span>self, x, shift)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.ExtendedSoftplus.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, inputs) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.ExtendedSoftplus.get_shift"><code class="name flex">
<span>def <span class="ident">get_shift</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.ExtendedSoftplus.log_diag_jacobian_neg"><code class="name flex">
<span>def <span class="ident">log_diag_jacobian_neg</span></span>(<span>self, x, shift)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.ExtendedSoftplus.log_diag_jacobian_pos"><code class="name flex">
<span>def <span class="ident">log_diag_jacobian_pos</span></span>(<span>self, x, shift)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.ExtendedSoftplus.softminus"><code class="name flex">
<span>def <span class="ident">softminus</span></span>(<span>self, x, shift)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.ExtendedSoftplus.softplus"><code class="name flex">
<span>def <span class="ident">softplus</span></span>(<span>self, x, shift)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="flowcon.transforms.nonlinearities.GatedLinearUnit"><code class="flex name class">
<span>class <span class="ident">GatedLinearUnit</span></span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all transform objects.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GatedLinearUnit(Transform):
    def __init__(self):
        super().__init__()

    def forward(self, inputs, context=None):
        gate = torch.sigmoid(context)
        # return inputs * (1 + gate), torch.log(torch.ones_like(gate) + gate).reshape(-1)
        return inputs * gate, torch.log(gate).reshape(-1)

    def inverse(self, inputs, context=None):
        gate = torch.sigmoid(context)
        # return inputs / (1 + gate), - torch.log(torch.ones_like(gate) + gate).reshape(-1)
        return inputs / gate, -torch.log(gate).reshape(-1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.GatedLinearUnit.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.GatedLinearUnit.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.GatedLinearUnit.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.GatedLinearUnit.inverse"><code class="name flex">
<span>def <span class="ident">inverse</span></span>(<span>self, inputs, context=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.base.Transform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.nonlinearities.LeakyReLU"><code class="flex name class">
<span>class <span class="ident">LeakyReLU</span></span>
<span>(</span><span>negative_slope=0.01)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all transform objects.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LeakyReLU(Transform):
    def __init__(self, negative_slope=1e-2):
        if negative_slope &lt;= 0:
            raise ValueError(&#34;Slope must be positive.&#34;)
        super().__init__()
        # self.device = device
        self.negative_slope = negative_slope
        self.log_negative_slope = torch.nn.Parameter(torch.log(torch.as_tensor(self.negative_slope)))  # .to(device)

    def forward(self, inputs, context=None):
        outputs = F.leaky_relu(inputs, negative_slope=self.negative_slope)
        mask = (inputs &lt; 0).type(torch.Tensor).to(inputs.device)
        logabsdet = self.log_negative_slope * mask
        logabsdet = torchutils.sum_except_batch(logabsdet, num_batch_dims=1)
        return outputs, logabsdet

    def inverse(self, inputs, context=None):
        outputs = F.leaky_relu(inputs, negative_slope=(1 / self.negative_slope))
        mask = (inputs &lt; 0).type(torch.Tensor).to(inputs.device)
        logabsdet = -self.log_negative_slope * mask
        logabsdet = torchutils.sum_except_batch(logabsdet, num_batch_dims=1)
        return outputs, logabsdet</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.LeakyReLU.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.LeakyReLU.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.LeakyReLU.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.LeakyReLU.inverse"><code class="name flex">
<span>def <span class="ident">inverse</span></span>(<span>self, inputs, context=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.base.Transform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.nonlinearities.LogTanh"><code class="flex name class">
<span>class <span class="ident">LogTanh</span></span>
<span>(</span><span>cut_point=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Tanh with unbounded output. </p>
<p>Constructed by selecting a cut_point, and replacing values to the right of cut_point
with alpha * log(beta * x), and to the left of -cut_point with -alpha * log(-beta *
x). alpha and beta are set to match the value and the first derivative of tanh at
cut_point.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LogTanh(Transform):
    &#34;&#34;&#34;Tanh with unbounded output. 

    Constructed by selecting a cut_point, and replacing values to the right of cut_point
    with alpha * log(beta * x), and to the left of -cut_point with -alpha * log(-beta *
    x). alpha and beta are set to match the value and the first derivative of tanh at
    cut_point.&#34;&#34;&#34;

    def __init__(self, cut_point=1):
        if cut_point &lt;= 0:
            raise ValueError(&#34;Cut point must be positive.&#34;)
        super().__init__()

        self.cut_point = cut_point
        self.inv_cut_point = np.tanh(cut_point)

        self.alpha = (1 - np.tanh(np.tanh(cut_point))) / cut_point
        self.beta = np.exp(
            (np.tanh(cut_point) - self.alpha * np.log(cut_point)) / self.alpha
        )

    def forward(self, inputs, context=None):
        mask_right = inputs &gt; self.cut_point
        mask_left = inputs &lt; -self.cut_point
        mask_middle = ~(mask_right | mask_left)

        outputs = torch.zeros_like(inputs)
        outputs[mask_middle] = torch.tanh(inputs[mask_middle])
        outputs[mask_right] = self.alpha * torch.log(self.beta * inputs[mask_right])
        outputs[mask_left] = self.alpha * -torch.log(-self.beta * inputs[mask_left])

        logabsdet = torch.zeros_like(inputs)
        logabsdet[mask_middle] = torch.log(1 - outputs[mask_middle] ** 2)
        logabsdet[mask_right] = torch.log(self.alpha / inputs[mask_right])
        logabsdet[mask_left] = torch.log(-self.alpha / inputs[mask_left])
        logabsdet = torchutils.sum_except_batch(logabsdet, num_batch_dims=1)

        return outputs, logabsdet

    def inverse(self, inputs, context=None):
        mask_right = inputs &gt; self.inv_cut_point
        mask_left = inputs &lt; -self.inv_cut_point
        mask_middle = ~(mask_right | mask_left)

        outputs = torch.zeros_like(inputs)
        outputs[mask_middle] = 0.5 * torch.log(
            (1 + inputs[mask_middle]) / (1 - inputs[mask_middle])
        )
        outputs[mask_right] = torch.exp(inputs[mask_right] / self.alpha) / self.beta
        outputs[mask_left] = -torch.exp(-inputs[mask_left] / self.alpha) / self.beta

        logabsdet = torch.zeros_like(inputs)
        logabsdet[mask_middle] = -torch.log(1 - inputs[mask_middle] ** 2)
        logabsdet[mask_right] = (
                -np.log(self.alpha * self.beta) + inputs[mask_right] / self.alpha
        )
        logabsdet[mask_left] = (
                -np.log(self.alpha * self.beta) - inputs[mask_left] / self.alpha
        )
        logabsdet = torchutils.sum_except_batch(logabsdet, num_batch_dims=1)

        return outputs, logabsdet</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.LogTanh.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.LogTanh.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.LogTanh.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.LogTanh.inverse"><code class="name flex">
<span>def <span class="ident">inverse</span></span>(<span>self, inputs, context=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.base.Transform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.nonlinearities.Logit"><code class="flex name class">
<span>class <span class="ident">Logit</span></span>
<span>(</span><span>temperature=1, eps=1e-06)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a transform that is the inverse of a given transform.</p>
<p>Constructor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>transform</code></strong></dt>
<dd>An object of type <code>Transform</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Logit(InverseTransform):
    def __init__(self, temperature=1, eps=1e-6):
        super().__init__(Sigmoid(temperature=temperature, eps=eps))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.base.InverseTransform" href="base.html#flowcon.transforms.base.InverseTransform">InverseTransform</a></li>
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.Logit.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.Logit.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.Logit.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.base.InverseTransform" href="base.html#flowcon.transforms.base.InverseTransform">InverseTransform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.base.InverseTransform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.nonlinearities.PiecewiseCubicCDF"><code class="flex name class">
<span>class <span class="ident">PiecewiseCubicCDF</span></span>
<span>(</span><span>shape, num_bins=10, tails=None, tail_bound=1.0, min_bin_width=0.001, min_bin_height=0.001)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all transform objects.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PiecewiseCubicCDF(Transform):
    def __init__(
            self,
            shape,
            num_bins=10,
            tails=None,
            tail_bound=1.0,
            min_bin_width=splines.cubic.DEFAULT_MIN_BIN_WIDTH,
            min_bin_height=splines.cubic.DEFAULT_MIN_BIN_HEIGHT,
    ):
        super().__init__()

        self.min_bin_width = min_bin_width
        self.min_bin_height = min_bin_height
        self.tail_bound = tail_bound
        self.tails = tails

        self.unnormalized_widths = nn.Parameter(torch.randn(*shape, num_bins))
        self.unnormalized_heights = nn.Parameter(torch.randn(*shape, num_bins))
        self.unnorm_derivatives_left = nn.Parameter(torch.randn(*shape, 1))
        self.unnorm_derivatives_right = nn.Parameter(torch.randn(*shape, 1))

    def _spline(self, inputs, inverse=False):
        batch_size = inputs.shape[0]

        unnormalized_widths = _share_across_batch(self.unnormalized_widths, batch_size)
        unnormalized_heights = _share_across_batch(
            self.unnormalized_heights, batch_size
        )
        unnorm_derivatives_left = _share_across_batch(
            self.unnorm_derivatives_left, batch_size
        )
        unnorm_derivatives_right = _share_across_batch(
            self.unnorm_derivatives_right, batch_size
        )

        if self.tails is None:
            spline_fn = splines.cubic_spline
            spline_kwargs = {}
        else:
            spline_fn = splines.unconstrained_cubic_spline
            spline_kwargs = {&#34;tails&#34;: self.tails, &#34;tail_bound&#34;: self.tail_bound}

        outputs, logabsdet = spline_fn(
            inputs=inputs,
            unnormalized_widths=unnormalized_widths,
            unnormalized_heights=unnormalized_heights,
            unnorm_derivatives_left=unnorm_derivatives_left,
            unnorm_derivatives_right=unnorm_derivatives_right,
            inverse=inverse,
            min_bin_width=self.min_bin_width,
            min_bin_height=self.min_bin_height,
            **spline_kwargs
        )

        return outputs, torchutils.sum_except_batch(logabsdet)

    def forward(self, inputs, context=None):
        return self._spline(inputs, inverse=False)

    def inverse(self, inputs, context=None):
        return self._spline(inputs, inverse=True)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.PiecewiseCubicCDF.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.PiecewiseCubicCDF.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.PiecewiseCubicCDF.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.PiecewiseCubicCDF.inverse"><code class="name flex">
<span>def <span class="ident">inverse</span></span>(<span>self, inputs, context=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.base.Transform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.nonlinearities.PiecewiseLinearCDF"><code class="flex name class">
<span>class <span class="ident">PiecewiseLinearCDF</span></span>
<span>(</span><span>shape, num_bins=10, tails=None, tail_bound=1.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all transform objects.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PiecewiseLinearCDF(Transform):
    def __init__(self, shape, num_bins=10, tails=None, tail_bound=1.0):
        super().__init__()

        self.tail_bound = tail_bound
        self.tails = tails

        self.unnormalized_pdf = nn.Parameter(torch.randn(*shape, num_bins))

    def _spline(self, inputs, inverse=False):
        batch_size = inputs.shape[0]

        unnormalized_pdf = _share_across_batch(self.unnormalized_pdf, batch_size)

        if self.tails is None:
            outputs, logabsdet = splines.linear_spline(
                inputs=inputs, unnormalized_pdf=unnormalized_pdf, inverse=inverse
            )
        else:
            outputs, logabsdet = splines.unconstrained_linear_spline(
                inputs=inputs,
                unnormalized_pdf=unnormalized_pdf,
                inverse=inverse,
                tails=self.tails,
                tail_bound=self.tail_bound,
            )

        return outputs, torchutils.sum_except_batch(logabsdet)

    def forward(self, inputs, context=None):
        return self._spline(inputs, inverse=False)

    def inverse(self, inputs, context=None):
        return self._spline(inputs, inverse=True)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.PiecewiseLinearCDF.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.PiecewiseLinearCDF.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.PiecewiseLinearCDF.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.PiecewiseLinearCDF.inverse"><code class="name flex">
<span>def <span class="ident">inverse</span></span>(<span>self, inputs, context=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.base.Transform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.nonlinearities.PiecewiseQuadraticCDF"><code class="flex name class">
<span>class <span class="ident">PiecewiseQuadraticCDF</span></span>
<span>(</span><span>shape, num_bins=10, tails=None, tail_bound=1.0, min_bin_width=0.001, min_bin_height=0.001)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all transform objects.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PiecewiseQuadraticCDF(Transform):
    def __init__(
            self,
            shape,
            num_bins=10,
            tails=None,
            tail_bound=1.0,
            min_bin_width=splines.quadratic.DEFAULT_MIN_BIN_WIDTH,
            min_bin_height=splines.quadratic.DEFAULT_MIN_BIN_HEIGHT,
    ):
        super().__init__()
        self.min_bin_width = min_bin_width
        self.min_bin_height = min_bin_height
        self.tail_bound = tail_bound
        self.tails = tails

        self.unnormalized_widths = nn.Parameter(torch.randn(*shape, num_bins))
        if tails is None:
            self.unnormalized_heights = nn.Parameter(torch.randn(*shape, num_bins + 1))
        else:
            self.unnormalized_heights = nn.Parameter(torch.randn(*shape, num_bins - 1))

    def _spline(self, inputs, inverse=False):
        batch_size = inputs.shape[0]

        unnormalized_widths = _share_across_batch(self.unnormalized_widths, batch_size)
        unnormalized_heights = _share_across_batch(
            self.unnormalized_heights, batch_size
        )

        if self.tails is None:
            spline_fn = splines.quadratic_spline
            spline_kwargs = {}
        else:
            spline_fn = splines.unconstrained_quadratic_spline
            spline_kwargs = {&#34;tails&#34;: self.tails, &#34;tail_bound&#34;: self.tail_bound}

        outputs, logabsdet = spline_fn(
            inputs=inputs,
            unnormalized_widths=unnormalized_widths,
            unnormalized_heights=unnormalized_heights,
            inverse=inverse,
            min_bin_width=self.min_bin_width,
            min_bin_height=self.min_bin_height,
            **spline_kwargs
        )

        return outputs, torchutils.sum_except_batch(logabsdet)

    def forward(self, inputs, context=None):
        return self._spline(inputs, inverse=False)

    def inverse(self, inputs, context=None):
        return self._spline(inputs, inverse=True)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.PiecewiseQuadraticCDF.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.PiecewiseQuadraticCDF.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.PiecewiseQuadraticCDF.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.PiecewiseQuadraticCDF.inverse"><code class="name flex">
<span>def <span class="ident">inverse</span></span>(<span>self, inputs, context=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.base.Transform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.nonlinearities.PiecewiseRationalQuadraticCDF"><code class="flex name class">
<span>class <span class="ident">PiecewiseRationalQuadraticCDF</span></span>
<span>(</span><span>shape, num_bins=10, tails=None, tail_bound=1.0, identity_init=False, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all transform objects.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PiecewiseRationalQuadraticCDF(Transform):
    def __init__(
            self,
            shape,
            num_bins=10,
            tails=None,
            tail_bound=1.0,
            identity_init=False,
            min_bin_width=splines.rational_quadratic.DEFAULT_MIN_BIN_WIDTH,
            min_bin_height=splines.rational_quadratic.DEFAULT_MIN_BIN_HEIGHT,
            min_derivative=splines.rational_quadratic.DEFAULT_MIN_DERIVATIVE,
    ):
        super().__init__()

        self.min_bin_width = min_bin_width
        self.min_bin_height = min_bin_height
        self.min_derivative = min_derivative

        self.tail_bound = tail_bound
        self.tails = tails

        if isinstance(shape, int):
            shape = (shape,)
        if identity_init:
            self.unnormalized_widths = nn.Parameter(torch.zeros(*shape, num_bins))
            self.unnormalized_heights = nn.Parameter(torch.zeros(*shape, num_bins))

            constant = np.log(np.exp(1 - min_derivative) - 1)
            num_derivatives = (
                (num_bins - 1) if self.tails == &#34;linear&#34; else (num_bins + 1)
            )
            self.unnormalized_derivatives = nn.Parameter(
                constant * torch.ones(*shape, num_derivatives)
            )
        else:
            self.unnormalized_widths = nn.Parameter(torch.rand(*shape, num_bins))
            self.unnormalized_heights = nn.Parameter(torch.rand(*shape, num_bins))

            num_derivatives = (
                (num_bins - 1) if self.tails == &#34;linear&#34; else (num_bins + 1)
            )
            self.unnormalized_derivatives = nn.Parameter(
                torch.rand(*shape, num_derivatives)
            )

    def _spline(self, inputs, inverse=False):
        batch_size = inputs.shape[0]

        unnormalized_widths = _share_across_batch(self.unnormalized_widths, batch_size)
        unnormalized_heights = _share_across_batch(
            self.unnormalized_heights, batch_size
        )
        unnormalized_derivatives = _share_across_batch(
            self.unnormalized_derivatives, batch_size
        )

        if self.tails is None:
            spline_fn = splines.rational_quadratic_spline
            spline_kwargs = {}
        else:
            spline_fn = splines.unconstrained_rational_quadratic_spline
            spline_kwargs = {&#34;tails&#34;: self.tails, &#34;tail_bound&#34;: self.tail_bound}

        outputs, logabsdet = spline_fn(
            inputs=inputs,
            unnormalized_widths=unnormalized_widths,
            unnormalized_heights=unnormalized_heights,
            unnormalized_derivatives=unnormalized_derivatives,
            inverse=inverse,
            min_bin_width=self.min_bin_width,
            min_bin_height=self.min_bin_height,
            min_derivative=self.min_derivative,
            **spline_kwargs
        )

        return outputs, torchutils.sum_except_batch(logabsdet)

    def forward(self, inputs, context=None):
        return self._spline(inputs, inverse=False)

    def inverse(self, inputs, context=None):
        return self._spline(inputs, inverse=True)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.PiecewiseRationalQuadraticCDF.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.PiecewiseRationalQuadraticCDF.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.PiecewiseRationalQuadraticCDF.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.PiecewiseRationalQuadraticCDF.inverse"><code class="name flex">
<span>def <span class="ident">inverse</span></span>(<span>self, inputs, context=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.base.Transform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.nonlinearities.Sigmoid"><code class="flex name class">
<span>class <span class="ident">Sigmoid</span></span>
<span>(</span><span>temperature=1, eps=1e-06, learn_temperature=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all transform objects.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Sigmoid(Transform):
    def __init__(self, temperature=1, eps=1e-6, learn_temperature=False):
        super().__init__()
        self.eps = eps
        if learn_temperature:
            self.temperature = nn.Parameter(torch.Tensor([temperature]))
        else:
            temperature = torch.Tensor([temperature])
            self.register_buffer(&#39;temperature&#39;, temperature)

    def forward(self, inputs, context=None):
        inputs = self.temperature * inputs
        outputs = torch.sigmoid(inputs)
        logabsdet = torchutils.sum_except_batch(
            torch.log(self.temperature) - F.softplus(-inputs) - F.softplus(inputs)
        )
        return outputs, logabsdet

    def inverse(self, inputs, context=None):
        if torch.min(inputs) &lt; 0 or torch.max(inputs) &gt; 1:
            raise InputOutsideDomain()

        inputs = torch.clamp(inputs, self.eps, 1 - self.eps)

        outputs = (1 / self.temperature) * (torch.log(inputs) - torch.log1p(-inputs))
        logabsdet = -torchutils.sum_except_batch(
            torch.log(self.temperature)
            - F.softplus(-self.temperature * outputs)
            - F.softplus(self.temperature * outputs)
        )
        return outputs, logabsdet</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.Sigmoid.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.Sigmoid.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.Sigmoid.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.Sigmoid.inverse"><code class="name flex">
<span>def <span class="ident">inverse</span></span>(<span>self, inputs, context=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.base.Transform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.nonlinearities.Softplus"><code class="flex name class">
<span>class <span class="ident">Softplus</span></span>
<span>(</span><span>threshold=20, eps=0.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all transform objects.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Softplus(Transform):
    def __init__(self, threshold=20, eps=0.):
        super().__init__()

        self.eps = eps
        self.softplus = torch.nn.Softplus(beta=1, threshold=threshold)
        self.log_sigmoid = torch.nn.LogSigmoid()

    def forward(self, inputs, context=None):
        outputs = self.softplus(inputs) + self.eps
        logabsdet = self.log_sigmoid(inputs).sum(-1)
        return outputs, logabsdet

    def inverse(self, inputs, context=None):
        inputs = inputs - self.eps
        outputs = torch.where(inputs &gt; self.softplus.threshold, inputs, inputs.expm1().log())
        logabsdet = -torch.log(-torch.expm1(-inputs)).sum(-1)
        return outputs, logabsdet</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.Softplus.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.Softplus.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.Softplus.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.Softplus.inverse"><code class="name flex">
<span>def <span class="ident">inverse</span></span>(<span>self, inputs, context=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.base.Transform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flowcon.transforms.nonlinearities.Tanh"><code class="flex name class">
<span>class <span class="ident">Tanh</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all transform objects.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Tanh(Transform):
    def forward(self, inputs, context=None):
        outputs = torch.tanh(inputs)
        logabsdet = torch.log(1 - outputs ** 2)
        logabsdet = torchutils.sum_except_batch(logabsdet, num_batch_dims=1)
        return outputs, logabsdet

    def inverse(self, inputs, context=None):
        if torch.min(inputs) &lt;= -1 or torch.max(inputs) &gt;= 1:
            raise InputOutsideDomain()
        outputs = 0.5 * torch.log((1 + inputs) / (1 - inputs))
        logabsdet = -torch.log(1 - inputs ** 2)
        logabsdet = torchutils.sum_except_batch(logabsdet, num_batch_dims=1)
        return outputs, logabsdet</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.Tanh.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.Tanh.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flowcon.transforms.nonlinearities.Tanh.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flowcon.transforms.nonlinearities.Tanh.inverse"><code class="name flex">
<span>def <span class="ident">inverse</span></span>(<span>self, inputs, context=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flowcon.transforms.base.Transform" href="base.html#flowcon.transforms.base.Transform">Transform</a></b></code>:
<ul class="hlist">
<li><code><a title="flowcon.transforms.base.Transform.forward" href="base.html#flowcon.transforms.base.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="flowcon.transforms" href="index.html">flowcon.transforms</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="flowcon.transforms.nonlinearities.CauchyCDF" href="#flowcon.transforms.nonlinearities.CauchyCDF">CauchyCDF</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.nonlinearities.CauchyCDF.call_super_init" href="#flowcon.transforms.nonlinearities.CauchyCDF.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.CauchyCDF.dump_patches" href="#flowcon.transforms.nonlinearities.CauchyCDF.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.CauchyCDF.inverse" href="#flowcon.transforms.nonlinearities.CauchyCDF.inverse">inverse</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.CauchyCDF.training" href="#flowcon.transforms.nonlinearities.CauchyCDF.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.nonlinearities.CauchyCDFInverse" href="#flowcon.transforms.nonlinearities.CauchyCDFInverse">CauchyCDFInverse</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.nonlinearities.CauchyCDFInverse.call_super_init" href="#flowcon.transforms.nonlinearities.CauchyCDFInverse.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.CauchyCDFInverse.dump_patches" href="#flowcon.transforms.nonlinearities.CauchyCDFInverse.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.CauchyCDFInverse.training" href="#flowcon.transforms.nonlinearities.CauchyCDFInverse.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.nonlinearities.CompositeCDFTransform" href="#flowcon.transforms.nonlinearities.CompositeCDFTransform">CompositeCDFTransform</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.nonlinearities.CompositeCDFTransform.call_super_init" href="#flowcon.transforms.nonlinearities.CompositeCDFTransform.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.CompositeCDFTransform.dump_patches" href="#flowcon.transforms.nonlinearities.CompositeCDFTransform.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.CompositeCDFTransform.training" href="#flowcon.transforms.nonlinearities.CompositeCDFTransform.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.nonlinearities.Exp" href="#flowcon.transforms.nonlinearities.Exp">Exp</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.nonlinearities.Exp.call_super_init" href="#flowcon.transforms.nonlinearities.Exp.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.Exp.dump_patches" href="#flowcon.transforms.nonlinearities.Exp.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.Exp.inverse" href="#flowcon.transforms.nonlinearities.Exp.inverse">inverse</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.Exp.training" href="#flowcon.transforms.nonlinearities.Exp.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.nonlinearities.ExtendedSoftplus" href="#flowcon.transforms.nonlinearities.ExtendedSoftplus">ExtendedSoftplus</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.nonlinearities.ExtendedSoftplus.call_super_init" href="#flowcon.transforms.nonlinearities.ExtendedSoftplus.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.ExtendedSoftplus.diag_jacobian_neg" href="#flowcon.transforms.nonlinearities.ExtendedSoftplus.diag_jacobian_neg">diag_jacobian_neg</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.ExtendedSoftplus.diag_jacobian_pos" href="#flowcon.transforms.nonlinearities.ExtendedSoftplus.diag_jacobian_pos">diag_jacobian_pos</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.ExtendedSoftplus.dump_patches" href="#flowcon.transforms.nonlinearities.ExtendedSoftplus.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.ExtendedSoftplus.forward" href="#flowcon.transforms.nonlinearities.ExtendedSoftplus.forward">forward</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.ExtendedSoftplus.get_shift" href="#flowcon.transforms.nonlinearities.ExtendedSoftplus.get_shift">get_shift</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.ExtendedSoftplus.log_diag_jacobian_neg" href="#flowcon.transforms.nonlinearities.ExtendedSoftplus.log_diag_jacobian_neg">log_diag_jacobian_neg</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.ExtendedSoftplus.log_diag_jacobian_pos" href="#flowcon.transforms.nonlinearities.ExtendedSoftplus.log_diag_jacobian_pos">log_diag_jacobian_pos</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.ExtendedSoftplus.softminus" href="#flowcon.transforms.nonlinearities.ExtendedSoftplus.softminus">softminus</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.ExtendedSoftplus.softplus" href="#flowcon.transforms.nonlinearities.ExtendedSoftplus.softplus">softplus</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.ExtendedSoftplus.training" href="#flowcon.transforms.nonlinearities.ExtendedSoftplus.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.nonlinearities.GatedLinearUnit" href="#flowcon.transforms.nonlinearities.GatedLinearUnit">GatedLinearUnit</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.nonlinearities.GatedLinearUnit.call_super_init" href="#flowcon.transforms.nonlinearities.GatedLinearUnit.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.GatedLinearUnit.dump_patches" href="#flowcon.transforms.nonlinearities.GatedLinearUnit.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.GatedLinearUnit.inverse" href="#flowcon.transforms.nonlinearities.GatedLinearUnit.inverse">inverse</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.GatedLinearUnit.training" href="#flowcon.transforms.nonlinearities.GatedLinearUnit.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.nonlinearities.LeakyReLU" href="#flowcon.transforms.nonlinearities.LeakyReLU">LeakyReLU</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.nonlinearities.LeakyReLU.call_super_init" href="#flowcon.transforms.nonlinearities.LeakyReLU.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.LeakyReLU.dump_patches" href="#flowcon.transforms.nonlinearities.LeakyReLU.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.LeakyReLU.inverse" href="#flowcon.transforms.nonlinearities.LeakyReLU.inverse">inverse</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.LeakyReLU.training" href="#flowcon.transforms.nonlinearities.LeakyReLU.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.nonlinearities.LogTanh" href="#flowcon.transforms.nonlinearities.LogTanh">LogTanh</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.nonlinearities.LogTanh.call_super_init" href="#flowcon.transforms.nonlinearities.LogTanh.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.LogTanh.dump_patches" href="#flowcon.transforms.nonlinearities.LogTanh.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.LogTanh.inverse" href="#flowcon.transforms.nonlinearities.LogTanh.inverse">inverse</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.LogTanh.training" href="#flowcon.transforms.nonlinearities.LogTanh.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.nonlinearities.Logit" href="#flowcon.transforms.nonlinearities.Logit">Logit</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.nonlinearities.Logit.call_super_init" href="#flowcon.transforms.nonlinearities.Logit.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.Logit.dump_patches" href="#flowcon.transforms.nonlinearities.Logit.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.Logit.training" href="#flowcon.transforms.nonlinearities.Logit.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.nonlinearities.PiecewiseCubicCDF" href="#flowcon.transforms.nonlinearities.PiecewiseCubicCDF">PiecewiseCubicCDF</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.nonlinearities.PiecewiseCubicCDF.call_super_init" href="#flowcon.transforms.nonlinearities.PiecewiseCubicCDF.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.PiecewiseCubicCDF.dump_patches" href="#flowcon.transforms.nonlinearities.PiecewiseCubicCDF.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.PiecewiseCubicCDF.inverse" href="#flowcon.transforms.nonlinearities.PiecewiseCubicCDF.inverse">inverse</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.PiecewiseCubicCDF.training" href="#flowcon.transforms.nonlinearities.PiecewiseCubicCDF.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.nonlinearities.PiecewiseLinearCDF" href="#flowcon.transforms.nonlinearities.PiecewiseLinearCDF">PiecewiseLinearCDF</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.nonlinearities.PiecewiseLinearCDF.call_super_init" href="#flowcon.transforms.nonlinearities.PiecewiseLinearCDF.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.PiecewiseLinearCDF.dump_patches" href="#flowcon.transforms.nonlinearities.PiecewiseLinearCDF.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.PiecewiseLinearCDF.inverse" href="#flowcon.transforms.nonlinearities.PiecewiseLinearCDF.inverse">inverse</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.PiecewiseLinearCDF.training" href="#flowcon.transforms.nonlinearities.PiecewiseLinearCDF.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.nonlinearities.PiecewiseQuadraticCDF" href="#flowcon.transforms.nonlinearities.PiecewiseQuadraticCDF">PiecewiseQuadraticCDF</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.nonlinearities.PiecewiseQuadraticCDF.call_super_init" href="#flowcon.transforms.nonlinearities.PiecewiseQuadraticCDF.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.PiecewiseQuadraticCDF.dump_patches" href="#flowcon.transforms.nonlinearities.PiecewiseQuadraticCDF.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.PiecewiseQuadraticCDF.inverse" href="#flowcon.transforms.nonlinearities.PiecewiseQuadraticCDF.inverse">inverse</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.PiecewiseQuadraticCDF.training" href="#flowcon.transforms.nonlinearities.PiecewiseQuadraticCDF.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.nonlinearities.PiecewiseRationalQuadraticCDF" href="#flowcon.transforms.nonlinearities.PiecewiseRationalQuadraticCDF">PiecewiseRationalQuadraticCDF</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.nonlinearities.PiecewiseRationalQuadraticCDF.call_super_init" href="#flowcon.transforms.nonlinearities.PiecewiseRationalQuadraticCDF.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.PiecewiseRationalQuadraticCDF.dump_patches" href="#flowcon.transforms.nonlinearities.PiecewiseRationalQuadraticCDF.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.PiecewiseRationalQuadraticCDF.inverse" href="#flowcon.transforms.nonlinearities.PiecewiseRationalQuadraticCDF.inverse">inverse</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.PiecewiseRationalQuadraticCDF.training" href="#flowcon.transforms.nonlinearities.PiecewiseRationalQuadraticCDF.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.nonlinearities.Sigmoid" href="#flowcon.transforms.nonlinearities.Sigmoid">Sigmoid</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.nonlinearities.Sigmoid.call_super_init" href="#flowcon.transforms.nonlinearities.Sigmoid.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.Sigmoid.dump_patches" href="#flowcon.transforms.nonlinearities.Sigmoid.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.Sigmoid.inverse" href="#flowcon.transforms.nonlinearities.Sigmoid.inverse">inverse</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.Sigmoid.training" href="#flowcon.transforms.nonlinearities.Sigmoid.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.nonlinearities.Softplus" href="#flowcon.transforms.nonlinearities.Softplus">Softplus</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.nonlinearities.Softplus.call_super_init" href="#flowcon.transforms.nonlinearities.Softplus.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.Softplus.dump_patches" href="#flowcon.transforms.nonlinearities.Softplus.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.Softplus.inverse" href="#flowcon.transforms.nonlinearities.Softplus.inverse">inverse</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.Softplus.training" href="#flowcon.transforms.nonlinearities.Softplus.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flowcon.transforms.nonlinearities.Tanh" href="#flowcon.transforms.nonlinearities.Tanh">Tanh</a></code></h4>
<ul class="">
<li><code><a title="flowcon.transforms.nonlinearities.Tanh.call_super_init" href="#flowcon.transforms.nonlinearities.Tanh.call_super_init">call_super_init</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.Tanh.dump_patches" href="#flowcon.transforms.nonlinearities.Tanh.dump_patches">dump_patches</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.Tanh.inverse" href="#flowcon.transforms.nonlinearities.Tanh.inverse">inverse</a></code></li>
<li><code><a title="flowcon.transforms.nonlinearities.Tanh.training" href="#flowcon.transforms.nonlinearities.Tanh.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.0</a>.</p>
</footer>
</body>
</html>
